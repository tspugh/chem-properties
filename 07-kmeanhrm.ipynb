{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":13052622,"sourceType":"datasetVersion","datasetId":8264295}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/khrm-for-openpolymer /kaggle/working/.\n!cd /kaggle/working/khrm-for-openpolymer\n!pip install torch-geometric\n!pip install torch-cluster /kaggle/input/torch-geometric/torch_cluster-1.6.3-cp310-cp310-linux_x86_64.whl\n!pip install torch-scatter # /kaggle/input/torch-geometric/torch_scatter-2.1.2-cp310-cp310-linux_x86_64.whl\n#!pip install /kaggle/input/torch-geometric/torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl\n!pip install  \"pyscf>=2.10.0\" \"pyscf-semiempirical>=0.1.1\" \"pysmiles>=2.0.0\" \"rdkit>=2025.3.5\"","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport json\nfrom typing import List, Tuple, Dict\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\n\n# Data props\nPROPERTIES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\nINPUTS = [\"SMILES\"]\nAUX_INFO = [\"monomer_count\", \"original_atoms\", \"final_atoms\", \"SMILES\"]\n\n# y output\nTARGET_DIM = len(PROPERTIES)\n\n# Params\nEPOCHS = 100\nBATCH_SIZE = 16\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nCHECKPOINT_EVERY_N_STEPS = 100\nK_HEADS = 16\n\n# data params\nVAL_RATIO = 0.1\nMIN_LENGTH = 150\nMAX_OUTPUT = 10\n\n\n# Random seeding\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.608Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\nPROJECT_ROOT='/kaggle/working/'\nCHECKPOINT_DIR = os.path.join(PROJECT_ROOT, \"checkpoints\", \"hrm\")\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nLOAD_COLS = (INPUTS + PROPERTIES)\n\nraw_df = pd.DataFrame(columns=LOAD_COLS)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if ('dataset' in filename and '.csv' in filename) or 'train.csv' in filename:\n            df_temp = pd.read_csv(os.path.join(dirname, filename))\n            df_temp.reindex(columns=LOAD_COLS)\n            raw_df = pd.concat([raw_df, df_temp], ignore_index=True)\n\nprint(len(raw_df))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":" # !pip install -e /kaggle/input/khrm-for-openpolymer --no-deps --no-build-isolation","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pysmiles\n!pip install rdkit","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/khrm-for-openpolymer/src\")\nimport data_gen_helpers\nfrom data_gen_helpers import iterative_extend_smiles, count_non_hydrogen_atoms\n\nimport logging\nfrom datetime import datetime\n\nfrom multiprocessing import Pool, cpu_count\nN_PROCESSES = max(cpu_count()-1, 1)\n\ndef process_row(args):\n    idx, row = args\n    original_smiles = row[\"SMILES\"]\n    original_atoms = count_non_hydrogen_atoms(original_smiles)\n\n    try:\n        extensions = list(iterative_extend_smiles(\n            original_smiles,\n            min_length=MIN_LENGTH,\n            max_output=MAX_OUTPUT\n        ))\n\n        results = []\n        if extensions:\n            for extended_smiles, monomer_count in extensions:\n                final_atoms = count_non_hydrogen_atoms(extended_smiles)\n                new_row = row.copy()\n                new_row[\"SMILES\"] = extended_smiles\n                new_row[\"monomer_count\"] = monomer_count\n                new_row[\"original_smiles\"] = original_smiles\n                new_row[\"original_atoms\"] = original_atoms\n                new_row[\"final_atoms\"] = final_atoms\n                results.append((\"success\", new_row))\n        else:\n            return [(\"fail\", (idx, original_smiles, \"No extensions generated\"))]\n\n        return [(\"success\", r) for r in results]\n\n    except Exception as e:\n        return [(\"fail\", (idx, original_smiles, str(e)))]\n\n# ---- Parallel Execution ----\ndef parallel_extend(raw_df):\n    extended_data = []\n    failed_extensions = []\n\n    with Pool(N_PROCESSES) as pool:\n        for results in pool.imap_unordered(process_row, raw_df.iterrows(), chunksize=10):\n            for status, payload in results:\n                if status == \"success\":\n                    extended_data.append(payload)\n                else:\n                    failed_extensions.append(payload)\n\n    # Build DataFrame\n    extended_df = pd.DataFrame(extended_data)\n    print(f\"\\nSuccessfully extended: {len(extended_df)} molecules\")\n    print(f\"Failed extensions: {len(failed_extensions)}\")\n\n    if failed_extensions:\n        print(\"\\n\\nFailed molecules:\\n\\n\")\n        for idx, smiles, error in failed_extensions[:5]:\n            print(f\"  {idx}: {smiles} - {error}\")\n\n    if len(extended_df) == 0:\n        raise RuntimeError(\"No molecules could be extended. Check your data and extension logic.\")\n\n    return extended_df, failed_extensions\n\n# ---- Run ----\nextended_df, failed_extensions = parallel_extend(raw_df[:2])\nif len(failed_extensions) > 0:\n    logger.debug(\"\\n\\nFailed molecules:\\n\\n\")\n    for idx, smiles, error in failed_extensions:  # Show first 5 errors\n        logger.debug(f\"  {idx}: {smiles} - {error}\")\n\n# Use extended data for training\nif len(extended_df) == 0:\n    raise RuntimeError(\"No molecules could be extended. Check your data and extension logic.\")\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install geometric","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataset_helpers import smiles_iter_to_graph_dataset\n\nimport numpy as np\n\n# Split into Train/Val\nnum_rows = len(extended_df)\nperm = np.random.RandomState(SEED).permutation(num_rows)\ntrain_count = int((1.0 - VAL_RATIO) * num_rows)\ntrain_idx, val_idx = perm[:train_count], perm[train_count:]\ntrain_df = extended_df.iloc[train_idx].reset_index(drop=True)\nval_df = extended_df.iloc[val_idx].reset_index(drop=True)\n\n# Create graph datasets\ntrain_dataset = smiles_iter_to_graph_dataset(train_df[\"SMILES\"], torch.tensor(train_df[PROPERTIES].to_numpy(), dtype=torch.float32, device=device), aux_info=train_df[AUX_INFO])\nval_dataset = smiles_iter_to_graph_dataset(val_df[\"SMILES\"], torch.tensor(val_df[PROPERTIES].to_numpy(), dtype=torch.float32, device=device), aux_info=val_df[AUX_INFO])\n\nprint(f\"example aux_info: {train_dataset[0].aux_info}\")\nprint(f\"Train graphs: {len(train_dataset)} | Val graphs: {len(val_dataset)}\")\n\n# DataListLoader (batched lists of Data objects)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Derive input dimension from first training graph\nif len(train_dataset) == 0:\n    raise RuntimeError(\"Training dataset is empty after preprocessing.\")\nINPUT_DIM = train_dataset[0].x.shape[1]\nprint(f\"Input dim: {INPUT_DIM}, Target dim: {TARGET_DIM}\")\n\nEDGE_DIM = train_dataset[0].edge_attr.shape[1]\nprint(f\"Edge dim: {EDGE_DIM}\")\n\n# Show some examples of the extensions\nprint(f\"\\nExamples of SMILES extensions:\")\nfor i in range(min(3, len(train_df))):\n    row = train_df.iloc[i]\n    print(f\"Original ({row.get('original_atoms', 'N/A')} atoms): {row.get('original_smiles', 'N/A')}\")\n    print(f\"Extended ({row.get('final_atoms', 'N/A')} atoms): {row['SMILES']}\")\n    print()\n\nTOTAL_BATCH_COUNT = len(train_loader)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Use KMeansHRMModule from kmeans_hrm_model.py\nfrom chem_properties.kmeans_hrm_model import (\n    KMeansHRMModule, KMeansHRMConfig, KMeansHRMInnerModuleConfig, KMeansHRMInitialCarry,\n    KMeansConfig, KMeansHeadConfig, OutputHeadConfig,\n    SpectralWeighting, SpectralWeightingConfig,\n    DiscreteMeanCenter, DiscreteMeanCenterConfig,\n    RadiusAttentionWeights, RadiusMaskConfig\n)\n\nfrom torch_geometric.nn import GATConv\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_kmeans_hrm_config(input_dim: int, edge_dim: int, hidden_dim: int = 128, latent_dim: int = 128, output_dim: int = TARGET_DIM, k: int = K_HEADS) -> KMeansHRMConfig:\n    \n    # Spectral Weighting Configuration\n    spectral_config = SpectralWeightingConfig(\n        node_channels=latent_dim,\n        K=3,  # Chebyshev polynomial order\n        num_layers=3,\n        normalization='sym',\n        bias=True,\n        dropout=0.2,\n        norm='batch',\n        norm_kwargs={'in_channels': latent_dim}\n    )\n    \n    # Center Module Configuration\n    center_config = DiscreteMeanCenterConfig(\n        distance_metric='euclidean'\n    )\n    \n    # Radius Mask Configuration (simplified weighting module)\n    radius_weighting = GATConv(latent_dim, latent_dim)\n    radius_config = RadiusMaskConfig(\n        max_num_neighbors=50,\n        radius=20,\n        weighting_module=radius_weighting,\n        threshold=0.1,\n        node_dim=latent_dim\n    )\n    \n    # KMeans Head Configuration\n    kmeans_head_config = KMeansHeadConfig(\n        node_count=k,\n        node_dim=latent_dim,\n        max_nodes=100,  \n        num_layers=5,\n        dropout=0.2,\n        weighting_module=SpectralWeighting(spectral_config),\n        center_module=DiscreteMeanCenter(center_config),\n        mask_module=RadiusAttentionWeights(radius_config),\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # KMeans Configuration\n    kmeans_config = KMeansConfig(\n        k=k,\n        max_iter=15,\n        thresh=1e-6,\n        max_overlap=2,\n        head_module=kmeans_head_config,\n        excluded_is_cluster=True\n    )\n    \n    # Output Head Configuration\n    output_head_config = OutputHeadConfig(\n        node_dim=latent_dim,\n        hidden_dim=hidden_dim,\n        output_dim=output_dim,\n        pooling_type='mean',\n        norm='batch',\n        norm_kwargs={'in_channels': hidden_dim},\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # Policy Module Configuration\n    policy_config = OutputHeadConfig(\n        node_dim=latent_dim,\n        hidden_dim=hidden_dim//2,\n        output_dim=2,  # halt=0, continue=1\n        pooling_type='mean',\n        norm='batch',\n        norm_kwargs={'in_channels': hidden_dim//2},\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # Inner Module Configuration\n    inner_config = KMeansHRMInnerModuleConfig(\n        add_self_loops=True,\n        add_negative_edges=True,\n        dropout=0.2,\n        hidden_dim=hidden_dim,        # inner-side hidden size, reused\n        node_dim=latent_dim,          # must equal vgae_latent_dim\n        attention_dim=64,           # bigger in final\n        edge_dim=edge_dim,\n        layers=3,\n        kmeans_config=kmeans_config,\n        output_head_config=output_head_config,\n        policy_module_config=policy_config,\n        K_cycles=8,\n        L_cycles=30,\n        batch_size=BATCH_SIZE,\n        halt_max_steps=10,\n        halt_exploration_prob=0.1,\n    )\n    \n    config = KMeansHRMConfig(\n        inner_module=inner_config,\n        explore_steps_prob=0.1,\n        halt_max_steps=10,\n        pre_encoder_conv_layers=2,\n        vgae_encoder_type=\"cheb\",\n        input_dim=input_dim,\n        edge_attr_dim=edge_dim,\n        vgae_latent_dim=latent_dim,           # must equal inner.node_dim\n        vgae_encoder_layers=2,\n        vgae_encoder_dropout=0.1,\n        vgae_decoder_type=None,\n        vgae_kl_weight=1.0,\n    )\n    \n    return config\n\n# Modell initialisieren\nhrm_config = create_kmeans_hrm_config(INPUT_DIM, EDGE_DIM, latent_dim=256, hidden_dim=128, output_dim=TARGET_DIM, k=16)\nmodel = KMeansHRMModule(hrm_config, training=True).to(device)\noptimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# Print number of parameters\nnum_params = count_parameters(model)\nprint(f\"KMeansHRMModule created with {num_params:,} trainable parameters\")\nprint(f\"Model size: {num_params * 4 / 1024 / 1024:.2f} MB (float32)\")\nprint(model)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from chem_properties.training_loops import composite_loss, compute_mae_in_bounds, init_property_bounds\n\ninit_property_bounds(PROPERTIES, extended_df)\n\nfrom typing import TypedDict, Dict, Any\nfrom dataclasses import dataclass\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split\nfrom torch.optim import Adam\n\ndef save_checkpoint(state: Dict, step: int, is_best: bool = False):\n    path = os.path.join(CHECKPOINT_DIR, f\"step_{step}.pt\")\n    torch.save(state, path)\n    if is_best:\n        best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n        torch.save(state, best_path)\n\n\nclass PretrainConfig(TypedDict):\n    # Data\n    data_path: str\n\n    # Hyperparams\n    global_batch_size: int\n    epochs: int\n    total_iters: int\n\n    lr: float\n    lr_min_ratio: float\n    lr_warmup_steps: int\n\n    weight_decay: float\n    beta1: float\n    beta2: float\n\n\n@dataclass\nclass TrainState:\n    model: nn.Module\n    optimizer: torch.optim.Optimizer\n    carry: KMeansHRMInitialCarry | None\n\n    step: int\n    total_steps: int\n\n\ndef compute_warmup_weight(step: int, warmup_steps: int, min_ratio: float) -> float:\n    if warmup_steps <= 0:\n        return 1.0\n    if step < warmup_steps:\n        # Linear warmup from min_ratio -> 1.0\n        return float(min_ratio + (1.0 - min_ratio) * (step / max(1, warmup_steps)))\n    return 1.0\n\n\ndef pack_train_state_for_save(ts: TrainState) -> Dict[str, Any]:\n    return {\n        \"step\": int(ts.step),\n        \"total_steps\": int(ts.total_steps),\n        # Carry can be large; still useful for exact resume within the same batch sequence\n        \"carry\": ts.carry,\n    }\n\n\ndef train_batch(epoch: int, train_state: TrainState, batch_data: Batch, config: PretrainConfig) -> Dict[str, float]:\n    model = train_state.model\n    optimizer = train_state.optimizer\n\n    model.train()\n\n    # Targets [B,5]\n    y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n    batch_data = batch_data.to(device)\n    related_info = batch_data.aux_info\n\n    # Reuse existing carry if provided; otherwise initialize\n    if train_state.carry is None:\n        print(f\"Initializing new carry for batch {batch_data.batch.shape[0]}\")\n        train_state.carry = model.initial_carry(batch_data)\n\n    # Forward\n    train_state.carry, hrm_output = model(train_state.carry, batch_data)\n    preds = hrm_output['y_pred']\n\n    loss = composite_loss(preds, y, related_info)\n    train_state.step += 1\n    warmup_w = compute_warmup_weight(train_state.step, config[\"lr_warmup_steps\"], config[\"lr_min_ratio\"])  # type: ignore[index]\n    scaled_loss = loss * warmup_w * (1.0 / TOTAL_BATCH_COUNT)\n\n    # Backward/update\n    optimizer.zero_grad()\n    scaled_loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n\n    # Metrics (unscaled loss for logging)\n    with torch.no_grad():\n        metrics = compute_mae_in_bounds(preds, y, related_info)\n        metrics.update({\n            \"loss\": loss.item(),\n            \"warmup_weight\": float(warmup_w),\n        })\n\n\n    # Periodic checkpoint\n    global_step = train_state.step\n    if global_step % CHECKPOINT_EVERY_N_STEPS == 0:\n        save_checkpoint({\n            \"epoch\": epoch,\n            \"global_step\": global_step,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"model_class\": model.__class__.__name__,\n            \"model_module\": model.__class__.__module__,\n            \"train_state\": pack_train_state_for_save(train_state),\n        }, step=global_step)\n\n    return metrics\n\n\n@torch.no_grad()\ndef validate(epoch: int, model: nn.Module, loader: DataLoader) -> Dict[str, float]:\n    model.eval()\n    running_loss = 0.0\n    mae_accum = {f\"mae_{p}\": 0.0 for p in PROPERTIES}\n    count_samples = 0\n\n    for batch_data in loader:\n        y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n        batch_data = batch_data.to(device)\n        related_info = batch_data.aux_info\n        \n        carry = model.initial_carry(batch_data)\n        _, hrm_output = model(carry, batch_data)\n        preds = hrm_output['y_pred']\n\n        loss = composite_loss(preds, y, related_info)\n\n        metrics = compute_mae_in_bounds(preds, y, related_info)\n        for k, v in metrics.items():\n            if not math.isnan(v):\n                mae_accum[k] += v * preds.size(0)\n        running_loss += loss.item() * preds.size(0)\n        count_samples += preds.size(0)\n\n    avg_loss = running_loss / max(1, count_samples)\n    avg_mae = {k: (v / max(1, count_samples)) for k, v in mae_accum.items()}\n\n    return {\"loss\": avg_loss, **avg_mae}\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from training_loops import train_until_total_iters\n\n# Build a minimal config for warmup from existing hyperparams\nCONFIG: PretrainConfig = {\n    \"data_path\": DATA_CSV,\n    \"global_batch_size\": BATCH_SIZE,\n    \"epochs\": EPOCHS,\n    # Step-based training support; used by helper loop\n    \"total_iters\": int(EPOCHS * len(train_loader)),\n    \"lr\": LR,\n    \"lr_min_ratio\": 0.1,\n    \"lr_warmup_steps\": max(1, len(train_loader) * 2),  # warm up first ~2 epochs of steps\n    \"weight_decay\": WEIGHT_DECAY,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n}\n\nhistory = {\"train\": [], \"val\": []}\nbest_val_loss = float(\"inf\")\n\n# Initialize TrainState with total steps estimated from loader length and epochs\ntrain_state = TrainState(\n    model=model,\n    optimizer=optimizer,\n    carry=None,\n    step=0,\n    total_steps=EPOCHS * len(train_loader)\n)\n\nlast_completed_epoch=0\n\ntrain_until_total_iters(\n    total_iters=CONFIG[\"total_iters\"],\n    start_epoch=last_completed_epoch,\n    train_state=train_state,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    config=CONFIG,\n    train_batch_fn=train_batch,\n    validate_fn=validate,\n    save_checkpoint_fn=save_checkpoint,\n    checkpoint_every_n=10,\n    print_every_n=1\n)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nmetrics_path = os.path.join(CHECKPOINT_DIR, \"history.json\")\nwith open(metrics_path, \"w\") as f:\n    json.dump(history, f, indent=2)\nprint(f\"Saved metrics to {metrics_path}\")\n\n# Plot\ndef plot_curves(history):\n    epochs = [e[\"epoch\"] for e in history[\"train\"]]\n    train_losses = [e[\"loss\"] for e in history[\"train\"]]\n    val_losses = [e[\"loss\"] for e in history[\"val\"]]\n\n    plt.figure(figsize=(8,4))\n    plt.plot(epochs, train_losses, label=\"train_loss\")\n    plt.plot(epochs, val_losses, label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_curves(history)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from smiles_iter_to_graph_dataset import smiles_to_graph_dataset\n\nsubmission_df = pd.read_csv('/kaggle/input/open-polymer-challenge/test.csv')\ninputs = submissions_df[\"SMILES\"]\n\ninputs = []\nfor smiles in inputs:\n    submission_dataset = smiles_iter_to_graph_dataset(smiles, {}, None)\n\nsubmissions_df","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\nmax_iters = 10\n\nin_loader = DataLoader(inputs, batch_size=1, shuffle=False)\nfor datum in inputs:\n    preds = [0, 0, 0, 0, 0]\n    while max_iters > 0:\n        carry = model.initial_carry(datum)\n        og_carry, hrm_output = model(carry, batch_data)\n        preds = hrm_output['y_pred']\n        q_policy = hrm_output['q_policy']\n\n        if q_policy[0] > q_policy[1]:\n            break\n        max_iters -= 1\n\n    submissions[PROPERTIES] = preds\n    \nprint('submission_df', submission_df)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-09-14T17:14:52.612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}