{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":13060531,"sourceType":"datasetVersion","datasetId":8264295}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/khrm-for-openpolymer /kaggle/working/.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:52:03.091078Z","iopub.execute_input":"2025-09-15T15:52:03.091340Z","iopub.status.idle":"2025-09-15T15:52:03.568141Z","shell.execute_reply.started":"2025-09-15T15:52:03.091318Z","shell.execute_reply":"2025-09-15T15:52:03.567149Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!cp -r /kaggle/input/khrm-for-openpolymer /kaggle/working/.\n!cd /kaggle/working/khrm-for-openpolymer\n!pip install --no-cache-dir torch-geometric -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n!pip install --no-cache-dir torch-cluster -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n!pip install --no-cache-dir torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n#!pip install /kaggle/input/torch-geometric/torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl\n!pip install  \"pyscf>=2.10.0\" \"pyscf-semiempirical>=0.1.1\" \"pysmiles>=2.0.0\" \"rdkit>=2025.3.5\"","metadata":{"_kg_hide-output":true,"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:52:03.569896Z","iopub.execute_input":"2025-09-15T15:52:03.570169Z","iopub.status.idle":"2025-09-15T15:52:32.610893Z","shell.execute_reply.started":"2025-09-15T15:52:03.570143Z","shell.execute_reply":"2025-09-15T15:52:32.610201Z"}},"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\nCollecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.12.13)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.5.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m34.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\nLooking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\nCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.15.3)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-cluster) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster) (2024.2.0)\nInstalling collected packages: torch-cluster\nSuccessfully installed torch-cluster-1.6.3+pt26cu124\nLooking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m123.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.2+pt26cu124\nCollecting pyscf>=2.10.0\n  Downloading pyscf-2.10.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nCollecting pyscf-semiempirical>=0.1.1\n  Downloading pyscf-semiempirical-0.1.1.tar.gz (30 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting pysmiles>=2.0.0\n  Downloading pysmiles-2.0.0-py2.py3-none-any.whl.metadata (14 kB)\nCollecting rdkit>=2025.3.5\n  Downloading rdkit-2025.3.6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: numpy!=1.16,!=1.17,>=1.13 in /usr/local/lib/python3.11/dist-packages (from pyscf>=2.10.0) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pyscf>=2.10.0) (1.15.3)\nRequirement already satisfied: h5py>=2.7 in /usr/local/lib/python3.11/dist-packages (from pyscf>=2.10.0) (3.14.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyscf>=2.10.0) (75.2.0)\nCollecting pbr (from pysmiles>=2.0.0)\n  Downloading pbr-7.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pysmiles>=2.0.0) (3.5)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit>=2025.3.5) (11.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2024.2.0)\nDownloading pyscf-2.10.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 MB\u001b[0m \u001b[31m32.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pysmiles-2.0.0-py2.py3-none-any.whl (37 kB)\nDownloading rdkit-2025.3.6-cp311-cp311-manylinux_2_28_x86_64.whl (36.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m45.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pbr-7.0.1-py2.py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.1/126.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyscf-semiempirical\n  Building wheel for pyscf-semiempirical (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyscf-semiempirical: filename=pyscf_semiempirical-0.1.1-cp311-cp311-linux_x86_64.whl size=57471 sha256=d72f0df6729da3962f7a37e286b71fb3b8830b4ca5a63bbf9f8ed5d1ea278a9e\n  Stored in directory: /root/.cache/pip/wheels/68/ed/27/b9089a75f0fc2374f29a237630f068dec796312ccc2bafd6b3\nSuccessfully built pyscf-semiempirical\nInstalling collected packages: pbr, pysmiles, pyscf, rdkit, pyscf-semiempirical\nSuccessfully installed pbr-7.0.1 pyscf-2.10.0 pyscf-semiempirical-0.1.1 pysmiles-2.0.0 rdkit-2025.3.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport json\nfrom typing import List, Tuple, Dict\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\n\n# Data props\nPROPERTIES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\nINPUTS = [\"SMILES\"]\nAUX_INFO = [\"monomer_count\", \"original_atoms\", \"final_atoms\", \"SMILES\"]\n\n# y output\nTARGET_DIM = len(PROPERTIES)\n\n# Params\nEPOCHS = 10\nBATCH_SIZE = 16\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nCHECKPOINT_EVERY_N_STEPS = 100\nK_HEADS = 8\n\n# data params\nVAL_RATIO = 0.1\nMIN_LENGTH = 75\nMAX_OUTPUT = 10\n\n\n# Random seeding\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:52:32.611898Z","iopub.execute_input":"2025-09-15T15:52:32.612183Z","iopub.status.idle":"2025-09-15T15:52:41.172622Z","shell.execute_reply.started":"2025-09-15T15:52:32.612155Z","shell.execute_reply":"2025-09-15T15:52:41.171919Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n\nPROJECT_ROOT = \"/kaggle/input/\"\nDATA_CSV = os.path.join(PROJECT_ROOT, \"neurips-open-polymer-prediction-2025\")\nCHECKPOINT_DIR = os.path.join(\"/kaggle/working\", \"checkpoints\", \"hrm\")\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:52:41.173509Z","iopub.execute_input":"2025-09-15T15:52:41.173999Z","iopub.status.idle":"2025-09-15T15:52:41.178226Z","shell.execute_reply.started":"2025-09-15T15:52:41.173969Z","shell.execute_reply":"2025-09-15T15:52:41.177678Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load data\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nLOAD_COLS = (INPUTS + PROPERTIES)\n\nraw_df = pd.DataFrame(columns=LOAD_COLS)\nimport os\nfor dirname, _, filenames in os.walk(PROJECT_ROOT + \"/neurips-open-polymer-prediction-2025\"):\n    for filename in filenames:\n        if ('dataset' in filename and '.csv' in filename) or 'train.csv' in filename:\n            df_temp = pd.read_csv(os.path.join(dirname, filename))\n            df_temp.reindex(columns=LOAD_COLS)\n            raw_df = pd.concat([raw_df, df_temp], ignore_index=True)\n\nprint(len(raw_df))\nraw_df = (\n    raw_df\n    .sort_values(\"SMILES\")\n    .groupby(\"SMILES\", as_index=False)\n    .first()\n)\nprint(len(raw_df))","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:52:41.179913Z","iopub.execute_input":"2025-09-15T15:52:41.180181Z","iopub.status.idle":"2025-09-15T15:52:41.379410Z","shell.execute_reply.started":"2025-09-15T15:52:41.180159Z","shell.execute_reply":"2025-09-15T15:52:41.378658Z"}},"outputs":[{"name":"stdout","text":"16963\n10345\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/2920036644.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  raw_df = pd.concat([raw_df, df_temp], ignore_index=True)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"!pip install pysmiles\n!pip install rdkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:52:41.380181Z","iopub.execute_input":"2025-09-15T15:52:41.380423Z","iopub.status.idle":"2025-09-15T15:52:47.611475Z","shell.execute_reply.started":"2025-09-15T15:52:41.380405Z","shell.execute_reply":"2025-09-15T15:52:47.610759Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pysmiles in /usr/local/lib/python3.11/dist-packages (2.0.0)\nRequirement already satisfied: pbr in /usr/local/lib/python3.11/dist-packages (from pysmiles) (7.0.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pysmiles) (3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pbr->pysmiles) (75.2.0)\nRequirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit) (2024.2.0)\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/khrm-for-openpolymer/src\")\nimport data_gen_helpers\nfrom data_gen_helpers import iterative_extend_smiles, count_non_hydrogen_atoms\n\nfrom IPython.display import clear_output\nimport logging\nfrom datetime import datetime\n\nfrom multiprocessing import Pool, cpu_count\nN_PROCESSES = max(cpu_count()-1, 1)\n\ndef process_row(args):\n    idx, row = args\n    original_smiles = row[\"SMILES\"]\n    original_atoms = count_non_hydrogen_atoms(original_smiles)\n\n    try:\n        extensions = list(iterative_extend_smiles(\n            original_smiles,\n            min_length=MIN_LENGTH,\n            max_output=MAX_OUTPUT\n        ))\n\n        results = []\n        if extensions:\n            for extended_smiles, monomer_count in extensions:\n                final_atoms = count_non_hydrogen_atoms(extended_smiles)\n                new_row = row.copy()\n                new_row[\"SMILES\"] = extended_smiles\n                new_row[\"monomer_count\"] = monomer_count\n                new_row[\"original_smiles\"] = original_smiles\n                new_row[\"original_atoms\"] = original_atoms\n                new_row[\"final_atoms\"] = final_atoms\n                results.append((\"success\", new_row))\n        else:\n            return [(\"fail\", (idx, original_smiles, \"No extensions generated\"))]\n\n        # FIX: don't double-wrap\n        return results\n\n    except Exception as e:\n        return [(\"fail\", (idx, original_smiles, str(e)))]\n        \n# ---- Parallel Execution ----\ndef parallel_extend(raw_df):\n    extended_data = []\n    failed_extensions = []\n    counter = 1\n    with Pool(N_PROCESSES) as pool:\n        for results in pool.imap_unordered(process_row, raw_df.iterrows(), chunksize=10):\n            for status, payload in results:\n                if status == \"success\":\n                    extended_data.append(payload)\n                else:\n                    failed_extensions.append(payload)\n            if counter % 100 == 0:\n                clear_output(wait=True)  # clears the current cell output\n                print(f\"Reached mol {counter}/{len(raw_df)}\")\n                print(f\"Successes: {len(extended_data)}\")\n                print(f\"Failures: {len(failed_extensions)}\")\n            counter += 1\n\n    # Build DataFrame\n    mixed_fields=([\"SMILES\", \"monomer_count\", \"original_smiles\", \"original_atoms\", \"final_atoms\"]+PROPERTIES)\n    extended_df = pd.DataFrame(extended_data, columns = mixed_fields)\n    print(f\"\\nSuccessfully extended: {len(extended_df)} molecules\")\n    print(f\"Failed extensions: {len(failed_extensions)}\")\n\n    if failed_extensions:\n        print(\"\\n\\nFailed molecules:\\n\\n\")\n        for idx, smiles, error in failed_extensions[:5]:\n            print(f\"  {idx}: {smiles} - {error}\")\n\n    if len(extended_df) == 0:\n        raise RuntimeError(\"No molecules could be extended. Check your data and extension logic.\")\n\n    return extended_df, failed_extensions\n\n# ---- Run ----\nextended_df, failed_extensions = parallel_extend(raw_df)\nif len(failed_extensions) > 0:\n    print(\"\\n\\nFailed molecules:\\n\\n\")\n    for idx, smiles, error in failed_extensions:  # Show first 5 errors\n        print(f\"  {idx}: {smiles} - {error}\")\n\nprint(extended_df.columns)\n\n# Use extended data for training\nif len(extended_df) == 0:\n    raise RuntimeError(\"No molecules could be extended. Check your data and extension logic.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T15:52:47.612583Z","iopub.execute_input":"2025-09-15T15:52:47.612853Z","iopub.status.idle":"2025-09-15T16:05:24.574994Z","shell.execute_reply.started":"2025-09-15T15:52:47.612829Z","shell.execute_reply":"2025-09-15T16:05:24.574081Z"}},"outputs":[{"name":"stdout","text":"Reached mol 10300/10345\nSuccesses: 59721\nFailures: 162\n\nSuccessfully extended: 59984 molecules\nFailed extensions: 162\n\n\nFailed molecules:\n\n\n  70: */C=C(\\C#N)C(=O)NC1CCCCC1NC(=O)/C(C#N)=C/c1ccc(/C=C/c2ccc(N(CC)Cc3cccc(CN(CC)c4ccc(/C=C/c5ccc(*)s5)cc4)c3)cc2)s1 - Dangling E/Z isomer token for double bond between 15 16.\n  71: */C=C(\\C#N)C(=O)NC1CCCCC1NC(=O)/C(C#N)=C/c1ccc(/C=C/c2ccc(N(CC)Cc3ccccc3CN(CC)c3ccc(/C=C/c4ccc(*)s4)cc3)cc2)s1 - Dangling E/Z isomer token for double bond between 15 16.\n  72: */C=C(\\C#N)C(=O)NC1CCCCC1NC(=O)/C(C#N)=C/c1ccc(/C=C/c2ccc(N(c3ccccc3)c3ccc(N(c4ccccc4)c4ccc(/C=C/c5ccc(*)s5)cc4)cc3)cc2)s1 - Dangling E/Z isomer token for double bond between 15 16.\n  74: */C=C(\\C#N)C(=O)Nc1cccc(NC(=O)/C(C#N)=C/c2ccc(/C=C/c3ccc(N(c4ccccc4)c4ccc(N(c5ccccc5)c5ccc(/C=C/c6ccc(*)s6)cc5)cc4)cc3)s2)c1 - Dangling E/Z isomer token for double bond between 14 15.\n  75: */C=C(\\C#N)C(=O)OCCCCCCCCCCOC(=O)/C(C#N)=C/c1ccc2c(c1)c1cc(*)ccc1n2CCCCCCC - Dangling E/Z isomer token for double bond between 19 20.\n\n\nFailed molecules:\n\n\n  70: */C=C(\\C#N)C(=O)NC1CCCCC1NC(=O)/C(C#N)=C/c1ccc(/C=C/c2ccc(N(CC)Cc3cccc(CN(CC)c4ccc(/C=C/c5ccc(*)s5)cc4)c3)cc2)s1 - Dangling E/Z isomer token for double bond between 15 16.\n  71: */C=C(\\C#N)C(=O)NC1CCCCC1NC(=O)/C(C#N)=C/c1ccc(/C=C/c2ccc(N(CC)Cc3ccccc3CN(CC)c3ccc(/C=C/c4ccc(*)s4)cc3)cc2)s1 - Dangling E/Z isomer token for double bond between 15 16.\n  72: */C=C(\\C#N)C(=O)NC1CCCCC1NC(=O)/C(C#N)=C/c1ccc(/C=C/c2ccc(N(c3ccccc3)c3ccc(N(c4ccccc4)c4ccc(/C=C/c5ccc(*)s5)cc4)cc3)cc2)s1 - Dangling E/Z isomer token for double bond between 15 16.\n  74: */C=C(\\C#N)C(=O)Nc1cccc(NC(=O)/C(C#N)=C/c2ccc(/C=C/c3ccc(N(c4ccccc4)c4ccc(N(c5ccccc5)c5ccc(/C=C/c6ccc(*)s6)cc5)cc4)cc3)s2)c1 - Dangling E/Z isomer token for double bond between 14 15.\n  75: */C=C(\\C#N)C(=O)OCCCCCCCCCCOC(=O)/C(C#N)=C/c1ccc2c(c1)c1cc(*)ccc1n2CCCCCCC - Dangling E/Z isomer token for double bond between 19 20.\n  76: */C=C(\\C#N)C(=O)OCCCCCCCCCCOC(=O)/C(C#N)=C/c1ccc2c(c1)c1cc(*)ccc1n2CCCCCCCCCCCCCC - Dangling E/Z isomer token for double bond between 19 20.\n  77: */C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c1ccc2c(c1)c1cc(*)ccc1n2CCCCCCC - Dangling E/Z isomer token for double bond between 15 16.\n  78: */C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c1ccc2c(c1)c1cc(*)ccc1n2CCCCCCCCCCCCCC - Dangling E/Z isomer token for double bond between 15 16.\n  79: */C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c1ccc2c(c1)c1cc(*)ccc1n2CCCCCCCCCCCOc1ccc(/C=C/c2ccc([N+](=O)[O-])cc2)cc1 - Dangling E/Z isomer token for double bond between 15 16.\n  80: */C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c1ccc2c(c1)c1cc(*)ccc1n2CCCCCCCCCCCn1c2ccccc2c2ccccc21 - Dangling E/Z isomer token for double bond between 15 16.\n  81: */C=C(\\C#N)c1cccc(/C(C#N)=C/c2ccc(/C=C/c3ccc(N(CC)Cc4ccccc4CN(CC)c4ccc(/C=C/c5ccc(*)s5)cc4)cc3)s2)c1 - Dangling E/Z isomer token for double bond between 14 15.\n  82: */C=C(\\C#N)c1cccc(/C(C#N)=C/c2ccc(/C=C/c3ccc(N(c4ccccc4)c4ccc(N(c5ccccc5)c5ccc(/C=C/c6ccc(*)s6)cc5)cc4)cc3)s2)c1 - Dangling E/Z isomer token for double bond between 14 15.\n  105: */C=C/C1=C(*)CC(C(=O)OCC)(C(=O)OCC)C1 - Dangling E/Z isomer token for double bond between 3 4.\n  170: */C=C/c1cc(CCCCCCCCCCCC)c(*)s1 - Dangling E/Z isomer token for double bond between 3 4.\n  186: */C=C/c1cc(OCCCCCCCC)c(/C=C/c2ccc(*)c3nsnc23)cc1OCCCCCCCC - Dangling E/Z isomer token for double bond between 18 19.\n  215: */C=C/c1ccc(/C=C/c2ccc3c(c2)Sc2cc(*)ccc2N3c2ccc(OCCCCCCCCCCCC)cc2)s1 - Dangling E/Z isomer token for double bond between 3 4.\n  216: */C=C/c1ccc(/N=N/c2ccc(N(C)CCOC(=O)Nc3ccc(C)c(NC(=O)OCCN(C)c4ccc(/N=N/c5ccc(/C=C/C6=CC(=C(C#N)C#N)C=C(*)O6)cc5)cc4)c3)cc2)cc1 - Dangling E/Z isomer token for double bond between 47 48.\n  217: */C=C/c1ccc(N(C)CCOC(=O)Nc2ccc(C)c(NC(=O)OCCN(C)c3ccc(/C=C/C4=CC(=C(C#N)C#N)C=C(*)O4)s3)c2)s1 - Dangling E/Z isomer token for double bond between 3 4.\n  218: */C=C/c1ccc(N(CC)Cc2ccccc2CN(CC)c2ccc(/C=C/c3ccc(/C=C(\\C#N)S(=O)(=O)/C(C#N)=C/c4ccc(*)s4)s3)cc2)cc1 - Dangling E/Z isomer token for double bond between 27 28.\n  240: */C=C/c1sc(-c2ccc(-c3sc(/C=C/C4=CC(=C(C#N)C#N)C=C(*)O4)c(CCCCCC)c3CCCCCC)s2)c(CCCCCC)c1CCCCCC - Dangling E/Z isomer token for double bond between 3 49.\n  242: */C=C/c1sc(/C=C/c2cc(CCCCCCCCCCCC)c(*)s2)cc1CCCCCCCCCCCC - Dangling E/Z isomer token for double bond between 3 27.\n  243: */C=C/c1sc(/C=C/c2ccc3c4ccc(*)cc4n(CC(CC)CCCC)c3c2)c(CC(CC)CCCC)c1CC(CC)CCCC - Dangling E/Z isomer token for double bond between 3 39.\n  244: */C=N/c1ccc(C(=O)OCCCCOCCCCOCCCCOC(=O)c2ccc(/N=C/c3ccc(*)s3)cc2)cc1 - Dangling E/Z isomer token for double bond between 33 34.\n  237: */C=C/c1sc(*)c(OCCCCCCCCCCCC)c1OCCCCCCCCCCCC - Dangling E/Z isomer token for double bond between 3 21.\n  238: */C=C/c1sc(-c2ccc(-c3ccc(-c4ccc(-c5sc(/C=C/C6=CC(=C(C#N)C#N)C=C(*)O6)c(CCCCCC)c5CCCCCC)s4)s3)s2)c(CCCCCC)c1CCCCCC - Dangling E/Z isomer token for double bond between 3 59.\n  239: */C=C/c1sc(-c2ccc(-c3ccc(-c4sc(/C=C/C5=CC(=C(C#N)C#N)C=C(*)O5)c(CCCCCC)c4CCCCCC)s3)s2)c(CCCCCC)c1CCCCCC - Dangling E/Z isomer token for double bond between 3 54.\n  275: *=Nc1ccc(/N=C(/C=C/C(=*)c2ccccc2)c2ccccc2)cc1 - Dangling E/Z isomer token for double bond between 10 11.\n  285: *=Nc1cccc(/N=C(/C=C/C(=*)c2ccccc2)c2ccccc2)c1 - Dangling E/Z isomer token for double bond between 11 12.\n  587: *C(=O)Nc1cccc(NC(=O)/C=C/c2cccc(N3C(=O)c4ccc(*)cc4C3=O)c2)c1 - Dangling E/Z isomer token for double bond between 10 11.\n  793: *C(=O)c1ccc2c(c1)C(=O)N(c1cc(/C=N/C(=S)Nc3ccc(N4C(=O)c5ccc(*)cc5C4=O)cc3)ccc1Cl)C2=O - Dangling E/Z isomer token for double bond between 17 18.\n  1039: *C/C=C/COC(=O)C(Cc1ccccc1)NC(=O)/C=C/C(=O)NC(Cc1ccccc1)C(=O)O* - Dangling E/Z isomer token for double bond between 17 18.\n  1371: *CC(*)(C)C(=O)Nc1ccc(C(=O)/C=C/c2ccc(OC)cc2)cc1 - Dangling E/Z isomer token for double bond between 12 13.\n  1512: *CC(*)(C)C(=O)OCCCCCCCCCCOc1ccc(C(=O)Oc2ccc(C(=O)/C=C/c3c(C)c4ccccc4n3C)cc2)cc1 - Dangling E/Z isomer token for double bond between 30 31.\n  1513: *CC(*)(C)C(=O)OCCCCCCCCCCOc1ccc(C(=O)Oc2ccc(C(=O)/C=C/c3c(C)c4ccccc4n3CCCC)cc2)cc1 - Dangling E/Z isomer token for double bond between 30 31.\n  1533: *CC(*)(C)C(=O)OCCCCCCCCCCn1c2ccccc2c2cc(/C=C/C3=C(C#N)C(=C(C#N)C#N)OC3(C)C)ccc21 - Dangling E/Z isomer token for double bond between 30 31.\n  1544: *CC(*)(C)C(=O)OCCCCCCCCOc1ccc(C(=O)Oc2ccc(C(=O)/C=C/c3c(C)c4ccccc4n3C)cc2)cc1 - Dangling E/Z isomer token for double bond between 28 29.\n  1545: *CC(*)(C)C(=O)OCCCCCCCCOc1ccc(C(=O)Oc2ccc(C(=O)/C=C/c3c(C)c4ccccc4n3CCCC)cc2)cc1 - Dangling E/Z isomer token for double bond between 28 29.\n  1581: *CC(*)(C)C(=O)OCCCCCCOc1ccc(C(=O)Oc2ccc(C(=O)/C=C/c3c(C)c4ccccc4n3C)cc2)cc1 - Dangling E/Z isomer token for double bond between 26 27.\n  1582: *CC(*)(C)C(=O)OCCCCCCOc1ccc(C(=O)Oc2ccc(C(=O)/C=C/c3c(C)c4ccccc4n3CCCC)cc2)cc1 - Dangling E/Z isomer token for double bond between 26 27.\n  1675: *CC(*)(C)C(=O)OCCN(CC)c1ccc(/N=N/c2nc3ccc([N+](=O)[O-])cc3s2)cc1 - Dangling E/Z isomer token for double bond between 19 20.\n  1739: *CC(*)(C)C(=O)OCc1ccc(/C=C/C(=O)C2c3ccccc3C(=O)c3ccccc32)cc1 - Dangling E/Z isomer token for double bond between 15 16.\n  1740: *CC(*)(C)C(=O)OCc1ccc(/C=C/C(=O)Oc2ccc3oc(=O)ccc3c2)cc1 - Dangling E/Z isomer token for double bond between 15 16.\n  1784: *CC(*)(C)C(=O)Oc1ccc(C(=O)/C=C/c2ccc(Cl)cc2)cc1 - Dangling E/Z isomer token for double bond between 12 13.\n  1785: *CC(*)(C)C(=O)Oc1ccc(C(=O)/C=C/c2ccc(OC)c(OC)c2)cc1 - Dangling E/Z isomer token for double bond between 12 13.\n  1786: *CC(*)(C)C(=O)Oc1ccc(C(=O)/C=C/c2ccc(OCc3ccccc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 12 13.\n  1952: *CC(*)C(=O)Nc1ccc(C(=O)/C=C/c2ccc(Br)cc2)cc1 - Dangling E/Z isomer token for double bond between 11 12.\n  1953: *CC(*)C(=O)Nc1ccc(C(=O)/C=C/c2cccc(Br)c2)cc1 - Dangling E/Z isomer token for double bond between 11 12.\n  2229: *CC(*)OC(=O)/C=C/c1ccccc1 - Dangling E/Z isomer token for double bond between 5 6.\n  2805: *CC(COc1ccc(C(C)(C)c2ccc(O*)cc2)cc1)OC(=O)/C=C/c1ccccc1 - Dangling E/Z isomer token for double bond between 23 24.\n  2806: *CC(COc1ccc(C(C)(C)c2ccc(O*)cc2)cc1)OC(=O)/C=C/c1ccco1 - Dangling E/Z isomer token for double bond between 23 24.\n  2900: *CC(O)COC(=O)/C=C/C(=O)Oc1ccc(C(C)(C)c2ccc(O*)cc2)cc1 - Dangling E/Z isomer token for double bond between 6 7.\n  2901: *CC(O)COC(=O)/C=C\\C(=O)Oc1ccc(C(C)(C)c2ccc(O*)cc2)cc1 - Dangling E/Z isomer token for double bond between 6 7.\n  3080: *CC1CCC(COP(=O)(/N=N/c2ccc(-c3ccc(/N=N/P(=O)(O*)OC)cc3)cc2)OC)CC1 - Dangling E/Z isomer token for double bond between 8 9.\n  3081: *CC1CCC(COP(=O)(/N=N/c2ccc(C(=O)c3ccc(/N=N/P(=O)(O*)OC)cc3)cc2)OC)CC1 - Dangling E/Z isomer token for double bond between 8 9.\n  3082: *CC1CCC(COP(=O)(/N=N/c2ccc(Oc3ccc(/N=N/P(=O)(O*)OC)cc3)cc2)OC)CC1 - Dangling E/Z isomer token for double bond between 8 9.\n  3496: *CCCCCCCCCCNC(=O)/C=C(\\C)C(=O)N* - Dangling E/Z isomer token for double bond between 12 13.\n  3603: *CCCCCCCCCCn1c2ccccc2c2cc(/C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c3ccc4c(c3)c3ccccc3n4*)ccc21 - Dangling E/Z isomer token for double bond between 35 36.\n  3634: *CCCCCCCCCn1c2ccccc2c2cc(/C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c3ccc4c(c3)c3ccccc3n4*)ccc21 - Dangling E/Z isomer token for double bond between 34 35.\n  3709: *CCCCCCCCn1c2ccccc2c2cc(/C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c3ccc4c(c3)c3ccccc3n4*)ccc21 - Dangling E/Z isomer token for double bond between 33 34.\n  3735: *CCCCCCCn1c2ccccc2c2cc(/C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c3ccc4c(c3)c3ccccc3n4*)ccc21 - Dangling E/Z isomer token for double bond between 32 33.\n  3761: *CCCCCCNC(=O)/C=C(\\C)C(=O)N* - Dangling E/Z isomer token for double bond between 8 9.\n  3762: *CCCCCCNC(=O)/C=C/CC/C=C/C(=O)N* - Dangling E/Z isomer token for double bond between 8 9.\n  3836: *CCCCCCOC(=O)C(Cc1ccccc1)NC(=O)/C=C/C(=O)NC(Cc1ccccc1)C(=O)O* - Dangling E/Z isomer token for double bond between 19 20.\n  3910: *CCCCCCOP(=O)(/N=N/c1ccc(CCOC(=O)c2cc(C(=O)OCCc3ccc(/N=N/P(=O)(O*)OC)cc3)cc(C(C)(C)C)c2)cc1)OC - Dangling E/Z isomer token for double bond between 8 9.\n  3911: *CCCCCCOP(=O)(/N=N/c1ccc(COC(=O)c2cc(C(=O)OCc3ccc(/N=N/P(=O)(O*)OC)cc3)cc(C(C)(C)C)c2)cc1)OC - Dangling E/Z isomer token for double bond between 8 9.\n  3912: *CCCCCCOP(=O)(/N=N/c1ccc(Oc2ccc(/N=N/P(=O)(O*)OC)cc2)cc1)OC - Dangling E/Z isomer token for double bond between 8 9.\n  3908: *CCCCCCOP(=O)(/N=N/c1ccc(-c2ccc(/N=N/P(=O)(O*)OC)cc2)cc1)OC - Dangling E/Z isomer token for double bond between 8 9.\n  3909: *CCCCCCOP(=O)(/N=N/c1ccc(C(=O)c2ccc(/N=N/P(=O)(O*)OC)cc2)cc1)OC - Dangling E/Z isomer token for double bond between 8 9.\n  3959: *CCCCCCn1c2ccccc2c2cc(/C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c3ccc4c(c3)c3ccccc3n4*)ccc21 - Dangling E/Z isomer token for double bond between 31 32.\n  4000: *CCCCCOC(=O)c1ccccc1C(=O)O[Pb]OC(=O)c1ccccc1C(=O)OCCCCCOC(=O)NCCCCCCNC(=O)O* - 'Node 18 ([Pb]) has non-standard valence: ...(=O)O[Pb]OC(=O...'\n  4001: *CCCCCOC(=O)c1ccccc1C(=O)O[Pb]OC(=O)c1ccccc1C(=O)OCCCCCOC(=O)Nc1ccc(C)c(NC(=O)O*)c1 - 'Node 18 ([Pb]) has non-standard valence: ...(=O)O[Pb]OC(=O...'\n  4004: *CCCCCOc1ccc(/C=C/C(=O)OCCN(CCOC(=O)/C=C/c2ccc(O*)cc2)c2ccc(/N=N/c3ccc([N+](=O)[O-])cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 13 14.\n  4040: *CCCCOC(=O)/C=C\\C(=O)O* - Dangling E/Z isomer token for double bond between 6 7.\n  4041: *CCCCOC(=O)C(Cc1ccccc1)NC(=O)/C=C/C(=O)NC(Cc1ccccc1)C(=O)O* - Dangling E/Z isomer token for double bond between 17 18.\n  4039: *CCCCOC(=O)/C=C/C(=O)O* - Dangling E/Z isomer token for double bond between 6 7.\n  4065: *CCCCOC(=O)Nc1ccc(C)c(NC(=O)OCCCCn2c3ccccc3c3cc(/C=C(\\C#N)C(=O)OCCCCCCOC(=O)/C(C#N)=C/c4ccc5c(c4)c4ccccc4n5*)ccc32)c1 - Dangling E/Z isomer token for double bond between 47 48.\n  4278: *CCN(CCN1C(=O)c2ccc(C(c3ccc4c(c3)C(=O)N(*)C4=O)(C(F)(F)F)C(F)(F)F)cc2C1=O)c1ccc(/C=C/C2=CC(=C(C#N)C#N)CC(C)(C)C2)cc1 - Dangling E/Z isomer token for double bond between 44 45.\n  4293: *CCN(CCOC(=O)/C=C/c1ccc(/C=C/C(=O)O*)cc1)c1ccc(/N=N/c2ccc([N+](=O)[O-])cc2)cc1 - Dangling E/Z isomer token for double bond between 7 8.\n  4326: *CCN(CCOC(=O)Nc1ccc(C)c(NC(=O)O*)c1)c1ccc(/N=N/c2ccc(/C=C/c3nc4ccc([N+](=O)[O-])cc4n3CC)cc2)cc1 - Dangling E/Z isomer token for double bond between 34 35.\n  4365: *CCN(CCOC(=O)c1cc(OCc2c(F)c(F)c(OC)c(F)c2F)cc(C(=O)O*)c1)c1ccc(/C=C/c2ccc(/C=C/C3=C(C#N)C(=C(C#N)C#N)OC3(c3ccccc3)C(F)(F)F)s2)cc1 - Dangling E/Z isomer token for double bond between 39 40.\n  4370: *CCN(CCOC(=O)c1ccc(C(=O)O*)c(OC)c1)c1ccc(/N=N/c2ccc(/C=C/c3nc4ccc([N+](=O)[O-])cc4n3CC)cc2)cc1 - Dangling E/Z isomer token for double bond between 33 34.\n  4376: *CCN(CCOC(=O)c1ccc(C(=O)O*)c(OCCC)c1)c1ccc(/N=N/c2nc(C#N)c(C#N)[nH]2)cc1 - Dangling E/Z isomer token for double bond between 29 30.\n  4380: *CCN(CCOC(=O)c1ccc(C(=O)O*)c(OCCCCC)c1)c1ccc(/N=N/c2nc(C#N)c(C#N)[nH]2)cc1 - Dangling E/Z isomer token for double bond between 31 32.\n  4420: *CCOC(=O)/C=C/C(=O)NCCCCCCNC(=O)/C=C/C(=O)O* - Dangling E/Z isomer token for double bond between 4 5.\n  4421: *CCOC(=O)/C=C/C(=O)O* - Dangling E/Z isomer token for double bond between 4 5.\n  4457: *CCOC(=O)Nc1ccc(C)c(NC(=O)OCCOc2cc([N+](=O)[O-])ccc2/N=N/c2cn(*)c3ccc(-c4ccc5c(c4)c4ccccc4n5CCCC)cc23)c1 - Dangling E/Z isomer token for double bond between 31 32.\n  4458: *CCOC(=O)Nc1ccc(C)c(NC(=O)OCCOc2cc([N+](=O)[O-])ccc2/N=N/c2cn(*)c3ccc(-c4ccccc4)cc23)c1 - Dangling E/Z isomer token for double bond between 31 32.\n  4459: *CCOC(=O)Nc1ccc(C)c(NC(=O)OCCOc2cc([N+](=O)[O-])ccc2/N=N/c2cn(*)c3ccccc23)c1 - Dangling E/Z isomer token for double bond between 31 32.\n  4492: *CCOCCOC(=O)/C=C/C(=O)NCCCCCCNC(=O)/C=C/C(=O)O* - Dangling E/Z isomer token for double bond between 7 8.\n  4493: *CCOCCOC(=O)/C=C/C(=O)O* - Dangling E/Z isomer token for double bond between 7 8.\n  4528: *CCOCCOCCOC(=O)/C=C/C(=O)NCCCCCCNC(=O)/C=C/C(=O)O* - Dangling E/Z isomer token for double bond between 10 11.\n  4568: *CCOCCOP(=O)(/N=N/c1ccc(-c2ccc(/N=N/P(=O)(O*)OC)cc2)cc1)OC - Dangling E/Z isomer token for double bond between 7 8.\n  4569: *CCOCCOP(=O)(/N=N/c1ccc(C(=O)c2ccc(/N=N/P(=O)(O*)OC)cc2)cc1)OC - Dangling E/Z isomer token for double bond between 7 8.\n  4542: *CCOCCOCCOCCOC(=O)/C=C/C(=O)NCCCCCCNC(=O)/C=C/C(=O)O* - Dangling E/Z isomer token for double bond between 13 14.\n  4570: *CCOCCOP(=O)(/N=N/c1ccc(Oc2ccc(/N=N/P(=O)(O*)OC)cc2)cc1)OC - Dangling E/Z isomer token for double bond between 7 8.\n  4584: *CCOP(=O)(/N=N/c1ccc(-c2ccc(/N=N/P(=O)(O*)OCC)cc2)cc1)OCC - Dangling E/Z isomer token for double bond between 4 5.\n  4585: *CCOP(=O)(/N=N/c1ccc(C(=O)c2ccc(/N=N/P(=O)(O*)OCC)cc2)cc1)OCC - Dangling E/Z isomer token for double bond between 4 5.\n  4586: *CCOP(=O)(/N=N/c1ccc(CCOC(=O)c2cc(C(=O)OCCc3ccc(/N=N/P(=O)(O*)OC)cc3)cc(C(C)(C)C)c2)cc1)OC - Dangling E/Z isomer token for double bond between 4 5.\n  4587: *CCOP(=O)(/N=N/c1ccc(COC(=O)c2cc(C(=O)OCc3ccc(/N=N/P(=O)(O*)OC)cc3)cc(C(C)(C)C)c2)cc1)OC - Dangling E/Z isomer token for double bond between 4 5.\n  4588: *CCOP(=O)(/N=N/c1ccc(Oc2ccc(/N=N/P(=O)(O*)OCC)cc2)cc1)OCC - Dangling E/Z isomer token for double bond between 4 5.\n  4751: *Cc1ccc(COP(=O)(/N=N/c2ccc(-c3ccc(/N=N/P(=O)(O*)OC)cc3)cc2)OC)cc1 - Dangling E/Z isomer token for double bond between 8 9.\n  4752: *Cc1ccc(COP(=O)(/N=N/c2ccc(C(=O)c3ccc(/N=N/P(=O)(O*)OC)cc3)cc2)OC)cc1 - Dangling E/Z isomer token for double bond between 8 9.\n  4753: *Cc1ccc(COP(=O)(/N=N/c2ccc(Oc3ccc(/N=N/P(=O)(O*)OC)cc3)cc2)OC)cc1 - Dangling E/Z isomer token for double bond between 8 9.\n  4780: *Cc1ccccc1CN(CC)c1ccc(/C=C(\\C#N)C(=O)Nc2cccc(NC(=O)/C(C#N)=C/c3ccc(N(*)CC)cc3)c2)cc1 - Dangling E/Z isomer token for double bond between 29 30.\n  4781: *Cc1ccccc1CN(CC)c1ccc(/C=C/c2ccc(/C=C(\\C#N)C(=O)NC3CCCCC3NC(=O)/C(C#N)=C/c3ccc(/C=C/c4ccc(N(*)CC)cc4)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 36 37.\n  4770: *Cc1cccc(CNC(=O)/C=C(\\C)C(=O)N*)c1 - Dangling E/Z isomer token for double bond between 9 10.\n  4795: *N/C1=N/OC(O)(O)O/N=C(/N*)C/C(N)=N/OC(O)(O)O/N=C(\\N)C1 - Conflicting cis/trans assignment for ligands on node 10.\n  4905: *N=P(Cl)(Cl)N=P(Cl)(Cl)/N=S(/*)Cl - Dangling E/Z isomer token for double bond between 5 6.\n  4907: *N=P(Cl)(N=P(/N=S(/*)Oc1cccc(-c2ccccc2)c1)(Oc1cccc(-c2ccccc2)c1)Oc1cccc(-c2ccccc2)c1)Oc1cccc(-c2ccccc2)c1 - Dangling E/Z isomer token for double bond between 4 5.\n  4908: *N=P(Cl)(N=P(Cl)(/N=S(/*)Oc1ccccc1-c1ccccc1)Oc1ccccc1-c1ccccc1)Oc1ccccc1-c1ccccc1 - Dangling E/Z isomer token for double bond between 4 5.\n  4909: *N=P(N=P(/N=S(/*)Oc1ccc(-c2ccccc2)cc1)(Oc1ccc(-c2ccccc2)cc1)Oc1ccc(-c2ccccc2)cc1)(Oc1ccc(-c2ccccc2)cc1)Oc1ccc(-c2ccccc2)cc1 - Dangling E/Z isomer token for double bond between 3 4.\n  4910: *N=P(N=P(/N=S(/*)Oc1ccc(C(C)(C)C)cc1)(Oc1ccc(-c2ccccc2)cc1)Oc1ccc(-c2ccccc2)cc1)(Oc1ccc(-c2ccccc2)cc1)Oc1ccc(-c2ccccc2)cc1 - Dangling E/Z isomer token for double bond between 3 4.\n  4920: *NC(=C)/C=C\\C(=C/C)C1(c2ccc(N*)cc2)CCCCC1 - Dangling E/Z isomer token for double bond between 2 3.\n  4955: *NC(N*)=C(N)/C(N)=N/C - Dangling E/Z isomer token for double bond between 2 5.\n  4961: *NC1=C(/C=C/c2ccc(N*)cc2)C=C(Cc2ccccc2)C(N)(N)[C@@H]1Cc1ccccc1 - Dangling E/Z isomer token for double bond between 2 3.\n  5066: *NNC(=O)/C=C/C(=O)NCCCCCCNC(=O)/C=C/C(*)=O - Dangling E/Z isomer token for double bond between 3 4.\n  5067: *NNC(=O)/C=C/C(=O)Nc1cccc(/C=C2\\CC/C(=C\\c3cccc(NC(=O)/C=C/C(*)=O)c3)C2=O)c1 - Dangling E/Z isomer token for double bond between 3 4.\n  5068: *NNC(=O)/C=C/C(=O)Nc1cccc(/C=C2\\CCC/C(=C\\c3cccc(NC(=O)/C=C/C(*)=O)c3)C2=O)c1 - Dangling E/Z isomer token for double bond between 3 4.\n  5282: *Nc1cc(NC(=O)c2ccc(-c3ccc(C(*)=O)cc3)cc2)cc(C(=O)OCCOc2ccc(/C=C/C(=O)c3ccccc3)cc2)c1 - Dangling E/Z isomer token for double bond between 37 38.\n  5288: *Nc1cc(NC(=O)c2ccc(C(*)=O)cc2)cc(C(=O)OCCOC(=O)/C=C/c2ccc(N(C)C)cc2)c1 - Dangling E/Z isomer token for double bond between 25 26.\n  5290: *Nc1cc(NC(=O)c2ccc(C(*)=O)cc2)cc(C(=O)OCCOc2ccc(/C=C/C(=O)c3ccccc3)cc2)c1 - Dangling E/Z isomer token for double bond between 31 32.\n  5295: *Nc1cc(NC(=O)c2ccc3cc(C(*)=O)ccc3c2)cc(C(=O)OCCOc2ccc(/C=C/C(=O)c3ccccc3)cc2)c1 - Dangling E/Z isomer token for double bond between 35 36.\n  5299: *Nc1cc(NC(=O)c2cccc(C(*)=O)c2)cc(C(=O)OCCOC(=O)/C=C/c2ccc(N(C)C)cc2)c1 - Dangling E/Z isomer token for double bond between 25 26.\n  5301: *Nc1cc(NC(=O)c2cccc(C(*)=O)c2)cc(C(=O)OCCOc2ccc(/C=C/C(=O)c3ccccc3)cc2)c1 - Dangling E/Z isomer token for double bond between 31 32.\n  5908: *Nc1ccc(C2=C(c3ccccc3)/C(=C/c3ccccc3)[C@@H](N*)/C(=C/c3ccccc3)C2)cc1 - Dangling E/Z isomer token for double bond between 6 7.\n  5996: *Nc1ccc(Cc2ccc(NC(=O)/C=C/c3ccc(/C=C/C(*)=O)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 12 13.\n  6115: *Nc1ccc([C@@H](C)/C=C2/C=CC/C(=C\\[C@H](C)c3ccc(N*)cc3)C2)cc1 - Dangling E/Z isomer token for double bond between 10 11.\n  6242: *Nc1ccc2c(c1)C(/C(C)=C/C(=C)C(=C)c1ccccc1)(C(/C=C(\\C)C(=C)c1ccccc1C)=C/C)c1cc(N*)ccc1-2 - Dangling E/Z isomer token for double bond between 12 13.\n  6576: *OC(=O)/C=C/C(=O)OC1COC2C(*)COC12 - Dangling E/Z isomer token for double bond between 2 3.\n  6605: *OC(=O)Nc1ccc(C)c(NC(=O)OC2CCN(c3ccc(/C=C/C4=CC(=C(C#N)C#N)C=C(/C=C/c5ccc(N6CCC(*)CC6)s5)O4)s3)CC2)c1 - Dangling E/Z isomer token for double bond between 21 22.\n  6661: *OC(C)COC(=O)/C=C/C(*)=O - Dangling E/Z isomer token for double bond between 6 7.\n  6662: *OC(C)COC(=O)/C=C\\C(*)=O - Dangling E/Z isomer token for double bond between 6 7.\n  7074: *O[Sn](CCCC)(CCCC)OC(=O)/C=C/C(*)=O - Dangling E/Z isomer token for double bond between 12 13.\n  7226: *Oc1ccc(-c2ccc(OC(=O)/C=C/c3ccc(/C=C/C(*)=O)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 11 12.\n  7281: *Oc1ccc(/C=C/C(=O)c2ccc(Oc3ccc(C(C)(C)c4ccc(*)c(C)c4)cc3C)cc2)cc1 - Dangling E/Z isomer token for double bond between 8 9.\n  7282: *Oc1ccc(/C=C/C(=O)c2ccc(Oc3ccc(C(C)(C)c4ccc(*)cc4)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 8 9.\n  7283: *Oc1ccc(/C=C/C(=O)c2ccc(Oc3ccc(C(C)(c4ccccc4)c4ccc(*)cc4)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 8 9.\n  7342: *Oc1ccc(C(=O)Nc2cc(NC(=O)c3ccc(*)cc3)cc(C(=O)OCCOC(=O)/C=C/c3ccc(N(C)C)cc3)c2)cc1 - Dangling E/Z isomer token for double bond between 30 31.\n  7365: *Oc1ccc(C(=O)OCCN(CCOC(=O)c2ccc(OC3(F)C(*)(F)C(F)(F)C3(F)F)cc2)c2ccc(/C=C/C3=CC(=C(C#N)C#N)CC(C)(C)C3)cc2)cc1 - Dangling E/Z isomer token for double bond between 41 42.\n  7654: *Oc1ccc(C2(c3ccc(Oc4ccc(/C=C/C(=O)c5ccc(*)cc5)cc4)cc3)c3ccccc3-c3ccccc32)cc1 - Dangling E/Z isomer token for double bond between 18 19.\n  7825: *Oc1ccc(N(CC)c2ccc(/C=C(\\C#N)C(=O)NC3CCCCC3NC(=O)/C(C#N)=C/c3ccc(N(CC)c4ccc(*)cc4)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 27 28.\n  7858: *Oc1ccc(NC(=O)/C=C/C(=O)NNC(=O)/C=C/C(=O)Nc2ccc(OC(=O)/C=C/C(=O)NNC(=O)/C=C/C(*)=O)cc2)cc1 - Dangling E/Z isomer token for double bond between 7 8.\n  7859: *Oc1ccc(NC(=O)/C=C/C(=O)NNC(=O)/C=C/C(=O)Nc2ccc(OC(=O)c3ccccc3C(=O)NNC(=O)c3ccccc3C(*)=O)cc2)cc1 - Dangling E/Z isomer token for double bond between 7 8.\n  7860: *Oc1ccc(NC(=O)/C=C/c2ccc(/C=C/C(=O)Nc3ccc(*)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 7 8.\n  7949: *Oc1ccc(OC(=O)/C=C/c2ccc(/C=C/C(*)=O)cc2)cc1 - Dangling E/Z isomer token for double bond between 7 8.\n  8532: *c1cc(/C=N/C(=S)Nc2ccc(-n3c(=O)c4cc5c(=O)n(*)c(=O)c5cc4c3=O)cc2)ccc1Cl - Dangling E/Z isomer token for double bond between 6 7.\n  8908: *c1ccc(/C=C(\\C#N)C(=O)NC2CCCCC2NC(=O)/C(C#N)=C/c2ccc(N(c3ccccc3)c3ccc(N(*)c4ccccc4)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 19 20.\n  8955: *c1ccc(C(=O)OCCCCCCCCOc2ccc(/C=C/C(=O)/C=C/c3ccc(OCCCCCCCCOC(=O)c4ccc(-c5nnc(*)o5)cc4)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 23 24.\n  8965: *c1ccc(C(=O)OCCCCCCOc2ccc(/C=C/C(=O)/C=C/c3ccc(OCCCCCCOC(=O)c4ccc(-c5nnc(*)o5)cc4)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 21 22.\n  8947: *c1ccc(C(=O)OCCCCCCCCCCOc2ccc(/C=C/C(=O)/C=C/c3ccc(OCCCCCCCCCCOC(=O)c4ccc(-c5nnc(*)o5)cc4)cc3)cc2)cc1 - Dangling E/Z isomer token for double bond between 25 26.\n  9350: *c1ccc(Oc2ccc(-c3csc(/N=C/c4ccc(OCCCCCCCCOc5ccc(/C=N/c6nc(*)cs6)cc5OC)c(OC)c4)n3)cc2)cc1 - Dangling E/Z isomer token for double bond between 13 50.\n  9351: *c1ccc(Oc2ccc(-c3csc(/N=C/c4ccc(OCCCCCCOc5ccc(/C=N/c6nc(*)cs6)cc5)cc4)n3)cc2)cc1 - Dangling E/Z isomer token for double bond between 13 44.\n  9352: *c1ccc(Oc2ccc(-c3csc(/N=C/c4ccc(OCCCCCCOc5ccc(/C=N/c6nc(*)cs6)cc5OC)c(OC)c4)n3)cc2)cc1 - Dangling E/Z isomer token for double bond between 13 48.\n  9353: *c1ccc(Oc2ccc(-c3csc(/N=C/c4ccc(OCCCCOc5ccc(/C=N/c6nc(*)cs6)cc5OC)c(OC)c4)n3)cc2)cc1 - Dangling E/Z isomer token for double bond between 13 46.\n  9354: *c1ccc(Oc2ccc(-c3csc(/N=C/c4ccc(OCCOc5ccc(/C=N/c6nc(*)cs6)cc5OC)c(OC)c4)n3)cc2)cc1 - Dangling E/Z isomer token for double bond between 13 44.\n  9348: *c1ccc(Oc2ccc(-c3csc(/N=C/c4ccc(OCCCCCCCCCCOc5ccc(/C=N/c6nc(*)cs6)cc5OC)c(OC)c4)n3)cc2)cc1 - Dangling E/Z isomer token for double bond between 13 52.\n  9349: *c1ccc(Oc2ccc(-c3csc(/N=C/c4ccc(OCCCCCCCCOc5ccc(/C=N/c6nc(*)cs6)cc5)cc4)n3)cc2)cc1 - Dangling E/Z isomer token for double bond between 13 46.\n  9883: *c1ccc2c(c1)C(=O)N(c1ccc(-c3ccc(N4C(=O)c5ccc(C(*)(C(F)(F)F)C(F)(F)F)cc5C4=O)c(OC(=O)/C=C/c4ccccc4)c3)cc1OC(=O)/C=C/c1ccccc1)C2=O - Dangling E/Z isomer token for double bond between 41 42.\n  9922: *c1ccc2c(c1)C(=O)N(c1ccc(NC(=S)/N=C/c3ccc(Cl)c(N4C(=O)c5ccc(C(*)(C(F)(F)F)C(F)(F)F)cc5C4=O)c3)cc1)C2=O - Dangling E/Z isomer token for double bond between 15 16.\n  10059: *c1ccc2c(c1)Sc1cc(-c3sc(/C=C/C4=CC(=C(C#N)C#N)C=C(/C=C/c5sc(*)c(CCCCCC)c5CCCCCC)O4)c(CCCCCC)c3CCCCCC)ccc1N2CCCCCC - Dangling E/Z isomer token for double bond between 13 47.\n  10162: *c1cccc(N/C=C/C(=O)c2cccc(C(=O)/C=C/Nc3cccc(S(*)(=O)=O)c3)c2)c1 - Dangling E/Z isomer token for double bond between 9 10.\n  10163: *c1cccc(N/C=C\\C(=O)c2cccc(C(=O)/C=C\\Nc3cccc(S(*)(=O)=O)c3)c2)c1 - Dangling E/Z isomer token for double bond between 9 10.\nIndex(['SMILES', 'monomer_count', 'original_smiles', 'original_atoms',\n       'final_atoms', 'Tg', 'FFV', 'Tc', 'Density', 'Rg'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!pip install geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T16:05:24.575924Z","iopub.execute_input":"2025-09-15T16:05:24.576379Z","iopub.status.idle":"2025-09-15T16:05:30.577006Z","shell.execute_reply.started":"2025-09-15T16:05:24.576353Z","shell.execute_reply":"2025-09-15T16:05:30.576049Z"}},"outputs":[{"name":"stdout","text":"Collecting geometric\n  Downloading geometric-1.1.tar.gz (386 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.0/386.0 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.11/dist-packages (from geometric) (1.26.4)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from geometric) (3.5)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from geometric) (1.17.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from geometric) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.11->geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.11->geometric) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.11->geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.11->geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.11->geometric) (2024.2.0)\nBuilding wheels for collected packages: geometric\n  Building wheel for geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for geometric: filename=geometric-1.1-py3-none-any.whl size=402087 sha256=b1c403d712b42bd97a693e36d91c94ec55385c20bb13a2999353e648fad9e4e4\n  Stored in directory: /root/.cache/pip/wheels/c5/77/0c/96a54539fe0560749fdbe283f92582d09bddb212856407a4cb\nSuccessfully built geometric\nInstalling collected packages: geometric\nSuccessfully installed geometric-1.1\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"from dataset_helpers import smiles_iter_to_graph_dataset\n\nimport numpy as np\nfrom torch_geometric.loader import DataLoader\n\n# Split into Train/Val\nnum_rows = len(extended_df)\nperm = np.random.RandomState(SEED).permutation(num_rows)\ntrain_count = int((1.0 - VAL_RATIO) * num_rows)\ntrain_idx, val_idx = perm[:train_count], perm[train_count:]\ntrain_df = extended_df.iloc[train_idx].reset_index(drop=True)\nval_df = extended_df.iloc[val_idx].reset_index(drop=True)\n\n# Create graph datasets\ntrain_dataset = smiles_iter_to_graph_dataset(train_df[\"SMILES\"], torch.tensor(train_df[PROPERTIES].to_numpy(), dtype=torch.float32, device=device), aux_info=train_df[AUX_INFO])\nval_dataset = smiles_iter_to_graph_dataset(val_df[\"SMILES\"], torch.tensor(val_df[PROPERTIES].to_numpy(), dtype=torch.float32, device=device), aux_info=val_df[AUX_INFO])\n\nprint(f\"example aux_info: {train_dataset[0].aux_info}\")\nprint(f\"Train graphs: {len(train_dataset)} | Val graphs: {len(val_dataset)}\")\n\n# DataListLoader (batched lists of Data objects)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Derive input dimension from first training graph\nif len(train_dataset) == 0:\n    raise RuntimeError(\"Training dataset is empty after preprocessing.\")\nINPUT_DIM = train_dataset[0].x.shape[1]\nprint(f\"Input dim: {INPUT_DIM}, Target dim: {TARGET_DIM}\")\n\nEDGE_DIM = train_dataset[0].edge_attr.shape[1]\nprint(f\"Edge dim: {EDGE_DIM}\")\n\n# Show some examples of the extensions\nprint(f\"\\nExamples of SMILES extensions:\")\nfor i in range(min(3, len(train_df))):\n    row = train_df.iloc[i]\n    print(f\"Original ({row.get('original_atoms', 'N/A')} atoms): {row.get('original_smiles', 'N/A')}\")\n    print(f\"Extended ({row.get('final_atoms', 'N/A')} atoms): {row['SMILES']}\")\n    print()\n\nTOTAL_BATCH_COUNT = len(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T16:05:30.578357Z","iopub.execute_input":"2025-09-15T16:05:30.578684Z","iopub.status.idle":"2025-09-15T16:07:27.066458Z","shell.execute_reply.started":"2025-09-15T16:05:30.578653Z","shell.execute_reply":"2025-09-15T16:07:27.065598Z"}},"outputs":[{"name":"stdout","text":"example aux_info: [4 11 46\n 'CC(CCC(CC(CC(C)OC(=O)c1ccccc1)OC(=O)c1ccccc1)OC(=O)c1ccccc1)OC(=O)c1ccccc1']\nTrain graphs: 53985 | Val graphs: 5999\nInput dim: 6, Target dim: 5\nEdge dim: 4\n\nExamples of SMILES extensions:\nOriginal (11 atoms): *CC(*)OC(=O)c1ccccc1\nExtended (46 atoms): CC(CCC(CC(CC(C)OC(=O)c1ccccc1)OC(=O)c1ccccc1)OC(=O)c1ccccc1)OC(=O)c1ccccc1\n\nOriginal (59 atoms): *C(=O)Nc1ccc(C2(c3ccc(NC(=O)c4ccc5c(c4)C(=O)N(c4cccc(N6C(=O)c7ccc(*)cc7C6=O)c4)C5=O)cc3)c3ccccc3-c3ccccc32)cc1\nExtended (120 atoms): Cc1ccc2c(c1)C(=O)N(c1cccc(N3C(=O)c4ccc(C(=O)Nc5ccc(C6(c7ccc(NC(=O)C(=O)Nc8ccc(C9(c%10ccc(NC(=O)c%11ccc%12c(c%11)C(=O)N(c%11cccc(N%13C(=O)c%14ccc(C)cc%14C%13=O)c%11)C%12=O)cc%10)c%10ccccc%10-c%10ccccc%109)cc8)cc7)c7ccccc7-c7ccccc76)cc5)cc4C3=O)c1)C2=O\n\nOriginal (15 atoms): */C=C/[Si](*)(c1ccccc1)c1ccccc1\nExtended (62 atoms): C[Si](C=C[Si](C=CC=C[Si](C=C[Si](C)(c1ccccc1)c1ccccc1)(c1ccccc1)c1ccccc1)(c1ccccc1)c1ccccc1)(c1ccccc1)c1ccccc1\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import kmeans_hrm_model\n# Use KMeansHRMModule from kmeans_hrm_model.py\nfrom kmeans_hrm_model import (\n    KMeansHRMModule, KMeansHRMConfig, KMeansHRMInnerModuleConfig, KMeansHRMInitialCarry,\n    KMeansConfig, KMeansHeadConfig, OutputHeadConfig,\n    SpectralWeighting, SpectralWeightingConfig,\n    DiscreteMeanCenter, DiscreteMeanCenterConfig,\n    RadiusAttentionWeights, RadiusMaskConfig\n)\nfrom torch.optim import Adam\nfrom torch_geometric.nn import GATConv\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_kmeans_hrm_config(input_dim: int, edge_dim: int, hidden_dim: int = 512, latent_dim: int = 128, output_dim: int = TARGET_DIM, k: int = 8) -> KMeansHRMConfig:\n    \n    # Spectral Weighting Configuration\n    spectral_config = SpectralWeightingConfig(\n        node_channels=latent_dim,\n        K=3,  # Chebyshev polynomial order\n        num_layers=2,\n        normalization='sym',\n        bias=True,\n        dropout=0.2,\n        norm='batch',\n        norm_kwargs={'in_channels': latent_dim}\n    )\n    \n    # Center Module Configuration\n    center_config = DiscreteMeanCenterConfig(\n        distance_metric='euclidean'\n    )\n    \n    # Radius Mask Configuration (simplified weighting module)\n    radius_weighting = GATConv(latent_dim, latent_dim)\n    radius_config = RadiusMaskConfig(\n        max_num_neighbors=50,\n        radius=20,\n        weighting_module=radius_weighting,\n        threshold=0.1,\n        node_dim=latent_dim\n    )\n    \n    # KMeans Head Configuration\n    kmeans_head_config = KMeansHeadConfig(\n        node_count=k,\n        node_dim=latent_dim,\n        max_nodes=80,  \n        num_layers=5,\n        dropout=0.2,\n        weighting_module=SpectralWeighting(spectral_config),\n        center_module=DiscreteMeanCenter(center_config),\n        mask_module=RadiusAttentionWeights(radius_config),\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # KMeans Configuration\n    kmeans_config = KMeansConfig(\n        k=k,\n        max_iter=15,\n        thresh=1e-6,\n        max_overlap=2,\n        head_module=kmeans_head_config,\n        excluded_is_cluster=True\n    )\n    \n    # Output Head Configuration\n    output_head_config = OutputHeadConfig(\n        node_dim=latent_dim,\n        hidden_dim=hidden_dim,\n        output_dim=output_dim,\n        pooling_type='mean',\n        norm='batch',\n        norm_kwargs={'in_channels': hidden_dim},\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # Policy Module Configuration\n    policy_config = OutputHeadConfig(\n        node_dim=latent_dim,\n        hidden_dim=hidden_dim//2,\n        output_dim=2,  # halt=0, continue=1\n        pooling_type='mean',\n        norm='batch',\n        norm_kwargs={'in_channels': hidden_dim//2},\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # Inner Module Configuration\n    inner_config = KMeansHRMInnerModuleConfig(\n        add_self_loops=True,\n        add_negative_edges=True,\n        dropout=0.2,\n        hidden_dim=hidden_dim,        # inner-side hidden size, reused\n        node_dim=latent_dim,          # must equal vgae_latent_dim\n        attention_dim=4,           # bigger in final\n        edge_dim=edge_dim,\n        layers=3,\n        kmeans_config=kmeans_config,\n        output_head_config=output_head_config,\n        policy_module_config=policy_config,\n        K_cycles=3,\n        L_cycles=5,\n        batch_size=BATCH_SIZE,\n        halt_max_steps=10,\n        halt_exploration_prob=0.1,\n    )\n    \n    config = KMeansHRMConfig(\n        inner_module=inner_config,\n        explore_steps_prob=0.1,\n        halt_max_steps=10,\n        pre_encoder_conv_layers=2,\n        vgae_encoder_type=\"cheb\",\n        input_dim=input_dim,\n        edge_attr_dim=edge_dim,\n        vgae_latent_dim=latent_dim,           # must equal inner.node_dim\n        vgae_encoder_layers=2,\n        vgae_encoder_dropout=0.1,\n        vgae_decoder_type=None,\n        vgae_kl_weight=1.0,\n    )\n    \n    return config\n\n# Modell initialisieren\nhrm_config = create_kmeans_hrm_config(INPUT_DIM, EDGE_DIM, latent_dim=32, hidden_dim=256, output_dim=TARGET_DIM, k=8)\n#hrm_config = create_kmeans_hrm_config(INPUT_DIM, EDGE_DIM, latent_dim=4, hidden_dim=4, output_dim=TARGET_DIM, k=2)\nmodel = KMeansHRMModule(hrm_config, training=True).to(device)\noptimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# Print number of parameters\nnum_params = count_parameters(model)\nprint(f\"KMeansHRMModule created with {num_params:,} trainable parameters\")\nprint(f\"Model size: {num_params * 4 / 1024 / 1024:.2f} MB (float32)\")\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T16:07:27.067347Z","iopub.execute_input":"2025-09-15T16:07:27.067936Z","iopub.status.idle":"2025-09-15T16:07:27.273043Z","shell.execute_reply.started":"2025-09-15T16:07:27.067915Z","shell.execute_reply":"2025-09-15T16:07:27.272242Z"}},"outputs":[{"name":"stdout","text":"KMeansHRMModule created with 71,884 trainable parameters\nModel size: 0.27 MB (float32)\nKMeansHRMModule(\n  (inner_module): KMeansHRMInnerModule(\n    (kmeans_module): KMeans(\n      (heads): ModuleList(\n        (0-7): 8 x KMeansHead(\n          (weighting_module): SpectralWeighting(\n            (cheb_convs): ModuleList(\n              (0-1): 2 x ChebConv(32, 32, K=3, normalization=sym)\n            )\n            (norms): ModuleList(\n              (0-1): 2 x BatchNorm(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (center_module): DiscreteMeanCenter(\n            (distance_module): PairwiseDistance()\n          )\n          (mask_module): RadiusAttentionWeights(\n            (weighting_module): GATConv(32, 32, heads=1)\n            (_mask_linear): Linear(in_features=32, out_features=1, bias=True)\n          )\n          (act): ReLU()\n        )\n      )\n    )\n    (vgae_encoder): VGAEEncoder(\n      (norms): ModuleList(\n        (0-2): 3 x LayerNorm(4, affine=True, mode=graph)\n      )\n      (convs): ModuleList(\n        (0): GCNConv(32, 4)\n        (1-2): 2 x GCNConv(4, 4)\n        (3): GCNConv(4, 32)\n      )\n      (edge_attr_linear): Linear(in_features=4, out_features=1, bias=True)\n    )\n    (vgae): VGAE(\n      (encoder): VGAEEncoder(\n        (norms): ModuleList(\n          (0-2): 3 x LayerNorm(4, affine=True, mode=graph)\n        )\n        (convs): ModuleList(\n          (0): GCNConv(32, 4)\n          (1-2): 2 x GCNConv(4, 4)\n          (3): GCNConv(4, 32)\n        )\n        (edge_attr_linear): Linear(in_features=4, out_features=1, bias=True)\n      )\n      (decoder): InnerProductDecoder()\n    )\n    (linear_post_attention): Sequential(\n      (0): Linear(in_features=32, out_features=256, bias=True)\n      (1): ReLU()\n      (2): Dropout(p=0.2, inplace=False)\n      (3): Linear(in_features=256, out_features=32, bias=True)\n      (4): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n    )\n    (norm): LayerNorm(32, affine=True, mode=graph)\n    (dropout_layer): Dropout(p=0.2, inplace=False)\n    (output_head): OutputHead(\n      (linear1): Linear(32, 256, bias=True)\n      (linear2): Linear(256, 5, bias=True)\n      (norm): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act): ReLU()\n    )\n    (policy_module): OutputHead(\n      (linear1): Linear(32, 128, bias=True)\n      (linear2): Linear(128, 2, bias=True)\n      (norm): BatchNorm(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act): ReLU()\n    )\n  )\n  (pre_encoder_conv): NNConv(6, 6, aggr=mean, nn=Sequential(\n    (0): Linear(in_features=4, out_features=24, bias=True)\n    (1): Linear(in_features=24, out_features=24, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Linear(in_features=24, out_features=24, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.1, inplace=False)\n    (7): Linear(in_features=24, out_features=36, bias=True)\n  ))\n  (vgae_encoder): VGAEEncoder(\n    (norms): ModuleList(\n      (0): LayerNorm(256, affine=True, mode=graph)\n    )\n    (convs): ModuleList(\n      (0): ChebConv(6, 256, K=3, normalization=sym)\n      (1): ChebConv(256, 32, K=3, normalization=sym)\n    )\n    (edge_attr_linear): Linear(in_features=4, out_features=1, bias=True)\n  )\n  (vgae): VGAE(\n    (encoder): VGAEEncoder(\n      (norms): ModuleList(\n        (0): LayerNorm(256, affine=True, mode=graph)\n      )\n      (convs): ModuleList(\n        (0): ChebConv(6, 256, K=3, normalization=sym)\n        (1): ChebConv(256, 32, K=3, normalization=sym)\n      )\n      (edge_attr_linear): Linear(in_features=4, out_features=1, bias=True)\n    )\n    (decoder): InnerProductDecoder()\n  )\n)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import training_loops\nfrom training_loops import composite_loss, compute_mae_in_bounds, init_property_bounds\n\ninit_property_bounds(PROPERTIES, extended_df)\n\nfrom typing import TypedDict, Dict, Any\nfrom dataclasses import dataclass\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split\nfrom torch.optim import Adam\n\nfrom torch_geometric.data import Data, Batch\n\ndef save_checkpoint(state: Dict, step: int, is_best: bool = False):\n    path = os.path.join(CHECKPOINT_DIR, f\"step_{step}.pt\")\n    torch.save(state, path)\n    if is_best:\n        best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n        torch.save(state, best_path)\n\n\nclass PretrainConfig(TypedDict):\n    # Data\n    data_path: str\n\n    # Hyperparams\n    global_batch_size: int\n    epochs: int\n    total_iters: int\n\n    lr: float\n    lr_min_ratio: float\n    lr_warmup_steps: int\n\n    weight_decay: float\n    beta1: float\n    beta2: float\n\n\n@dataclass\nclass TrainState:\n    model: nn.Module\n    optimizer: torch.optim.Optimizer\n    carry: KMeansHRMInitialCarry | None\n\n    step: int\n    total_steps: int\n\n\ndef compute_warmup_weight(step: int, warmup_steps: int, min_ratio: float) -> float:\n    if warmup_steps <= 0:\n        return 1.0\n    if step < warmup_steps:\n        # Linear warmup from min_ratio -> 1.0\n        return float(min_ratio + (1.0 - min_ratio) * (step / max(1, warmup_steps)))\n    return 1.0\n\n\ndef pack_train_state_for_save(ts: TrainState) -> Dict[str, Any]:\n    return {\n        \"step\": int(ts.step),\n        \"total_steps\": int(ts.total_steps),\n        # Carry can be large; still useful for exact resume within the same batch sequence\n        \"carry\": ts.carry,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T16:07:27.273871Z","iopub.execute_input":"2025-09-15T16:07:27.274142Z","iopub.status.idle":"2025-09-15T16:07:27.291508Z","shell.execute_reply.started":"2025-09-15T16:07:27.274118Z","shell.execute_reply":"2025-09-15T16:07:27.290610Z"}},"outputs":[{"name":"stdout","text":"Loss and metrics initialized.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from typing import Union, List\nfrom training_loops import range_violation_loss\n\ndef composite_loss(monomer_index: int, properties: List[str], preds: torch.Tensor, targets: torch.Tensor, related_info: torch.Tensor) -> torch.Tensor:\n    # preds: [B, 5], targets: [B, 5] with NaNs if missing\n    loss_items = []\n    \n    related_info.unsqueeze(-1)\n    monomer_counts = torch.tensor(related_info[:, monomer_index], dtype=torch.float32, device=preds.device)\n    weight = torch.pow(0.3, monomer_counts - 1).to(preds.device).view(-1)\n\n    for j in range(len(properties)):\n        t = targets[:, j]\n        p = preds[:, j]\n        mask_present = torch.isfinite(t).to(p.device)\n\n        assert p.device == t.device, f\"p.device: {p.device}, t.device: {t.device}\"\n        assert p.device == weight.device, f\"p.device: {p.device}, related_info.device: {weight.device}\"\n        assert mask_present.shape == p.shape, f\"mask_present.shape: {mask_present.shape}, p.shape: {p.shape}\"\n        assert mask_present.shape == t.shape, f\"mask_present.shape: {mask_present.shape}, t.shape: {t.shape}\"\n        assert mask_present.shape == weight.shape, f\"mask_present.shape: {mask_present.shape}, weight.shape: {weight.shape}\"\n\n\n        if mask_present.any():\n            mse = F.mse_loss(p[mask_present], t[mask_present], weight=weight[mask_present])\n            loss_items.append(mse)\n        # Range-violation for all (including available): no loss if within bounds\n        rv = range_violation_loss(properties, p, j).mean()\n        loss_items.append(rv)\n    return torch.stack(loss_items).mean()\n\n\n@torch.no_grad()\ndef compute_mae_in_bounds(monomer_index: int, properties: List[str], preds: torch.Tensor, targets: torch.Tensor, related_info: torch.Tensor) -> Dict[str, float]:\n    out = {}\n\n    assert type(related_info) == torch.Tensor, f\"related info is a {type(related_info)}\"\n\n    monomer_counts = torch.tensor(related_info[:, monomer_index], dtype=torch.float32, device=preds.device)\n    weight = torch.pow(0.3, monomer_counts - 1).to(preds.device).view(-1)\n\n    for j, name in enumerate(properties):\n\n        t = targets[:, j]\n        p = preds[:, j]\n        mask_present = torch.isfinite(t).to(p.device)\n\n        if mask_present.any():\n            out[f\"mae_{name}\"] = (torch.mul(p[mask_present] - t[mask_present], weight[mask_present])).abs().mean().item()\n        else:\n            out[f\"mae_{name}\"] = torch.tensor(float('nan'))\n    return out\n\nprint(\"Loss and metrics initialized.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T16:07:27.292535Z","iopub.execute_input":"2025-09-15T16:07:27.292879Z","iopub.status.idle":"2025-09-15T16:07:27.309613Z","shell.execute_reply.started":"2025-09-15T16:07:27.292854Z","shell.execute_reply":"2025-09-15T16:07:27.308955Z"}},"outputs":[{"name":"stdout","text":"Loss and metrics initialized.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def train_batch(epoch: int, train_state: TrainState, batch_data: Batch, config: PretrainConfig) -> Dict[str, float]:\n    model = train_state.model\n    optimizer = train_state.optimizer\n\n    model.train()\n\n    # Targets [B,5]\n    y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n    batch_data = batch_data.to(device)\n\n    related_info = torch.stack([torch.as_tensor(d[0]).to(device) for d in batch_data.aux_info]).unsqueeze(1).to(device)\n    #related_info = batch_data.aux_info\n    \n    # Reuse existing carry if provided; otherwise initialize or realign to current num_graphs\n    need_new_carry = (\n        train_state.carry is None\n        or (hasattr(batch_data, 'num_graphs') and train_state.carry.steps.numel() != batch_data.num_graphs)\n    )\n    if need_new_carry:\n        train_state.carry = model.initial_carry(batch_data)\n\n    # Forward\n    train_state.carry, hrm_output = model(train_state.carry, batch_data)\n    preds = hrm_output['y_pred'].to(device)\n    policy = hrm_output['q_policy']\n    target_policy = hrm_output.get('target_q_policy', None)\n\n    targets = y\n    properties = PROPERTIES\n\n    if target_policy is not None and target_policy[0].size == policy[0].size:\n\n        preds = torch.cat([preds, policy[0].unsqueeze(-1), policy[1].unsqueeze(-1)], dim=1)\n        targets = torch.cat([y, target_policy[0].unsqueeze(-1), target_policy[1].unsqueeze(-1)], dim=1)\n\n        properties += ['q_halt', 'q_cont']\n\n    assert preds.size(0) == targets.size(0), f\"batch mismatch: preds {preds.shape}, targets {targets.shape}\"\n    assert len(properties) == targets.size(1), f\"mismatch {y.size} {properties}\"\n\n    loss = composite_loss(0, properties, preds, targets, related_info)\n\n    train_state.step += 1\n    warmup_w = compute_warmup_weight(train_state.step, config[\"lr_warmup_steps\"], config[\"lr_min_ratio\"])  # type: ignore[index]\n    scaled_loss = loss * warmup_w * (1.0 / TOTAL_BATCH_COUNT)\n\n    # Backward/update\n    optimizer.zero_grad()\n    scaled_loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n\n    # Metrics (unscaled loss for logging)\n    with torch.no_grad():\n        metrics = compute_mae_in_bounds(0, properties, preds, targets, related_info)\n        metrics.update({\n            \"loss\": loss.item(),\n            \"warmup_weight\": float(warmup_w),\n        })\n\n\n    # Periodic checkpoint\n    global_step = train_state.step\n    if global_step % CHECKPOINT_EVERY_N_STEPS == 0:\n        save_checkpoint({\n            \"epoch\": epoch,\n            \"global_step\": global_step,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"model_class\": model.__class__.__name__,\n            \"model_module\": model.__class__.__module__,\n            \"train_state\": pack_train_state_for_save(train_state),\n        }, step=global_step)\n\n    return metrics\n\n\n@torch.no_grad()\ndef validate(epoch: int, model: nn.Module, loader: DataLoader) -> Dict[str, float]:\n    model.eval()\n    running_loss = 0.0\n    mae_accum = {f\"mae_{p}\": 0.0 for p in PROPERTIES + ['q_halt', 'q_cont']}\n    count_samples = 0\n\n    for batch_data in loader:\n        # Targets [B,5]\n        y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n        batch_data = batch_data.to(device)\n\n        related_info = torch.stack([torch.as_tensor(d[0]).to(device) for d in batch_data.aux_info]).unsqueeze(1).to(device)\n        #related_info = batch_data.aux_info\n        \n        carry = model.initial_carry(batch_data)\n        \n        carry, hrm_output = model(carry, batch_data)\n        preds = hrm_output['y_pred'].to(device)\n        policy = hrm_output['q_policy']\n        target_policy = hrm_output.get('target_q_policy', None)\n        \n        targets = y\n        properties = PROPERTIES\n\n        if target_policy is not None and target_policy[0].size == policy[0].size:\n    \n            preds = torch.cat([preds, policy[0].unsqueeze(-1), policy[1].unsqueeze(-1)], dim=1)\n            targets = torch.cat([y, target_policy[0].unsqueeze(-1), target_policy[1].unsqueeze(-1)], dim=1)\n    \n            properties += ['q_halt', 'q_cont']\n\n        assert preds.size(0) == y.size(0), f\"batch mismatch: preds {preds.shape}, targets {targets.shape}\"\n        assert len(properties) == targets.size(1), f\"mismatch {y.size} {properties}\"\n\n        loss = composite_loss(0, properties, preds, targets, related_info)\n\n        metrics = compute_mae_in_bounds(0, properties, preds, targets, related_info)\n        for k, v in metrics.items():\n            if not math.isnan(v):\n                mae_accum[k] += v * preds.size(0)\n        running_loss += loss.item() * preds.size(0)\n        count_samples += preds.size(0)\n\n    avg_loss = running_loss / max(1, count_samples)\n    avg_mae = {k: (v / max(1, count_samples)) for k, v in mae_accum.items()}\n\n    return {\"loss\": avg_loss, **avg_mae}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T20:20:49.192221Z","iopub.execute_input":"2025-09-15T20:20:49.192985Z","iopub.status.idle":"2025-09-15T20:20:49.209069Z","shell.execute_reply.started":"2025-09-15T20:20:49.192961Z","shell.execute_reply":"2025-09-15T20:20:49.208339Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# Added after failure at 3375 due to misplaced .cpu, changed from 0.001 to 0.01\noptimizer = Adam(model.parameters(), lr=0.01, weight_decay=WEIGHT_DECAY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T20:21:49.470833Z","iopub.execute_input":"2025-09-15T20:21:49.471116Z","iopub.status.idle":"2025-09-15T20:21:49.475423Z","shell.execute_reply.started":"2025-09-15T20:21:49.471094Z","shell.execute_reply":"2025-09-15T20:21:49.474748Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"from training_loops import train_until_total_iters\n\n# Build a minimal config for warmup from existing hyperparams\nCONFIG: PretrainConfig = {\n    \"data_path\": \"\",\n    \"global_batch_size\": BATCH_SIZE,\n    \"epochs\": EPOCHS,\n    # Step-based training support; used by helper loop\n    \"total_iters\": int(EPOCHS * len(train_loader)),\n    \"lr\": 0.01,\n    \"lr_min_ratio\": 0.1,\n    \"lr_warmup_steps\": max(1, len(train_loader) * 2),  # warm up first ~2 epochs of steps\n    \"weight_decay\": WEIGHT_DECAY,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n}\n\nhistory = {\"train\": [], \"val\": []}\nbest_val_loss = float(\"inf\")\n\n# Initialize TrainState with total steps estimated from loader length and epochs\ntrain_state = TrainState(\n    model=model,\n    optimizer=optimizer,\n    carry=None,\n    step=3375,\n    total_steps=0  #EPOCHS * len(train_loader)\n)\n\nlast_completed_epoch=0\n\nhistory, best_val_loss = train_until_total_iters(\n    total_iters=CONFIG[\"total_iters\"],\n    start_epoch=last_completed_epoch,\n    train_state=train_state,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    config=CONFIG,\n    train_batch_fn=train_batch,\n    validate_fn=validate,\n    save_checkpoint_fn=save_checkpoint,\n    checkpoint_every_n=10,\n    print_every_n=1\n)\n\nmetrics_path = os.path.join(CHECKPOINT_DIR, \"history.json\")\nwith open(metrics_path, \"w\") as f:\n    json.dump(history, f, indent=2)\nprint(f\"Saved metrics to {metrics_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T20:21:52.675321Z","iopub.execute_input":"2025-09-15T20:21:52.675583Z","iopub.status.idle":"2025-09-15T22:41:09.268242Z","shell.execute_reply.started":"2025-09-15T20:21:52.675562Z","shell.execute_reply":"2025-09-15T22:41:09.267101Z"}},"outputs":[{"name":"stdout","text":"step   3376/33750 | epoch   1 | batch    1 | loss=6673.9292 | lr=0.01\nstep   3377/33750 | epoch   1 | batch    2 | loss=87.2383 | lr=0.01\nstep   3378/33750 | epoch   1 | batch    3 | loss=0.6429 | lr=0.01\nstep   3379/33750 | epoch   1 | batch    4 | loss=0.0364 | lr=0.01\nstep   3380/33750 | epoch   1 | batch    5 | loss=547.2436 | lr=0.01\nstep   3381/33750 | epoch   1 | batch    6 | loss=432.6966 | lr=0.01\nstep   3382/33750 | epoch   1 | batch    7 | loss=5.5850 | lr=0.01\nstep   3383/33750 | epoch   1 | batch    8 | loss=581.3133 | lr=0.01\nstep   3384/33750 | epoch   1 | batch    9 | loss=11.8579 | lr=0.01\nstep   3385/33750 | epoch   1 | batch   10 | loss=20.0168 | lr=0.01\nstep   3386/33750 | epoch   1 | batch   11 | loss=885.5467 | lr=0.01\nstep   3387/33750 | epoch   1 | batch   12 | loss=3460.3269 | lr=0.01\nstep   3388/33750 | epoch   1 | batch   13 | loss=1211.5056 | lr=0.01\nstep   3389/33750 | epoch   1 | batch   14 | loss=0.0066 | lr=0.01\nstep   3390/33750 | epoch   1 | batch   15 | loss=545.0806 | lr=0.01\nstep   3391/33750 | epoch   1 | batch   16 | loss=939.2009 | lr=0.01\nstep   3392/33750 | epoch   1 | batch   17 | loss=549.9747 | lr=0.01\nstep   3393/33750 | epoch   1 | batch   18 | loss=107.0912 | lr=0.01\nstep   3394/33750 | epoch   1 | batch   19 | loss=1420.4946 | lr=0.01\nstep   3395/33750 | epoch   1 | batch   20 | loss=2278.9922 | lr=0.01\nstep   3396/33750 | epoch   1 | batch   21 | loss=387.3514 | lr=0.01\nstep   3397/33750 | epoch   1 | batch   22 | loss=335.0123 | lr=0.01\nstep   3398/33750 | epoch   1 | batch   23 | loss=900.3496 | lr=0.01\nstep   3399/33750 | epoch   1 | batch   24 | loss=2.0702 | lr=0.01\nstep   3400/33750 | epoch   1 | batch   25 | loss=251.2341 | lr=0.01\nstep   3401/33750 | epoch   1 | batch   26 | loss=676.4557 | lr=0.01\nstep   3402/33750 | epoch   1 | batch   27 | loss=9.9791 | lr=0.01\nstep   3403/33750 | epoch   1 | batch   28 | loss=781.5228 | lr=0.01\nstep   3404/33750 | epoch   1 | batch   29 | loss=4.1938 | lr=0.01\nstep   3405/33750 | epoch   1 | batch   30 | loss=87.1822 | lr=0.01\nstep   3406/33750 | epoch   1 | batch   31 | loss=2564.6719 | lr=0.01\nstep   3407/33750 | epoch   1 | batch   32 | loss=954.3298 | lr=0.01\nstep   3408/33750 | epoch   1 | batch   33 | loss=398.6718 | lr=0.01\nstep   3409/33750 | epoch   1 | batch   34 | loss=76.8420 | lr=0.01\nstep   3410/33750 | epoch   1 | batch   35 | loss=5.1671 | lr=0.01\nstep   3411/33750 | epoch   1 | batch   36 | loss=28.2366 | lr=0.01\nstep   3412/33750 | epoch   1 | batch   37 | loss=879.0762 | lr=0.01\nstep   3413/33750 | epoch   1 | batch   38 | loss=1.2676 | lr=0.01\nstep   3414/33750 | epoch   1 | batch   39 | loss=56.5147 | lr=0.01\nstep   3415/33750 | epoch   1 | batch   40 | loss=0.7270 | lr=0.01\nstep   3416/33750 | epoch   1 | batch   41 | loss=187.3534 | lr=0.01\nstep   3417/33750 | epoch   1 | batch   42 | loss=17.5786 | lr=0.01\nstep   3418/33750 | epoch   1 | batch   43 | loss=1081.8547 | lr=0.01\nstep   3419/33750 | epoch   1 | batch   44 | loss=3.2742 | lr=0.01\nstep   3420/33750 | epoch   1 | batch   45 | loss=2.8724 | lr=0.01\nstep   3421/33750 | epoch   1 | batch   46 | loss=2639.4714 | lr=0.01\nstep   3422/33750 | epoch   1 | batch   47 | loss=767.2208 | lr=0.01\nstep   3423/33750 | epoch   1 | batch   48 | loss=190.4592 | lr=0.01\nstep   3424/33750 | epoch   1 | batch   49 | loss=4.5974 | lr=0.01\nstep   3425/33750 | epoch   1 | batch   50 | loss=1.8530 | lr=0.01\nstep   3426/33750 | epoch   1 | batch   51 | loss=29.2932 | lr=0.01\nstep   3427/33750 | epoch   1 | batch   52 | loss=21.6365 | lr=0.01\nstep   3428/33750 | epoch   1 | batch   53 | loss=523.4847 | lr=0.01\nstep   3429/33750 | epoch   1 | batch   54 | loss=316.2719 | lr=0.01\nstep   3430/33750 | epoch   1 | batch   55 | loss=1422.5431 | lr=0.01\nstep   3431/33750 | epoch   1 | batch   56 | loss=4.0122 | lr=0.01\nstep   3432/33750 | epoch   1 | batch   57 | loss=0.1062 | lr=0.01\nstep   3433/33750 | epoch   1 | batch   58 | loss=48.0138 | lr=0.01\nstep   3434/33750 | epoch   1 | batch   59 | loss=12.2573 | lr=0.01\nstep   3435/33750 | epoch   1 | batch   60 | loss=587.8777 | lr=0.01\nstep   3436/33750 | epoch   1 | batch   61 | loss=0.1839 | lr=0.01\nstep   3437/33750 | epoch   1 | batch   62 | loss=5236.8027 | lr=0.01\nstep   3438/33750 | epoch   1 | batch   63 | loss=0.0912 | lr=0.01\nstep   3439/33750 | epoch   1 | batch   64 | loss=127.1954 | lr=0.01\nstep   3440/33750 | epoch   1 | batch   65 | loss=12.0754 | lr=0.01\nstep   3441/33750 | epoch   1 | batch   66 | loss=0.3811 | lr=0.01\nstep   3442/33750 | epoch   1 | batch   67 | loss=0.2839 | lr=0.01\nstep   3443/33750 | epoch   1 | batch   68 | loss=1840.0641 | lr=0.01\nstep   3444/33750 | epoch   1 | batch   69 | loss=1326.5492 | lr=0.01\nstep   3445/33750 | epoch   1 | batch   70 | loss=0.0864 | lr=0.01\nstep   3446/33750 | epoch   1 | batch   71 | loss=137.5261 | lr=0.01\nstep   3447/33750 | epoch   1 | batch   72 | loss=2900.2791 | lr=0.01\nstep   3448/33750 | epoch   1 | batch   73 | loss=0.4155 | lr=0.01\nstep   3449/33750 | epoch   1 | batch   74 | loss=1.0158 | lr=0.01\nstep   3450/33750 | epoch   1 | batch   75 | loss=279.5838 | lr=0.01\nstep   3451/33750 | epoch   1 | batch   76 | loss=150.0547 | lr=0.01\nstep   3452/33750 | epoch   1 | batch   77 | loss=1.5323 | lr=0.01\nstep   3453/33750 | epoch   1 | batch   78 | loss=4960.3535 | lr=0.01\nstep   3454/33750 | epoch   1 | batch   79 | loss=837.6992 | lr=0.01\nstep   3455/33750 | epoch   1 | batch   80 | loss=22.1233 | lr=0.01\nstep   3456/33750 | epoch   1 | batch   81 | loss=3.2953 | lr=0.01\nstep   3457/33750 | epoch   1 | batch   82 | loss=135.7608 | lr=0.01\nstep   3458/33750 | epoch   1 | batch   83 | loss=29.3746 | lr=0.01\nstep   3459/33750 | epoch   1 | batch   84 | loss=0.7424 | lr=0.01\nstep   3460/33750 | epoch   1 | batch   85 | loss=13.1119 | lr=0.01\nstep   3461/33750 | epoch   1 | batch   86 | loss=1.8969 | lr=0.01\nstep   3462/33750 | epoch   1 | batch   87 | loss=0.8954 | lr=0.01\nstep   3463/33750 | epoch   1 | batch   88 | loss=1714.6737 | lr=0.01\nstep   3464/33750 | epoch   1 | batch   89 | loss=610.9306 | lr=0.01\nstep   3465/33750 | epoch   1 | batch   90 | loss=474.4293 | lr=0.01\nstep   3466/33750 | epoch   1 | batch   91 | loss=229.0966 | lr=0.01\nstep   3467/33750 | epoch   1 | batch   92 | loss=13.4817 | lr=0.01\nstep   3468/33750 | epoch   1 | batch   93 | loss=664.6641 | lr=0.01\nstep   3469/33750 | epoch   1 | batch   94 | loss=884.1819 | lr=0.01\nstep   3470/33750 | epoch   1 | batch   95 | loss=37.6734 | lr=0.01\nstep   3471/33750 | epoch   1 | batch   96 | loss=0.5448 | lr=0.01\nstep   3472/33750 | epoch   1 | batch   97 | loss=1554.4948 | lr=0.01\nstep   3473/33750 | epoch   1 | batch   98 | loss=204.7242 | lr=0.01\nstep   3474/33750 | epoch   1 | batch   99 | loss=340.7578 | lr=0.01\nstep   3475/33750 | epoch   1 | batch  100 | loss=0.1166 | lr=0.01\nstep   3476/33750 | epoch   1 | batch  101 | loss=2065.5105 | lr=0.01\nstep   3477/33750 | epoch   1 | batch  102 | loss=2.2740 | lr=0.01\nstep   3478/33750 | epoch   1 | batch  103 | loss=17.3859 | lr=0.01\nstep   3479/33750 | epoch   1 | batch  104 | loss=658.5628 | lr=0.01\nstep   3480/33750 | epoch   1 | batch  105 | loss=1.9996 | lr=0.01\nstep   3481/33750 | epoch   1 | batch  106 | loss=31.6236 | lr=0.01\nstep   3482/33750 | epoch   1 | batch  107 | loss=270.8236 | lr=0.01\nstep   3483/33750 | epoch   1 | batch  108 | loss=13128.3877 | lr=0.01\nstep   3484/33750 | epoch   1 | batch  109 | loss=78.3190 | lr=0.01\nstep   3485/33750 | epoch   1 | batch  110 | loss=52.5168 | lr=0.01\nstep   3486/33750 | epoch   1 | batch  111 | loss=0.4483 | lr=0.01\nstep   3487/33750 | epoch   1 | batch  112 | loss=9.4966 | lr=0.01\nstep   3488/33750 | epoch   1 | batch  113 | loss=0.0036 | lr=0.01\nstep   3489/33750 | epoch   1 | batch  114 | loss=3.6027 | lr=0.01\nstep   3490/33750 | epoch   1 | batch  115 | loss=653.7092 | lr=0.01\nstep   3491/33750 | epoch   1 | batch  116 | loss=15.7611 | lr=0.01\nstep   3492/33750 | epoch   1 | batch  117 | loss=0.0097 | lr=0.01\nstep   3493/33750 | epoch   1 | batch  118 | loss=0.3032 | lr=0.01\nstep   3494/33750 | epoch   1 | batch  119 | loss=333.7372 | lr=0.01\nstep   3495/33750 | epoch   1 | batch  120 | loss=5.9269 | lr=0.01\nstep   3496/33750 | epoch   1 | batch  121 | loss=4.1735 | lr=0.01\nstep   3497/33750 | epoch   1 | batch  122 | loss=79.4275 | lr=0.01\nstep   3498/33750 | epoch   1 | batch  123 | loss=552.6193 | lr=0.01\nstep   3499/33750 | epoch   1 | batch  124 | loss=4.1919 | lr=0.01\nstep   3500/33750 | epoch   1 | batch  125 | loss=190.5631 | lr=0.01\nstep   3501/33750 | epoch   1 | batch  126 | loss=1670.1506 | lr=0.01\nstep   3502/33750 | epoch   1 | batch  127 | loss=0.0012 | lr=0.01\nstep   3503/33750 | epoch   1 | batch  128 | loss=0.6141 | lr=0.01\nstep   3504/33750 | epoch   1 | batch  129 | loss=799.2391 | lr=0.01\nstep   3505/33750 | epoch   1 | batch  130 | loss=14.9014 | lr=0.01\nstep   3506/33750 | epoch   1 | batch  131 | loss=679.7972 | lr=0.01\nstep   3507/33750 | epoch   1 | batch  132 | loss=6.1804 | lr=0.01\nstep   3508/33750 | epoch   1 | batch  133 | loss=6.7962 | lr=0.01\nstep   3509/33750 | epoch   1 | batch  134 | loss=5.3293 | lr=0.01\nstep   3510/33750 | epoch   1 | batch  135 | loss=0.1798 | lr=0.01\nstep   3511/33750 | epoch   1 | batch  136 | loss=5067.9038 | lr=0.01\nstep   3512/33750 | epoch   1 | batch  137 | loss=38.0873 | lr=0.01\nstep   3513/33750 | epoch   1 | batch  138 | loss=268.8224 | lr=0.01\nstep   3514/33750 | epoch   1 | batch  139 | loss=539.2726 | lr=0.01\nstep   3515/33750 | epoch   1 | batch  140 | loss=381.8062 | lr=0.01\nstep   3516/33750 | epoch   1 | batch  141 | loss=421.9635 | lr=0.01\nstep   3517/33750 | epoch   1 | batch  142 | loss=0.0572 | lr=0.01\nstep   3518/33750 | epoch   1 | batch  143 | loss=1392.6195 | lr=0.01\nstep   3519/33750 | epoch   1 | batch  144 | loss=1461.2271 | lr=0.01\nstep   3520/33750 | epoch   1 | batch  145 | loss=86.6743 | lr=0.01\nstep   3521/33750 | epoch   1 | batch  146 | loss=530.2078 | lr=0.01\nstep   3522/33750 | epoch   1 | batch  147 | loss=81.5423 | lr=0.01\nstep   3523/33750 | epoch   1 | batch  148 | loss=0.0015 | lr=0.01\nstep   3524/33750 | epoch   1 | batch  149 | loss=1286.5753 | lr=0.01\nstep   3525/33750 | epoch   1 | batch  150 | loss=4.0392 | lr=0.01\nstep   3526/33750 | epoch   1 | batch  151 | loss=70.6111 | lr=0.01\nstep   3527/33750 | epoch   1 | batch  152 | loss=2.6477 | lr=0.01\nstep   3528/33750 | epoch   1 | batch  153 | loss=0.2017 | lr=0.01\nstep   3529/33750 | epoch   1 | batch  154 | loss=7804.0015 | lr=0.01\nstep   3530/33750 | epoch   1 | batch  155 | loss=0.5216 | lr=0.01\nstep   3531/33750 | epoch   1 | batch  156 | loss=4.3583 | lr=0.01\nstep   3532/33750 | epoch   1 | batch  157 | loss=4088.7710 | lr=0.01\nstep   3533/33750 | epoch   1 | batch  158 | loss=0.0201 | lr=0.01\nstep   3534/33750 | epoch   1 | batch  159 | loss=701.3952 | lr=0.01\nstep   3535/33750 | epoch   1 | batch  160 | loss=10.0918 | lr=0.01\nstep   3536/33750 | epoch   1 | batch  161 | loss=27.3819 | lr=0.01\nstep   3537/33750 | epoch   1 | batch  162 | loss=201.0827 | lr=0.01\nstep   3538/33750 | epoch   1 | batch  163 | loss=67.2104 | lr=0.01\nstep   3539/33750 | epoch   1 | batch  164 | loss=1.5709 | lr=0.01\nstep   3540/33750 | epoch   1 | batch  165 | loss=2.2522 | lr=0.01\nstep   3541/33750 | epoch   1 | batch  166 | loss=0.2471 | lr=0.01\nstep   3542/33750 | epoch   1 | batch  167 | loss=380.9381 | lr=0.01\nstep   3543/33750 | epoch   1 | batch  168 | loss=28.7387 | lr=0.01\nstep   3544/33750 | epoch   1 | batch  169 | loss=722.9078 | lr=0.01\nstep   3545/33750 | epoch   1 | batch  170 | loss=745.7463 | lr=0.01\nstep   3546/33750 | epoch   1 | batch  171 | loss=405.8437 | lr=0.01\nstep   3547/33750 | epoch   1 | batch  172 | loss=466.1819 | lr=0.01\nstep   3548/33750 | epoch   1 | batch  173 | loss=0.0827 | lr=0.01\nstep   3549/33750 | epoch   1 | batch  174 | loss=42.2679 | lr=0.01\nstep   3550/33750 | epoch   1 | batch  175 | loss=934.5722 | lr=0.01\nstep   3551/33750 | epoch   1 | batch  176 | loss=94.7417 | lr=0.01\nstep   3552/33750 | epoch   1 | batch  177 | loss=271.1804 | lr=0.01\nstep   3553/33750 | epoch   1 | batch  178 | loss=1534.2740 | lr=0.01\nstep   3554/33750 | epoch   1 | batch  179 | loss=311.7693 | lr=0.01\nstep   3555/33750 | epoch   1 | batch  180 | loss=178.0945 | lr=0.01\nstep   3556/33750 | epoch   1 | batch  181 | loss=344.7890 | lr=0.01\nstep   3557/33750 | epoch   1 | batch  182 | loss=169.1425 | lr=0.01\nstep   3558/33750 | epoch   1 | batch  183 | loss=299.9957 | lr=0.01\nstep   3559/33750 | epoch   1 | batch  184 | loss=0.0047 | lr=0.01\nstep   3560/33750 | epoch   1 | batch  185 | loss=532.5010 | lr=0.01\nstep   3561/33750 | epoch   1 | batch  186 | loss=1.6363 | lr=0.01\nstep   3562/33750 | epoch   1 | batch  187 | loss=30.9010 | lr=0.01\nstep   3563/33750 | epoch   1 | batch  188 | loss=963.6089 | lr=0.01\nstep   3564/33750 | epoch   1 | batch  189 | loss=74.9280 | lr=0.01\nstep   3565/33750 | epoch   1 | batch  190 | loss=177.5843 | lr=0.01\nstep   3566/33750 | epoch   1 | batch  191 | loss=0.4800 | lr=0.01\nstep   3567/33750 | epoch   1 | batch  192 | loss=99.7358 | lr=0.01\nstep   3568/33750 | epoch   1 | batch  193 | loss=20.5275 | lr=0.01\nstep   3569/33750 | epoch   1 | batch  194 | loss=94.6370 | lr=0.01\nstep   3570/33750 | epoch   1 | batch  195 | loss=3.4665 | lr=0.01\nstep   3571/33750 | epoch   1 | batch  196 | loss=3243.8069 | lr=0.01\nstep   3572/33750 | epoch   1 | batch  197 | loss=185.7710 | lr=0.01\nstep   3573/33750 | epoch   1 | batch  198 | loss=105.4881 | lr=0.01\nstep   3574/33750 | epoch   1 | batch  199 | loss=0.7049 | lr=0.01\nstep   3575/33750 | epoch   1 | batch  200 | loss=241.9605 | lr=0.01\nstep   3576/33750 | epoch   1 | batch  201 | loss=0.0074 | lr=0.01\nstep   3577/33750 | epoch   1 | batch  202 | loss=439.7724 | lr=0.01\nstep   3578/33750 | epoch   1 | batch  203 | loss=11.8861 | lr=0.01\nstep   3579/33750 | epoch   1 | batch  204 | loss=4422.7812 | lr=0.01\nstep   3580/33750 | epoch   1 | batch  205 | loss=2460.7080 | lr=0.01\nstep   3581/33750 | epoch   1 | batch  206 | loss=3.3124 | lr=0.01\nstep   3582/33750 | epoch   1 | batch  207 | loss=671.5724 | lr=0.01\nstep   3583/33750 | epoch   1 | batch  208 | loss=0.0085 | lr=0.01\nstep   3584/33750 | epoch   1 | batch  209 | loss=275.1978 | lr=0.01\nstep   3585/33750 | epoch   1 | batch  210 | loss=11.9219 | lr=0.01\nstep   3586/33750 | epoch   1 | batch  211 | loss=55.9424 | lr=0.01\nstep   3587/33750 | epoch   1 | batch  212 | loss=0.4236 | lr=0.01\nstep   3588/33750 | epoch   1 | batch  213 | loss=0.0246 | lr=0.01\nstep   3589/33750 | epoch   1 | batch  214 | loss=1013.3966 | lr=0.01\nstep   3590/33750 | epoch   1 | batch  215 | loss=371.2653 | lr=0.01\nstep   3591/33750 | epoch   1 | batch  216 | loss=1.1815 | lr=0.01\nstep   3592/33750 | epoch   1 | batch  217 | loss=0.3749 | lr=0.01\nstep   3593/33750 | epoch   1 | batch  218 | loss=1883.9052 | lr=0.01\nstep   3594/33750 | epoch   1 | batch  219 | loss=3.5964 | lr=0.01\nstep   3595/33750 | epoch   1 | batch  220 | loss=419.8229 | lr=0.01\nstep   3596/33750 | epoch   1 | batch  221 | loss=4.9694 | lr=0.01\nstep   3597/33750 | epoch   1 | batch  222 | loss=10.6669 | lr=0.01\nstep   3598/33750 | epoch   1 | batch  223 | loss=1084.1875 | lr=0.01\nstep   3599/33750 | epoch   1 | batch  224 | loss=1010.7318 | lr=0.01\nstep   3600/33750 | epoch   1 | batch  225 | loss=1131.2299 | lr=0.01\nstep   3601/33750 | epoch   1 | batch  226 | loss=0.9502 | lr=0.01\nstep   3602/33750 | epoch   1 | batch  227 | loss=0.3569 | lr=0.01\nstep   3603/33750 | epoch   1 | batch  228 | loss=1000.9728 | lr=0.01\nstep   3604/33750 | epoch   1 | batch  229 | loss=6683.5586 | lr=0.01\nstep   3605/33750 | epoch   1 | batch  230 | loss=1.0918 | lr=0.01\nstep   3606/33750 | epoch   1 | batch  231 | loss=0.0623 | lr=0.01\nstep   3607/33750 | epoch   1 | batch  232 | loss=3.6347 | lr=0.01\nstep   3608/33750 | epoch   1 | batch  233 | loss=0.0281 | lr=0.01\nstep   3609/33750 | epoch   1 | batch  234 | loss=0.0061 | lr=0.01\nstep   3610/33750 | epoch   1 | batch  235 | loss=279.1427 | lr=0.01\nstep   3611/33750 | epoch   1 | batch  236 | loss=289.6131 | lr=0.01\nstep   3612/33750 | epoch   1 | batch  237 | loss=203.7014 | lr=0.01\nstep   3613/33750 | epoch   1 | batch  238 | loss=17.2547 | lr=0.01\nstep   3614/33750 | epoch   1 | batch  239 | loss=1.4646 | lr=0.01\nstep   3615/33750 | epoch   1 | batch  240 | loss=5.8635 | lr=0.01\nstep   3616/33750 | epoch   1 | batch  241 | loss=1151.1111 | lr=0.01\nstep   3617/33750 | epoch   1 | batch  242 | loss=1308.6862 | lr=0.01\nstep   3618/33750 | epoch   1 | batch  243 | loss=15.3784 | lr=0.01\nstep   3619/33750 | epoch   1 | batch  244 | loss=2051.1113 | lr=0.01\nstep   3620/33750 | epoch   1 | batch  245 | loss=807.5374 | lr=0.01\nstep   3621/33750 | epoch   1 | batch  246 | loss=41.1534 | lr=0.01\nstep   3622/33750 | epoch   1 | batch  247 | loss=596.4247 | lr=0.01\nstep   3623/33750 | epoch   1 | batch  248 | loss=447.8908 | lr=0.01\nstep   3624/33750 | epoch   1 | batch  249 | loss=503.7228 | lr=0.01\nstep   3625/33750 | epoch   1 | batch  250 | loss=30.5232 | lr=0.01\nstep   3626/33750 | epoch   1 | batch  251 | loss=2.3081 | lr=0.01\nstep   3627/33750 | epoch   1 | batch  252 | loss=28.9545 | lr=0.01\nstep   3628/33750 | epoch   1 | batch  253 | loss=10.4943 | lr=0.01\nstep   3629/33750 | epoch   1 | batch  254 | loss=0.2269 | lr=0.01\nstep   3630/33750 | epoch   1 | batch  255 | loss=294.8885 | lr=0.01\nstep   3631/33750 | epoch   1 | batch  256 | loss=42.8418 | lr=0.01\nstep   3632/33750 | epoch   1 | batch  257 | loss=2.7898 | lr=0.01\nstep   3633/33750 | epoch   1 | batch  258 | loss=0.3079 | lr=0.01\nstep   3634/33750 | epoch   1 | batch  259 | loss=550.7330 | lr=0.01\nstep   3635/33750 | epoch   1 | batch  260 | loss=10.4825 | lr=0.01\nstep   3636/33750 | epoch   1 | batch  261 | loss=2576.9617 | lr=0.01\nstep   3637/33750 | epoch   1 | batch  262 | loss=402.9946 | lr=0.01\nstep   3638/33750 | epoch   1 | batch  263 | loss=175.1514 | lr=0.01\nstep   3639/33750 | epoch   1 | batch  264 | loss=4.3894 | lr=0.01\nstep   3640/33750 | epoch   1 | batch  265 | loss=2216.6494 | lr=0.01\nstep   3641/33750 | epoch   1 | batch  266 | loss=0.4793 | lr=0.01\nstep   3642/33750 | epoch   1 | batch  267 | loss=11.2716 | lr=0.01\nstep   3643/33750 | epoch   1 | batch  268 | loss=1861.1038 | lr=0.01\nstep   3644/33750 | epoch   1 | batch  269 | loss=236.4405 | lr=0.01\nstep   3645/33750 | epoch   1 | batch  270 | loss=1423.7413 | lr=0.01\nstep   3646/33750 | epoch   1 | batch  271 | loss=82.1467 | lr=0.01\nstep   3647/33750 | epoch   1 | batch  272 | loss=0.0249 | lr=0.01\nstep   3648/33750 | epoch   1 | batch  273 | loss=913.0983 | lr=0.01\nstep   3649/33750 | epoch   1 | batch  274 | loss=15.4864 | lr=0.01\nstep   3650/33750 | epoch   1 | batch  275 | loss=218.0069 | lr=0.01\nstep   3651/33750 | epoch   1 | batch  276 | loss=0.0718 | lr=0.01\nstep   3652/33750 | epoch   1 | batch  277 | loss=540.1448 | lr=0.01\nstep   3653/33750 | epoch   1 | batch  278 | loss=6052.4199 | lr=0.01\nstep   3654/33750 | epoch   1 | batch  279 | loss=9434.0713 | lr=0.01\nstep   3655/33750 | epoch   1 | batch  280 | loss=9.1332 | lr=0.01\nstep   3656/33750 | epoch   1 | batch  281 | loss=0.0424 | lr=0.01\nstep   3657/33750 | epoch   1 | batch  282 | loss=16.6105 | lr=0.01\nstep   3658/33750 | epoch   1 | batch  283 | loss=0.4010 | lr=0.01\nstep   3659/33750 | epoch   1 | batch  284 | loss=0.2621 | lr=0.01\nstep   3660/33750 | epoch   1 | batch  285 | loss=3.7092 | lr=0.01\nstep   3661/33750 | epoch   1 | batch  286 | loss=15.3524 | lr=0.01\nstep   3662/33750 | epoch   1 | batch  287 | loss=216.3333 | lr=0.01\nstep   3663/33750 | epoch   1 | batch  288 | loss=6.3275 | lr=0.01\nstep   3664/33750 | epoch   1 | batch  289 | loss=0.1763 | lr=0.01\nstep   3665/33750 | epoch   1 | batch  290 | loss=539.5301 | lr=0.01\nstep   3666/33750 | epoch   1 | batch  291 | loss=10.6638 | lr=0.01\nstep   3667/33750 | epoch   1 | batch  292 | loss=375.8907 | lr=0.01\nstep   3668/33750 | epoch   1 | batch  293 | loss=1294.6063 | lr=0.01\nstep   3669/33750 | epoch   1 | batch  294 | loss=832.6062 | lr=0.01\nstep   3670/33750 | epoch   1 | batch  295 | loss=746.7214 | lr=0.01\nstep   3671/33750 | epoch   1 | batch  296 | loss=444.0520 | lr=0.01\nstep   3672/33750 | epoch   1 | batch  297 | loss=1.7061 | lr=0.01\nstep   3673/33750 | epoch   1 | batch  298 | loss=0.8047 | lr=0.01\nstep   3674/33750 | epoch   1 | batch  299 | loss=157.9469 | lr=0.01\nstep   3675/33750 | epoch   1 | batch  300 | loss=4.1190 | lr=0.01\nstep   3676/33750 | epoch   1 | batch  301 | loss=1603.2012 | lr=0.01\nstep   3677/33750 | epoch   1 | batch  302 | loss=26.0862 | lr=0.01\nstep   3678/33750 | epoch   1 | batch  303 | loss=1438.5487 | lr=0.01\nstep   3679/33750 | epoch   1 | batch  304 | loss=1.0835 | lr=0.01\nstep   3680/33750 | epoch   1 | batch  305 | loss=16.3541 | lr=0.01\nstep   3681/33750 | epoch   1 | batch  306 | loss=183.5419 | lr=0.01\nstep   3682/33750 | epoch   1 | batch  307 | loss=1962.6331 | lr=0.01\nstep   3683/33750 | epoch   1 | batch  308 | loss=3.2523 | lr=0.01\nstep   3684/33750 | epoch   1 | batch  309 | loss=1005.4919 | lr=0.01\nstep   3685/33750 | epoch   1 | batch  310 | loss=377.0272 | lr=0.01\nstep   3686/33750 | epoch   1 | batch  311 | loss=1585.5057 | lr=0.01\nstep   3687/33750 | epoch   1 | batch  312 | loss=840.7440 | lr=0.01\nstep   3688/33750 | epoch   1 | batch  313 | loss=20429.4922 | lr=0.01\nstep   3689/33750 | epoch   1 | batch  314 | loss=214.0728 | lr=0.01\nstep   3690/33750 | epoch   1 | batch  315 | loss=2.6281 | lr=0.01\nstep   3691/33750 | epoch   1 | batch  316 | loss=61.0771 | lr=0.01\nstep   3692/33750 | epoch   1 | batch  317 | loss=1983.5023 | lr=0.01\nstep   3693/33750 | epoch   1 | batch  318 | loss=123.5202 | lr=0.01\nstep   3694/33750 | epoch   1 | batch  319 | loss=0.0225 | lr=0.01\nstep   3695/33750 | epoch   1 | batch  320 | loss=3562.2117 | lr=0.01\nstep   3696/33750 | epoch   1 | batch  321 | loss=513.9880 | lr=0.01\nstep   3697/33750 | epoch   1 | batch  322 | loss=976.1849 | lr=0.01\nstep   3698/33750 | epoch   1 | batch  323 | loss=0.9697 | lr=0.01\nstep   3699/33750 | epoch   1 | batch  324 | loss=205.4133 | lr=0.01\nstep   3700/33750 | epoch   1 | batch  325 | loss=603.0706 | lr=0.01\nstep   3701/33750 | epoch   1 | batch  326 | loss=337.3573 | lr=0.01\nstep   3702/33750 | epoch   1 | batch  327 | loss=0.0016 | lr=0.01\nstep   3703/33750 | epoch   1 | batch  328 | loss=399.7228 | lr=0.01\nstep   3704/33750 | epoch   1 | batch  329 | loss=1.4398 | lr=0.01\nstep   3705/33750 | epoch   1 | batch  330 | loss=343.5203 | lr=0.01\nstep   3706/33750 | epoch   1 | batch  331 | loss=192.0172 | lr=0.01\nstep   3707/33750 | epoch   1 | batch  332 | loss=2416.9299 | lr=0.01\nstep   3708/33750 | epoch   1 | batch  333 | loss=258.1731 | lr=0.01\nstep   3709/33750 | epoch   1 | batch  334 | loss=4121.6260 | lr=0.01\nstep   3710/33750 | epoch   1 | batch  335 | loss=1208.8574 | lr=0.01\nstep   3711/33750 | epoch   1 | batch  336 | loss=2279.6687 | lr=0.01\nstep   3712/33750 | epoch   1 | batch  337 | loss=538.7082 | lr=0.01\nstep   3713/33750 | epoch   1 | batch  338 | loss=0.0038 | lr=0.01\nstep   3714/33750 | epoch   1 | batch  339 | loss=504.2311 | lr=0.01\nstep   3715/33750 | epoch   1 | batch  340 | loss=62.8309 | lr=0.01\nstep   3716/33750 | epoch   1 | batch  341 | loss=6.3910 | lr=0.01\nstep   3717/33750 | epoch   1 | batch  342 | loss=0.3824 | lr=0.01\nstep   3718/33750 | epoch   1 | batch  343 | loss=0.0007 | lr=0.01\nstep   3719/33750 | epoch   1 | batch  344 | loss=3.2513 | lr=0.01\nstep   3720/33750 | epoch   1 | batch  345 | loss=7.4740 | lr=0.01\nstep   3721/33750 | epoch   1 | batch  346 | loss=1171.8064 | lr=0.01\nstep   3722/33750 | epoch   1 | batch  347 | loss=36.5010 | lr=0.01\nstep   3723/33750 | epoch   1 | batch  348 | loss=212.9328 | lr=0.01\nstep   3724/33750 | epoch   1 | batch  349 | loss=7.4758 | lr=0.01\nstep   3725/33750 | epoch   1 | batch  350 | loss=0.0020 | lr=0.01\nstep   3726/33750 | epoch   1 | batch  351 | loss=611.3664 | lr=0.01\nstep   3727/33750 | epoch   1 | batch  352 | loss=34.6399 | lr=0.01\nstep   3728/33750 | epoch   1 | batch  353 | loss=794.1859 | lr=0.01\nstep   3729/33750 | epoch   1 | batch  354 | loss=1569.8606 | lr=0.01\nstep   3730/33750 | epoch   1 | batch  355 | loss=505.0692 | lr=0.01\nstep   3731/33750 | epoch   1 | batch  356 | loss=0.8080 | lr=0.01\nstep   3732/33750 | epoch   1 | batch  357 | loss=769.0010 | lr=0.01\nstep   3733/33750 | epoch   1 | batch  358 | loss=665.0660 | lr=0.01\nstep   3734/33750 | epoch   1 | batch  359 | loss=321.5769 | lr=0.01\nstep   3735/33750 | epoch   1 | batch  360 | loss=4.3433 | lr=0.01\nstep   3736/33750 | epoch   1 | batch  361 | loss=94.6999 | lr=0.01\nstep   3737/33750 | epoch   1 | batch  362 | loss=284.1766 | lr=0.01\nstep   3738/33750 | epoch   1 | batch  363 | loss=1297.9379 | lr=0.01\nstep   3739/33750 | epoch   1 | batch  364 | loss=248.8314 | lr=0.01\nstep   3740/33750 | epoch   1 | batch  365 | loss=0.0836 | lr=0.01\nstep   3741/33750 | epoch   1 | batch  366 | loss=2.6929 | lr=0.01\nstep   3742/33750 | epoch   1 | batch  367 | loss=0.1058 | lr=0.01\nstep   3743/33750 | epoch   1 | batch  368 | loss=165.3398 | lr=0.01\nstep   3744/33750 | epoch   1 | batch  369 | loss=1.5283 | lr=0.01\nstep   3745/33750 | epoch   1 | batch  370 | loss=353.8449 | lr=0.01\nstep   3746/33750 | epoch   1 | batch  371 | loss=3.1815 | lr=0.01\nstep   3747/33750 | epoch   1 | batch  372 | loss=1.7287 | lr=0.01\nstep   3748/33750 | epoch   1 | batch  373 | loss=259.5314 | lr=0.01\nstep   3749/33750 | epoch   1 | batch  374 | loss=0.0002 | lr=0.01\nstep   3750/33750 | epoch   1 | batch  375 | loss=466.8914 | lr=0.01\nstep   3751/33750 | epoch   1 | batch  376 | loss=1308.5853 | lr=0.01\nstep   3752/33750 | epoch   1 | batch  377 | loss=1.0743 | lr=0.01\nstep   3753/33750 | epoch   1 | batch  378 | loss=517.6601 | lr=0.01\nstep   3754/33750 | epoch   1 | batch  379 | loss=334.6790 | lr=0.01\nstep   3755/33750 | epoch   1 | batch  380 | loss=8345.5947 | lr=0.01\nstep   3756/33750 | epoch   1 | batch  381 | loss=0.0459 | lr=0.01\nstep   3757/33750 | epoch   1 | batch  382 | loss=0.5732 | lr=0.01\nstep   3758/33750 | epoch   1 | batch  383 | loss=0.0243 | lr=0.01\nstep   3759/33750 | epoch   1 | batch  384 | loss=786.9515 | lr=0.01\nstep   3760/33750 | epoch   1 | batch  385 | loss=1286.5903 | lr=0.01\nstep   3761/33750 | epoch   1 | batch  386 | loss=76.9629 | lr=0.01\nstep   3762/33750 | epoch   1 | batch  387 | loss=527.9199 | lr=0.01\nstep   3763/33750 | epoch   1 | batch  388 | loss=2.7468 | lr=0.01\nstep   3764/33750 | epoch   1 | batch  389 | loss=2087.2749 | lr=0.01\nstep   3765/33750 | epoch   1 | batch  390 | loss=1904.3333 | lr=0.01\nstep   3766/33750 | epoch   1 | batch  391 | loss=0.4494 | lr=0.01\nstep   3767/33750 | epoch   1 | batch  392 | loss=1662.3560 | lr=0.01\nstep   3768/33750 | epoch   1 | batch  393 | loss=0.0002 | lr=0.01\nstep   3769/33750 | epoch   1 | batch  394 | loss=0.0003 | lr=0.01\nstep   3770/33750 | epoch   1 | batch  395 | loss=0.0007 | lr=0.01\nstep   3771/33750 | epoch   1 | batch  396 | loss=111.4799 | lr=0.01\nstep   3772/33750 | epoch   1 | batch  397 | loss=4.4977 | lr=0.01\nstep   3773/33750 | epoch   1 | batch  398 | loss=16.7697 | lr=0.01\nstep   3774/33750 | epoch   1 | batch  399 | loss=3.4409 | lr=0.01\nstep   3775/33750 | epoch   1 | batch  400 | loss=842.0187 | lr=0.01\nstep   3776/33750 | epoch   1 | batch  401 | loss=0.1283 | lr=0.01\nstep   3777/33750 | epoch   1 | batch  402 | loss=453.8015 | lr=0.01\nstep   3778/33750 | epoch   1 | batch  403 | loss=363.9555 | lr=0.01\nstep   3779/33750 | epoch   1 | batch  404 | loss=59.1870 | lr=0.01\nstep   3780/33750 | epoch   1 | batch  405 | loss=8.8319 | lr=0.01\nstep   3781/33750 | epoch   1 | batch  406 | loss=180.8818 | lr=0.01\nstep   3782/33750 | epoch   1 | batch  407 | loss=637.9319 | lr=0.01\nstep   3783/33750 | epoch   1 | batch  408 | loss=243.7466 | lr=0.01\nstep   3784/33750 | epoch   1 | batch  409 | loss=0.0028 | lr=0.01\nstep   3785/33750 | epoch   1 | batch  410 | loss=883.1577 | lr=0.01\nstep   3786/33750 | epoch   1 | batch  411 | loss=4.8273 | lr=0.01\nstep   3787/33750 | epoch   1 | batch  412 | loss=0.0014 | lr=0.01\nstep   3788/33750 | epoch   1 | batch  413 | loss=65.4277 | lr=0.01\nstep   3789/33750 | epoch   1 | batch  414 | loss=1545.5414 | lr=0.01\nstep   3790/33750 | epoch   1 | batch  415 | loss=852.9298 | lr=0.01\nstep   3791/33750 | epoch   1 | batch  416 | loss=5.6870 | lr=0.01\nstep   3792/33750 | epoch   1 | batch  417 | loss=0.4269 | lr=0.01\nstep   3793/33750 | epoch   1 | batch  418 | loss=156.2191 | lr=0.01\nstep   3794/33750 | epoch   1 | batch  419 | loss=362.0062 | lr=0.01\nstep   3795/33750 | epoch   1 | batch  420 | loss=0.0019 | lr=0.01\nstep   3796/33750 | epoch   1 | batch  421 | loss=96.6519 | lr=0.01\nstep   3797/33750 | epoch   1 | batch  422 | loss=3.1987 | lr=0.01\nstep   3798/33750 | epoch   1 | batch  423 | loss=409.4872 | lr=0.01\nstep   3799/33750 | epoch   1 | batch  424 | loss=669.1467 | lr=0.01\nstep   3800/33750 | epoch   1 | batch  425 | loss=3.4477 | lr=0.01\nstep   3801/33750 | epoch   1 | batch  426 | loss=1.8619 | lr=0.01\nstep   3802/33750 | epoch   1 | batch  427 | loss=2.8225 | lr=0.01\nstep   3803/33750 | epoch   1 | batch  428 | loss=1639.4058 | lr=0.01\nstep   3804/33750 | epoch   1 | batch  429 | loss=1676.9738 | lr=0.01\nstep   3805/33750 | epoch   1 | batch  430 | loss=0.0296 | lr=0.01\nstep   3806/33750 | epoch   1 | batch  431 | loss=155.1822 | lr=0.01\nstep   3807/33750 | epoch   1 | batch  432 | loss=2.2421 | lr=0.01\nstep   3808/33750 | epoch   1 | batch  433 | loss=0.6234 | lr=0.01\nstep   3809/33750 | epoch   1 | batch  434 | loss=379.9503 | lr=0.01\nstep   3810/33750 | epoch   1 | batch  435 | loss=99.2760 | lr=0.01\nstep   3811/33750 | epoch   1 | batch  436 | loss=26.1985 | lr=0.01\nstep   3812/33750 | epoch   1 | batch  437 | loss=1329.6326 | lr=0.01\nstep   3813/33750 | epoch   1 | batch  438 | loss=722.7701 | lr=0.01\nstep   3814/33750 | epoch   1 | batch  439 | loss=7.0013 | lr=0.01\nstep   3815/33750 | epoch   1 | batch  440 | loss=2.1532 | lr=0.01\nstep   3816/33750 | epoch   1 | batch  441 | loss=307.5097 | lr=0.01\nstep   3817/33750 | epoch   1 | batch  442 | loss=2.3050 | lr=0.01\nstep   3818/33750 | epoch   1 | batch  443 | loss=733.8374 | lr=0.01\nstep   3819/33750 | epoch   1 | batch  444 | loss=1.0416 | lr=0.01\nstep   3820/33750 | epoch   1 | batch  445 | loss=291.8286 | lr=0.01\nstep   3821/33750 | epoch   1 | batch  446 | loss=1076.5043 | lr=0.01\nstep   3822/33750 | epoch   1 | batch  447 | loss=1093.0375 | lr=0.01\nstep   3823/33750 | epoch   1 | batch  448 | loss=555.5233 | lr=0.01\nstep   3824/33750 | epoch   1 | batch  449 | loss=122.6299 | lr=0.01\nstep   3825/33750 | epoch   1 | batch  450 | loss=314.6968 | lr=0.01\nstep   3826/33750 | epoch   1 | batch  451 | loss=10.1280 | lr=0.01\nstep   3827/33750 | epoch   1 | batch  452 | loss=171.8843 | lr=0.01\nstep   3828/33750 | epoch   1 | batch  453 | loss=0.9873 | lr=0.01\nstep   3829/33750 | epoch   1 | batch  454 | loss=0.9721 | lr=0.01\nstep   3830/33750 | epoch   1 | batch  455 | loss=0.9812 | lr=0.01\nstep   3831/33750 | epoch   1 | batch  456 | loss=4020.3464 | lr=0.01\nstep   3832/33750 | epoch   1 | batch  457 | loss=78.4925 | lr=0.01\nstep   3833/33750 | epoch   1 | batch  458 | loss=811.3920 | lr=0.01\nstep   3834/33750 | epoch   1 | batch  459 | loss=2027.1064 | lr=0.01\nstep   3835/33750 | epoch   1 | batch  460 | loss=962.0146 | lr=0.01\nstep   3836/33750 | epoch   1 | batch  461 | loss=2504.6257 | lr=0.01\nstep   3837/33750 | epoch   1 | batch  462 | loss=762.6962 | lr=0.01\nstep   3838/33750 | epoch   1 | batch  463 | loss=31.3155 | lr=0.01\nstep   3839/33750 | epoch   1 | batch  464 | loss=342.1545 | lr=0.01\nstep   3840/33750 | epoch   1 | batch  465 | loss=201.1613 | lr=0.01\nstep   3841/33750 | epoch   1 | batch  466 | loss=156.7483 | lr=0.01\nstep   3842/33750 | epoch   1 | batch  467 | loss=83.3103 | lr=0.01\nstep   3843/33750 | epoch   1 | batch  468 | loss=486.6588 | lr=0.01\nstep   3844/33750 | epoch   1 | batch  469 | loss=661.9733 | lr=0.01\nstep   3845/33750 | epoch   1 | batch  470 | loss=66.0770 | lr=0.01\nstep   3846/33750 | epoch   1 | batch  471 | loss=72.1148 | lr=0.01\nstep   3847/33750 | epoch   1 | batch  472 | loss=1278.0383 | lr=0.01\nstep   3848/33750 | epoch   1 | batch  473 | loss=5.2166 | lr=0.01\nstep   3849/33750 | epoch   1 | batch  474 | loss=162.2845 | lr=0.01\nstep   3850/33750 | epoch   1 | batch  475 | loss=1076.7662 | lr=0.01\nstep   3851/33750 | epoch   1 | batch  476 | loss=36.0370 | lr=0.01\nstep   3852/33750 | epoch   1 | batch  477 | loss=1235.8477 | lr=0.01\nstep   3853/33750 | epoch   1 | batch  478 | loss=1.8956 | lr=0.01\nstep   3854/33750 | epoch   1 | batch  479 | loss=5.5165 | lr=0.01\nstep   3855/33750 | epoch   1 | batch  480 | loss=5665.1240 | lr=0.01\nstep   3856/33750 | epoch   1 | batch  481 | loss=0.0011 | lr=0.01\nstep   3857/33750 | epoch   1 | batch  482 | loss=359.3375 | lr=0.01\nstep   3858/33750 | epoch   1 | batch  483 | loss=352.5099 | lr=0.01\nstep   3859/33750 | epoch   1 | batch  484 | loss=1.2988 | lr=0.01\nstep   3860/33750 | epoch   1 | batch  485 | loss=737.0566 | lr=0.01\nstep   3861/33750 | epoch   1 | batch  486 | loss=2.4824 | lr=0.01\nstep   3862/33750 | epoch   1 | batch  487 | loss=720.3051 | lr=0.01\nstep   3863/33750 | epoch   1 | batch  488 | loss=380.1302 | lr=0.01\nstep   3864/33750 | epoch   1 | batch  489 | loss=984.5643 | lr=0.01\nstep   3865/33750 | epoch   1 | batch  490 | loss=234.4493 | lr=0.01\nstep   3866/33750 | epoch   1 | batch  491 | loss=0.0012 | lr=0.01\nstep   3867/33750 | epoch   1 | batch  492 | loss=0.6532 | lr=0.01\nstep   3868/33750 | epoch   1 | batch  493 | loss=22576.1777 | lr=0.01\nstep   3869/33750 | epoch   1 | batch  494 | loss=572.1302 | lr=0.01\nstep   3870/33750 | epoch   1 | batch  495 | loss=552.1647 | lr=0.01\nstep   3871/33750 | epoch   1 | batch  496 | loss=1900.9006 | lr=0.01\nstep   3872/33750 | epoch   1 | batch  497 | loss=799.4368 | lr=0.01\nstep   3873/33750 | epoch   1 | batch  498 | loss=273.2580 | lr=0.01\nstep   3874/33750 | epoch   1 | batch  499 | loss=344.2848 | lr=0.01\nstep   3875/33750 | epoch   1 | batch  500 | loss=835.0588 | lr=0.01\nstep   3876/33750 | epoch   1 | batch  501 | loss=967.4617 | lr=0.01\nstep   3877/33750 | epoch   1 | batch  502 | loss=873.9991 | lr=0.01\nstep   3878/33750 | epoch   1 | batch  503 | loss=60.7304 | lr=0.01\nstep   3879/33750 | epoch   1 | batch  504 | loss=0.2429 | lr=0.01\nstep   3880/33750 | epoch   1 | batch  505 | loss=1013.4931 | lr=0.01\nstep   3881/33750 | epoch   1 | batch  506 | loss=0.2853 | lr=0.01\nstep   3882/33750 | epoch   1 | batch  507 | loss=514.1645 | lr=0.01\nstep   3883/33750 | epoch   1 | batch  508 | loss=390.1661 | lr=0.01\nstep   3884/33750 | epoch   1 | batch  509 | loss=3154.5178 | lr=0.01\nstep   3885/33750 | epoch   1 | batch  510 | loss=1026.6753 | lr=0.01\nstep   3886/33750 | epoch   1 | batch  511 | loss=7.4088 | lr=0.01\nstep   3887/33750 | epoch   1 | batch  512 | loss=1.8952 | lr=0.01\nstep   3888/33750 | epoch   1 | batch  513 | loss=3.7219 | lr=0.01\nstep   3889/33750 | epoch   1 | batch  514 | loss=0.4830 | lr=0.01\nstep   3890/33750 | epoch   1 | batch  515 | loss=0.0040 | lr=0.01\nstep   3891/33750 | epoch   1 | batch  516 | loss=157.7171 | lr=0.01\nstep   3892/33750 | epoch   1 | batch  517 | loss=500.8375 | lr=0.01\nstep   3893/33750 | epoch   1 | batch  518 | loss=123.5703 | lr=0.01\nstep   3894/33750 | epoch   1 | batch  519 | loss=3.4498 | lr=0.01\nstep   3895/33750 | epoch   1 | batch  520 | loss=838.8361 | lr=0.01\nstep   3896/33750 | epoch   1 | batch  521 | loss=320.6218 | lr=0.01\nstep   3897/33750 | epoch   1 | batch  522 | loss=638.3054 | lr=0.01\nstep   3898/33750 | epoch   1 | batch  523 | loss=0.0006 | lr=0.01\nstep   3899/33750 | epoch   1 | batch  524 | loss=73.0146 | lr=0.01\nstep   3900/33750 | epoch   1 | batch  525 | loss=2149.3721 | lr=0.01\nstep   3901/33750 | epoch   1 | batch  526 | loss=1.7373 | lr=0.01\nstep   3902/33750 | epoch   1 | batch  527 | loss=0.0589 | lr=0.01\nstep   3903/33750 | epoch   1 | batch  528 | loss=88.3255 | lr=0.01\nstep   3904/33750 | epoch   1 | batch  529 | loss=58.3404 | lr=0.01\nstep   3905/33750 | epoch   1 | batch  530 | loss=6.6063 | lr=0.01\nstep   3906/33750 | epoch   1 | batch  531 | loss=1186.0306 | lr=0.01\nstep   3907/33750 | epoch   1 | batch  532 | loss=176.7157 | lr=0.01\nstep   3908/33750 | epoch   1 | batch  533 | loss=339.4555 | lr=0.01\nstep   3909/33750 | epoch   1 | batch  534 | loss=262.0651 | lr=0.01\nstep   3910/33750 | epoch   1 | batch  535 | loss=1179.7932 | lr=0.01\nstep   3911/33750 | epoch   1 | batch  536 | loss=0.0009 | lr=0.01\nstep   3912/33750 | epoch   1 | batch  537 | loss=1.7852 | lr=0.01\nstep   3913/33750 | epoch   1 | batch  538 | loss=293.0234 | lr=0.01\nstep   3914/33750 | epoch   1 | batch  539 | loss=4.0039 | lr=0.01\nstep   3915/33750 | epoch   1 | batch  540 | loss=383.9308 | lr=0.01\nstep   3916/33750 | epoch   1 | batch  541 | loss=504.7795 | lr=0.01\nstep   3917/33750 | epoch   1 | batch  542 | loss=0.2364 | lr=0.01\nstep   3918/33750 | epoch   1 | batch  543 | loss=217.2651 | lr=0.01\nstep   3919/33750 | epoch   1 | batch  544 | loss=0.1840 | lr=0.01\nstep   3920/33750 | epoch   1 | batch  545 | loss=1820.6196 | lr=0.01\nstep   3921/33750 | epoch   1 | batch  546 | loss=8.6239 | lr=0.01\nstep   3922/33750 | epoch   1 | batch  547 | loss=8.4428 | lr=0.01\nstep   3923/33750 | epoch   1 | batch  548 | loss=5.6856 | lr=0.01\nstep   3924/33750 | epoch   1 | batch  549 | loss=0.0030 | lr=0.01\nstep   3925/33750 | epoch   1 | batch  550 | loss=343.1222 | lr=0.01\nstep   3926/33750 | epoch   1 | batch  551 | loss=0.0033 | lr=0.01\nstep   3927/33750 | epoch   1 | batch  552 | loss=1183.4293 | lr=0.01\nstep   3928/33750 | epoch   1 | batch  553 | loss=10.8541 | lr=0.01\nstep   3929/33750 | epoch   1 | batch  554 | loss=648.7673 | lr=0.01\nstep   3930/33750 | epoch   1 | batch  555 | loss=3.7306 | lr=0.01\nstep   3931/33750 | epoch   1 | batch  556 | loss=807.2607 | lr=0.01\nstep   3932/33750 | epoch   1 | batch  557 | loss=1667.1592 | lr=0.01\nstep   3933/33750 | epoch   1 | batch  558 | loss=0.9654 | lr=0.01\nstep   3934/33750 | epoch   1 | batch  559 | loss=108.4504 | lr=0.01\nstep   3935/33750 | epoch   1 | batch  560 | loss=78.4916 | lr=0.01\nstep   3936/33750 | epoch   1 | batch  561 | loss=1.0045 | lr=0.01\nstep   3937/33750 | epoch   1 | batch  562 | loss=58.8271 | lr=0.01\nstep   3938/33750 | epoch   1 | batch  563 | loss=379.1347 | lr=0.01\nstep   3939/33750 | epoch   1 | batch  564 | loss=167.2659 | lr=0.01\nstep   3940/33750 | epoch   1 | batch  565 | loss=0.6226 | lr=0.01\nstep   3941/33750 | epoch   1 | batch  566 | loss=0.0012 | lr=0.01\nstep   3942/33750 | epoch   1 | batch  567 | loss=782.1328 | lr=0.01\nstep   3943/33750 | epoch   1 | batch  568 | loss=7.6217 | lr=0.01\nstep   3944/33750 | epoch   1 | batch  569 | loss=0.0364 | lr=0.01\nstep   3945/33750 | epoch   1 | batch  570 | loss=0.0012 | lr=0.01\nstep   3946/33750 | epoch   1 | batch  571 | loss=7794.0552 | lr=0.01\nstep   3947/33750 | epoch   1 | batch  572 | loss=7.4120 | lr=0.01\nstep   3948/33750 | epoch   1 | batch  573 | loss=674.3128 | lr=0.01\nstep   3949/33750 | epoch   1 | batch  574 | loss=406.5719 | lr=0.01\nstep   3950/33750 | epoch   1 | batch  575 | loss=0.0057 | lr=0.01\nstep   3951/33750 | epoch   1 | batch  576 | loss=4.3901 | lr=0.01\nstep   3952/33750 | epoch   1 | batch  577 | loss=523.6539 | lr=0.01\nstep   3953/33750 | epoch   1 | batch  578 | loss=10.2330 | lr=0.01\nstep   3954/33750 | epoch   1 | batch  579 | loss=4.3018 | lr=0.01\nstep   3955/33750 | epoch   1 | batch  580 | loss=469.9377 | lr=0.01\nstep   3956/33750 | epoch   1 | batch  581 | loss=251.8938 | lr=0.01\nstep   3957/33750 | epoch   1 | batch  582 | loss=0.0010 | lr=0.01\nstep   3958/33750 | epoch   1 | batch  583 | loss=226.8063 | lr=0.01\nstep   3959/33750 | epoch   1 | batch  584 | loss=1030.8086 | lr=0.01\nstep   3960/33750 | epoch   1 | batch  585 | loss=5.1840 | lr=0.01\nstep   3961/33750 | epoch   1 | batch  586 | loss=3.3206 | lr=0.01\nstep   3962/33750 | epoch   1 | batch  587 | loss=2089.6965 | lr=0.01\nstep   3963/33750 | epoch   1 | batch  588 | loss=0.0996 | lr=0.01\nstep   3964/33750 | epoch   1 | batch  589 | loss=0.0448 | lr=0.01\nstep   3965/33750 | epoch   1 | batch  590 | loss=102.5562 | lr=0.01\nstep   3966/33750 | epoch   1 | batch  591 | loss=834.1746 | lr=0.01\nstep   3967/33750 | epoch   1 | batch  592 | loss=1296.6532 | lr=0.01\nstep   3968/33750 | epoch   1 | batch  593 | loss=1380.4978 | lr=0.01\nstep   3969/33750 | epoch   1 | batch  594 | loss=1285.1764 | lr=0.01\nstep   3970/33750 | epoch   1 | batch  595 | loss=200.0719 | lr=0.01\nstep   3971/33750 | epoch   1 | batch  596 | loss=210.7485 | lr=0.01\nstep   3972/33750 | epoch   1 | batch  597 | loss=47.3576 | lr=0.01\nstep   3973/33750 | epoch   1 | batch  598 | loss=243.3385 | lr=0.01\nstep   3974/33750 | epoch   1 | batch  599 | loss=442.1397 | lr=0.01\nstep   3975/33750 | epoch   1 | batch  600 | loss=4850.5146 | lr=0.01\nstep   3976/33750 | epoch   1 | batch  601 | loss=0.0005 | lr=0.01\nstep   3977/33750 | epoch   1 | batch  602 | loss=2478.2583 | lr=0.01\nstep   3978/33750 | epoch   1 | batch  603 | loss=41.4698 | lr=0.01\nstep   3979/33750 | epoch   1 | batch  604 | loss=2.1741 | lr=0.01\nstep   3980/33750 | epoch   1 | batch  605 | loss=1.2145 | lr=0.01\nstep   3981/33750 | epoch   1 | batch  606 | loss=3.9323 | lr=0.01\nstep   3982/33750 | epoch   1 | batch  607 | loss=1.4700 | lr=0.01\nstep   3983/33750 | epoch   1 | batch  608 | loss=1.8440 | lr=0.01\nstep   3984/33750 | epoch   1 | batch  609 | loss=0.7570 | lr=0.01\nstep   3985/33750 | epoch   1 | batch  610 | loss=5780.6118 | lr=0.01\nstep   3986/33750 | epoch   1 | batch  611 | loss=38.3061 | lr=0.01\nstep   3987/33750 | epoch   1 | batch  612 | loss=0.0416 | lr=0.01\nstep   3988/33750 | epoch   1 | batch  613 | loss=406.7681 | lr=0.01\nstep   3989/33750 | epoch   1 | batch  614 | loss=2387.0083 | lr=0.01\nstep   3990/33750 | epoch   1 | batch  615 | loss=447.3148 | lr=0.01\nstep   3991/33750 | epoch   1 | batch  616 | loss=0.5410 | lr=0.01\nstep   3992/33750 | epoch   1 | batch  617 | loss=45.0349 | lr=0.01\nstep   3993/33750 | epoch   1 | batch  618 | loss=1151.9913 | lr=0.01\nstep   3994/33750 | epoch   1 | batch  619 | loss=9.4944 | lr=0.01\nstep   3995/33750 | epoch   1 | batch  620 | loss=1090.0171 | lr=0.01\nstep   3996/33750 | epoch   1 | batch  621 | loss=575.6090 | lr=0.01\nstep   3997/33750 | epoch   1 | batch  622 | loss=182.2555 | lr=0.01\nstep   3998/33750 | epoch   1 | batch  623 | loss=0.0807 | lr=0.01\nstep   3999/33750 | epoch   1 | batch  624 | loss=0.0661 | lr=0.01\nstep   4000/33750 | epoch   1 | batch  625 | loss=0.3304 | lr=0.01\nstep   4001/33750 | epoch   1 | batch  626 | loss=279.4690 | lr=0.01\nstep   4002/33750 | epoch   1 | batch  627 | loss=79.3998 | lr=0.01\nstep   4003/33750 | epoch   1 | batch  628 | loss=144.0365 | lr=0.01\nstep   4004/33750 | epoch   1 | batch  629 | loss=1289.5607 | lr=0.01\nstep   4005/33750 | epoch   1 | batch  630 | loss=67.4627 | lr=0.01\nstep   4006/33750 | epoch   1 | batch  631 | loss=11.0715 | lr=0.01\nstep   4007/33750 | epoch   1 | batch  632 | loss=0.7053 | lr=0.01\nstep   4008/33750 | epoch   1 | batch  633 | loss=1.2392 | lr=0.01\nstep   4009/33750 | epoch   1 | batch  634 | loss=0.2149 | lr=0.01\nstep   4010/33750 | epoch   1 | batch  635 | loss=84.0209 | lr=0.01\nstep   4011/33750 | epoch   1 | batch  636 | loss=465.1898 | lr=0.01\nstep   4012/33750 | epoch   1 | batch  637 | loss=2170.2815 | lr=0.01\nstep   4013/33750 | epoch   1 | batch  638 | loss=3459.9963 | lr=0.01\nstep   4014/33750 | epoch   1 | batch  639 | loss=88.6217 | lr=0.01\nstep   4015/33750 | epoch   1 | batch  640 | loss=423.9208 | lr=0.01\nstep   4016/33750 | epoch   1 | batch  641 | loss=2.8523 | lr=0.01\nstep   4017/33750 | epoch   1 | batch  642 | loss=91.4569 | lr=0.01\nstep   4018/33750 | epoch   1 | batch  643 | loss=30.3985 | lr=0.01\nstep   4019/33750 | epoch   1 | batch  644 | loss=3200.1499 | lr=0.01\nstep   4020/33750 | epoch   1 | batch  645 | loss=123.0184 | lr=0.01\nstep   4021/33750 | epoch   1 | batch  646 | loss=51.3054 | lr=0.01\nstep   4022/33750 | epoch   1 | batch  647 | loss=119.0063 | lr=0.01\nstep   4023/33750 | epoch   1 | batch  648 | loss=0.0066 | lr=0.01\nstep   4024/33750 | epoch   1 | batch  649 | loss=549.8746 | lr=0.01\nstep   4025/33750 | epoch   1 | batch  650 | loss=2.7081 | lr=0.01\nstep   4026/33750 | epoch   1 | batch  651 | loss=0.2946 | lr=0.01\nstep   4027/33750 | epoch   1 | batch  652 | loss=474.7424 | lr=0.01\nstep   4028/33750 | epoch   1 | batch  653 | loss=199.0379 | lr=0.01\nstep   4029/33750 | epoch   1 | batch  654 | loss=318.3156 | lr=0.01\nstep   4030/33750 | epoch   1 | batch  655 | loss=2204.7964 | lr=0.01\nstep   4031/33750 | epoch   1 | batch  656 | loss=0.0389 | lr=0.01\nstep   4032/33750 | epoch   1 | batch  657 | loss=89.9190 | lr=0.01\nstep   4033/33750 | epoch   1 | batch  658 | loss=126.3745 | lr=0.01\nstep   4034/33750 | epoch   1 | batch  659 | loss=79.9623 | lr=0.01\nstep   4035/33750 | epoch   1 | batch  660 | loss=789.3300 | lr=0.01\nstep   4036/33750 | epoch   1 | batch  661 | loss=2.2510 | lr=0.01\nstep   4037/33750 | epoch   1 | batch  662 | loss=524.9603 | lr=0.01\nstep   4038/33750 | epoch   1 | batch  663 | loss=2.0323 | lr=0.01\nstep   4039/33750 | epoch   1 | batch  664 | loss=744.0912 | lr=0.01\nstep   4040/33750 | epoch   1 | batch  665 | loss=3.4829 | lr=0.01\nstep   4041/33750 | epoch   1 | batch  666 | loss=4.4155 | lr=0.01\nstep   4042/33750 | epoch   1 | batch  667 | loss=4.6904 | lr=0.01\nstep   4043/33750 | epoch   1 | batch  668 | loss=285.6018 | lr=0.01\nstep   4044/33750 | epoch   1 | batch  669 | loss=271.4714 | lr=0.01\nstep   4045/33750 | epoch   1 | batch  670 | loss=0.2657 | lr=0.01\nstep   4046/33750 | epoch   1 | batch  671 | loss=2.4463 | lr=0.01\nstep   4047/33750 | epoch   1 | batch  672 | loss=70.4551 | lr=0.01\nstep   4048/33750 | epoch   1 | batch  673 | loss=517.2509 | lr=0.01\nstep   4049/33750 | epoch   1 | batch  674 | loss=693.2886 | lr=0.01\nstep   4050/33750 | epoch   1 | batch  675 | loss=0.1612 | lr=0.01\nstep   4051/33750 | epoch   1 | batch  676 | loss=493.8878 | lr=0.01\nstep   4052/33750 | epoch   1 | batch  677 | loss=3088.2600 | lr=0.01\nstep   4053/33750 | epoch   1 | batch  678 | loss=216.4011 | lr=0.01\nstep   4054/33750 | epoch   1 | batch  679 | loss=700.6730 | lr=0.01\nstep   4055/33750 | epoch   1 | batch  680 | loss=22.6652 | lr=0.01\nstep   4056/33750 | epoch   1 | batch  681 | loss=353.9731 | lr=0.01\nstep   4057/33750 | epoch   1 | batch  682 | loss=0.0008 | lr=0.01\nstep   4058/33750 | epoch   1 | batch  683 | loss=495.8826 | lr=0.01\nstep   4059/33750 | epoch   1 | batch  684 | loss=42.1936 | lr=0.01\nstep   4060/33750 | epoch   1 | batch  685 | loss=0.3533 | lr=0.01\nstep   4061/33750 | epoch   1 | batch  686 | loss=922.5710 | lr=0.01\nstep   4062/33750 | epoch   1 | batch  687 | loss=3.1701 | lr=0.01\nstep   4063/33750 | epoch   1 | batch  688 | loss=232.1955 | lr=0.01\nstep   4064/33750 | epoch   1 | batch  689 | loss=96.7269 | lr=0.01\nstep   4065/33750 | epoch   1 | batch  690 | loss=133.9504 | lr=0.01\nstep   4066/33750 | epoch   1 | batch  691 | loss=1.6848 | lr=0.01\nstep   4067/33750 | epoch   1 | batch  692 | loss=1.7327 | lr=0.01\nstep   4068/33750 | epoch   1 | batch  693 | loss=862.6720 | lr=0.01\nstep   4069/33750 | epoch   1 | batch  694 | loss=8.3381 | lr=0.01\nstep   4070/33750 | epoch   1 | batch  695 | loss=23.5014 | lr=0.01\nstep   4071/33750 | epoch   1 | batch  696 | loss=0.7728 | lr=0.01\nstep   4072/33750 | epoch   1 | batch  697 | loss=837.6922 | lr=0.01\nstep   4073/33750 | epoch   1 | batch  698 | loss=687.9140 | lr=0.01\nstep   4074/33750 | epoch   1 | batch  699 | loss=13.0702 | lr=0.01\nstep   4075/33750 | epoch   1 | batch  700 | loss=10.0225 | lr=0.01\nstep   4076/33750 | epoch   1 | batch  701 | loss=26.0158 | lr=0.01\nstep   4077/33750 | epoch   1 | batch  702 | loss=320.9280 | lr=0.01\nstep   4078/33750 | epoch   1 | batch  703 | loss=994.3810 | lr=0.01\nstep   4079/33750 | epoch   1 | batch  704 | loss=15.3412 | lr=0.01\nstep   4080/33750 | epoch   1 | batch  705 | loss=93.4140 | lr=0.01\nstep   4081/33750 | epoch   1 | batch  706 | loss=10.6553 | lr=0.01\nstep   4082/33750 | epoch   1 | batch  707 | loss=0.6821 | lr=0.01\nstep   4083/33750 | epoch   1 | batch  708 | loss=0.0012 | lr=0.01\nstep   4084/33750 | epoch   1 | batch  709 | loss=125.9804 | lr=0.01\nstep   4085/33750 | epoch   1 | batch  710 | loss=0.6128 | lr=0.01\nstep   4086/33750 | epoch   1 | batch  711 | loss=0.8646 | lr=0.01\nstep   4087/33750 | epoch   1 | batch  712 | loss=1.0075 | lr=0.01\nstep   4088/33750 | epoch   1 | batch  713 | loss=208.9187 | lr=0.01\nstep   4089/33750 | epoch   1 | batch  714 | loss=7597.2212 | lr=0.01\nstep   4090/33750 | epoch   1 | batch  715 | loss=0.2216 | lr=0.01\nstep   4091/33750 | epoch   1 | batch  716 | loss=0.2433 | lr=0.01\nstep   4092/33750 | epoch   1 | batch  717 | loss=11.4219 | lr=0.01\nstep   4093/33750 | epoch   1 | batch  718 | loss=75.9893 | lr=0.01\nstep   4094/33750 | epoch   1 | batch  719 | loss=285.0323 | lr=0.01\nstep   4095/33750 | epoch   1 | batch  720 | loss=151.3850 | lr=0.01\nstep   4096/33750 | epoch   1 | batch  721 | loss=4.8672 | lr=0.01\nstep   4097/33750 | epoch   1 | batch  722 | loss=730.7722 | lr=0.01\nstep   4098/33750 | epoch   1 | batch  723 | loss=1.0396 | lr=0.01\nstep   4099/33750 | epoch   1 | batch  724 | loss=0.0072 | lr=0.01\nstep   4100/33750 | epoch   1 | batch  725 | loss=345.9407 | lr=0.01\nstep   4101/33750 | epoch   1 | batch  726 | loss=0.0054 | lr=0.01\nstep   4102/33750 | epoch   1 | batch  727 | loss=10.9291 | lr=0.01\nstep   4103/33750 | epoch   1 | batch  728 | loss=704.7045 | lr=0.01\nstep   4104/33750 | epoch   1 | batch  729 | loss=30.8924 | lr=0.01\nstep   4105/33750 | epoch   1 | batch  730 | loss=583.5254 | lr=0.01\nstep   4106/33750 | epoch   1 | batch  731 | loss=457.7035 | lr=0.01\nstep   4107/33750 | epoch   1 | batch  732 | loss=0.0015 | lr=0.01\nstep   4108/33750 | epoch   1 | batch  733 | loss=0.7754 | lr=0.01\nstep   4109/33750 | epoch   1 | batch  734 | loss=0.0012 | lr=0.01\nstep   4110/33750 | epoch   1 | batch  735 | loss=0.0498 | lr=0.01\nstep   4111/33750 | epoch   1 | batch  736 | loss=2147.2007 | lr=0.01\nstep   4112/33750 | epoch   1 | batch  737 | loss=1438.5631 | lr=0.01\nstep   4113/33750 | epoch   1 | batch  738 | loss=120.8907 | lr=0.01\nstep   4114/33750 | epoch   1 | batch  739 | loss=0.0014 | lr=0.01\nstep   4115/33750 | epoch   1 | batch  740 | loss=388.5211 | lr=0.01\nstep   4116/33750 | epoch   1 | batch  741 | loss=0.0011 | lr=0.01\nstep   4117/33750 | epoch   1 | batch  742 | loss=2.0535 | lr=0.01\nstep   4118/33750 | epoch   1 | batch  743 | loss=8.6316 | lr=0.01\nstep   4119/33750 | epoch   1 | batch  744 | loss=1.3047 | lr=0.01\nstep   4120/33750 | epoch   1 | batch  745 | loss=114.6946 | lr=0.01\nstep   4121/33750 | epoch   1 | batch  746 | loss=5.9599 | lr=0.01\nstep   4122/33750 | epoch   1 | batch  747 | loss=0.0041 | lr=0.01\nstep   4123/33750 | epoch   1 | batch  748 | loss=1495.0953 | lr=0.01\nstep   4124/33750 | epoch   1 | batch  749 | loss=411.8873 | lr=0.01\nstep   4125/33750 | epoch   1 | batch  750 | loss=1.2886 | lr=0.01\nstep   4126/33750 | epoch   1 | batch  751 | loss=494.1745 | lr=0.01\nstep   4127/33750 | epoch   1 | batch  752 | loss=0.5017 | lr=0.01\nstep   4128/33750 | epoch   1 | batch  753 | loss=0.3541 | lr=0.01\nstep   4129/33750 | epoch   1 | batch  754 | loss=0.0006 | lr=0.01\nstep   4130/33750 | epoch   1 | batch  755 | loss=2.9193 | lr=0.01\nstep   4131/33750 | epoch   1 | batch  756 | loss=168.1159 | lr=0.01\nstep   4132/33750 | epoch   1 | batch  757 | loss=226.4314 | lr=0.01\nstep   4133/33750 | epoch   1 | batch  758 | loss=52.1588 | lr=0.01\nstep   4134/33750 | epoch   1 | batch  759 | loss=757.2902 | lr=0.01\nstep   4135/33750 | epoch   1 | batch  760 | loss=0.2058 | lr=0.01\nstep   4136/33750 | epoch   1 | batch  761 | loss=1313.7251 | lr=0.01\nstep   4137/33750 | epoch   1 | batch  762 | loss=0.1879 | lr=0.01\nstep   4138/33750 | epoch   1 | batch  763 | loss=654.4554 | lr=0.01\nstep   4139/33750 | epoch   1 | batch  764 | loss=1928.4076 | lr=0.01\nstep   4140/33750 | epoch   1 | batch  765 | loss=204.9670 | lr=0.01\nstep   4141/33750 | epoch   1 | batch  766 | loss=0.0008 | lr=0.01\nstep   4142/33750 | epoch   1 | batch  767 | loss=9.3659 | lr=0.01\nstep   4143/33750 | epoch   1 | batch  768 | loss=29.7812 | lr=0.01\nstep   4144/33750 | epoch   1 | batch  769 | loss=5.5228 | lr=0.01\nstep   4145/33750 | epoch   1 | batch  770 | loss=3829.7046 | lr=0.01\nstep   4146/33750 | epoch   1 | batch  771 | loss=0.0008 | lr=0.01\nstep   4147/33750 | epoch   1 | batch  772 | loss=779.3106 | lr=0.01\nstep   4148/33750 | epoch   1 | batch  773 | loss=0.0586 | lr=0.01\nstep   4149/33750 | epoch   1 | batch  774 | loss=76.2480 | lr=0.01\nstep   4150/33750 | epoch   1 | batch  775 | loss=1754.7440 | lr=0.01\nstep   4151/33750 | epoch   1 | batch  776 | loss=800.1214 | lr=0.01\nstep   4152/33750 | epoch   1 | batch  777 | loss=5.1996 | lr=0.01\nstep   4153/33750 | epoch   1 | batch  778 | loss=0.0678 | lr=0.01\nstep   4154/33750 | epoch   1 | batch  779 | loss=527.3640 | lr=0.01\nstep   4155/33750 | epoch   1 | batch  780 | loss=3050.1660 | lr=0.01\nstep   4156/33750 | epoch   1 | batch  781 | loss=169.8955 | lr=0.01\nstep   4157/33750 | epoch   1 | batch  782 | loss=2747.8992 | lr=0.01\nstep   4158/33750 | epoch   1 | batch  783 | loss=256.8235 | lr=0.01\nstep   4159/33750 | epoch   1 | batch  784 | loss=1297.5330 | lr=0.01\nstep   4160/33750 | epoch   1 | batch  785 | loss=10.1965 | lr=0.01\nstep   4161/33750 | epoch   1 | batch  786 | loss=511.2028 | lr=0.01\nstep   4162/33750 | epoch   1 | batch  787 | loss=9.3912 | lr=0.01\nstep   4163/33750 | epoch   1 | batch  788 | loss=494.3509 | lr=0.01\nstep   4164/33750 | epoch   1 | batch  789 | loss=479.3962 | lr=0.01\nstep   4165/33750 | epoch   1 | batch  790 | loss=367.9485 | lr=0.01\nstep   4166/33750 | epoch   1 | batch  791 | loss=1017.8107 | lr=0.01\nstep   4167/33750 | epoch   1 | batch  792 | loss=7.6486 | lr=0.01\nstep   4168/33750 | epoch   1 | batch  793 | loss=0.0032 | lr=0.01\nstep   4169/33750 | epoch   1 | batch  794 | loss=1.1170 | lr=0.01\nstep   4170/33750 | epoch   1 | batch  795 | loss=48.8506 | lr=0.01\nstep   4171/33750 | epoch   1 | batch  796 | loss=1825.7184 | lr=0.01\nstep   4172/33750 | epoch   1 | batch  797 | loss=0.0032 | lr=0.01\nstep   4173/33750 | epoch   1 | batch  798 | loss=0.5204 | lr=0.01\nstep   4174/33750 | epoch   1 | batch  799 | loss=924.9435 | lr=0.01\nstep   4175/33750 | epoch   1 | batch  800 | loss=105.9856 | lr=0.01\nstep   4176/33750 | epoch   1 | batch  801 | loss=329.0650 | lr=0.01\nstep   4177/33750 | epoch   1 | batch  802 | loss=2.6694 | lr=0.01\nstep   4178/33750 | epoch   1 | batch  803 | loss=9.0230 | lr=0.01\nstep   4179/33750 | epoch   1 | batch  804 | loss=366.9766 | lr=0.01\nstep   4180/33750 | epoch   1 | batch  805 | loss=11.7299 | lr=0.01\nstep   4181/33750 | epoch   1 | batch  806 | loss=177.8453 | lr=0.01\nstep   4182/33750 | epoch   1 | batch  807 | loss=2432.1477 | lr=0.01\nstep   4183/33750 | epoch   1 | batch  808 | loss=0.0916 | lr=0.01\nstep   4184/33750 | epoch   1 | batch  809 | loss=258.7469 | lr=0.01\nstep   4185/33750 | epoch   1 | batch  810 | loss=25.3147 | lr=0.01\nstep   4186/33750 | epoch   1 | batch  811 | loss=0.0005 | lr=0.01\nstep   4187/33750 | epoch   1 | batch  812 | loss=415.0411 | lr=0.01\nstep   4188/33750 | epoch   1 | batch  813 | loss=565.8853 | lr=0.01\nstep   4189/33750 | epoch   1 | batch  814 | loss=6.3038 | lr=0.01\nstep   4190/33750 | epoch   1 | batch  815 | loss=0.0832 | lr=0.01\nstep   4191/33750 | epoch   1 | batch  816 | loss=0.0010 | lr=0.01\nstep   4192/33750 | epoch   1 | batch  817 | loss=309.5087 | lr=0.01\nstep   4193/33750 | epoch   1 | batch  818 | loss=4.0811 | lr=0.01\nstep   4194/33750 | epoch   1 | batch  819 | loss=3541.8196 | lr=0.01\nstep   4195/33750 | epoch   1 | batch  820 | loss=30.0132 | lr=0.01\nstep   4196/33750 | epoch   1 | batch  821 | loss=48.7183 | lr=0.01\nstep   4197/33750 | epoch   1 | batch  822 | loss=24.8818 | lr=0.01\nstep   4198/33750 | epoch   1 | batch  823 | loss=2.8876 | lr=0.01\nstep   4199/33750 | epoch   1 | batch  824 | loss=584.4728 | lr=0.01\nstep   4200/33750 | epoch   1 | batch  825 | loss=7.4312 | lr=0.01\nstep   4201/33750 | epoch   1 | batch  826 | loss=398.9396 | lr=0.01\nstep   4202/33750 | epoch   1 | batch  827 | loss=2.0270 | lr=0.01\nstep   4203/33750 | epoch   1 | batch  828 | loss=175.6809 | lr=0.01\nstep   4204/33750 | epoch   1 | batch  829 | loss=184.4638 | lr=0.01\nstep   4205/33750 | epoch   1 | batch  830 | loss=3286.9216 | lr=0.01\nstep   4206/33750 | epoch   1 | batch  831 | loss=3615.4172 | lr=0.01\nstep   4207/33750 | epoch   1 | batch  832 | loss=3297.0447 | lr=0.01\nstep   4208/33750 | epoch   1 | batch  833 | loss=408.1345 | lr=0.01\nstep   4209/33750 | epoch   1 | batch  834 | loss=157.3464 | lr=0.01\nstep   4210/33750 | epoch   1 | batch  835 | loss=1.4220 | lr=0.01\nstep   4211/33750 | epoch   1 | batch  836 | loss=211.2015 | lr=0.01\nstep   4212/33750 | epoch   1 | batch  837 | loss=376.7554 | lr=0.01\nstep   4213/33750 | epoch   1 | batch  838 | loss=3.5701 | lr=0.01\nstep   4214/33750 | epoch   1 | batch  839 | loss=87.3742 | lr=0.01\nstep   4215/33750 | epoch   1 | batch  840 | loss=1989.8555 | lr=0.01\nstep   4216/33750 | epoch   1 | batch  841 | loss=0.0003 | lr=0.01\nstep   4217/33750 | epoch   1 | batch  842 | loss=1684.3463 | lr=0.01\nstep   4218/33750 | epoch   1 | batch  843 | loss=2.3295 | lr=0.01\nstep   4219/33750 | epoch   1 | batch  844 | loss=3.6359 | lr=0.01\nstep   4220/33750 | epoch   1 | batch  845 | loss=21.9135 | lr=0.01\nstep   4221/33750 | epoch   1 | batch  846 | loss=4251.3452 | lr=0.01\nstep   4222/33750 | epoch   1 | batch  847 | loss=161.3898 | lr=0.01\nstep   4223/33750 | epoch   1 | batch  848 | loss=0.0141 | lr=0.01\nstep   4224/33750 | epoch   1 | batch  849 | loss=221.8916 | lr=0.01\nstep   4225/33750 | epoch   1 | batch  850 | loss=283.1007 | lr=0.01\nstep   4226/33750 | epoch   1 | batch  851 | loss=702.0997 | lr=0.01\nstep   4227/33750 | epoch   1 | batch  852 | loss=504.1035 | lr=0.01\nstep   4228/33750 | epoch   1 | batch  853 | loss=548.3422 | lr=0.01\nstep   4229/33750 | epoch   1 | batch  854 | loss=0.0166 | lr=0.01\nstep   4230/33750 | epoch   1 | batch  855 | loss=0.0845 | lr=0.01\nstep   4231/33750 | epoch   1 | batch  856 | loss=9.1553 | lr=0.01\nstep   4232/33750 | epoch   1 | batch  857 | loss=216.3597 | lr=0.01\nstep   4233/33750 | epoch   1 | batch  858 | loss=2.4397 | lr=0.01\nstep   4234/33750 | epoch   1 | batch  859 | loss=129.4090 | lr=0.01\nstep   4235/33750 | epoch   1 | batch  860 | loss=421.5701 | lr=0.01\nstep   4236/33750 | epoch   1 | batch  861 | loss=2.6661 | lr=0.01\nstep   4237/33750 | epoch   1 | batch  862 | loss=234.7209 | lr=0.01\nstep   4238/33750 | epoch   1 | batch  863 | loss=146.6495 | lr=0.01\nstep   4239/33750 | epoch   1 | batch  864 | loss=4.9693 | lr=0.01\nstep   4240/33750 | epoch   1 | batch  865 | loss=6.6464 | lr=0.01\nstep   4241/33750 | epoch   1 | batch  866 | loss=1840.0465 | lr=0.01\nstep   4242/33750 | epoch   1 | batch  867 | loss=4.0000 | lr=0.01\nstep   4243/33750 | epoch   1 | batch  868 | loss=1.7159 | lr=0.01\nstep   4244/33750 | epoch   1 | batch  869 | loss=744.9797 | lr=0.01\nstep   4245/33750 | epoch   1 | batch  870 | loss=231.5102 | lr=0.01\nstep   4246/33750 | epoch   1 | batch  871 | loss=359.2020 | lr=0.01\nstep   4247/33750 | epoch   1 | batch  872 | loss=84.0778 | lr=0.01\nstep   4248/33750 | epoch   1 | batch  873 | loss=0.0854 | lr=0.01\nstep   4249/33750 | epoch   1 | batch  874 | loss=624.2742 | lr=0.01\nstep   4250/33750 | epoch   1 | batch  875 | loss=0.0107 | lr=0.01\nstep   4251/33750 | epoch   1 | batch  876 | loss=0.0058 | lr=0.01\nstep   4252/33750 | epoch   1 | batch  877 | loss=2.4503 | lr=0.01\nstep   4253/33750 | epoch   1 | batch  878 | loss=80.4966 | lr=0.01\nstep   4254/33750 | epoch   1 | batch  879 | loss=318.9057 | lr=0.01\nstep   4255/33750 | epoch   1 | batch  880 | loss=73.9501 | lr=0.01\nstep   4256/33750 | epoch   1 | batch  881 | loss=7.8009 | lr=0.01\nstep   4257/33750 | epoch   1 | batch  882 | loss=187.3686 | lr=0.01\nstep   4258/33750 | epoch   1 | batch  883 | loss=5.6986 | lr=0.01\nstep   4259/33750 | epoch   1 | batch  884 | loss=499.6202 | lr=0.01\nstep   4260/33750 | epoch   1 | batch  885 | loss=0.0167 | lr=0.01\nstep   4261/33750 | epoch   1 | batch  886 | loss=173.5988 | lr=0.01\nstep   4262/33750 | epoch   1 | batch  887 | loss=1836.4407 | lr=0.01\nstep   4263/33750 | epoch   1 | batch  888 | loss=0.0044 | lr=0.01\nstep   4264/33750 | epoch   1 | batch  889 | loss=11.0905 | lr=0.01\nstep   4265/33750 | epoch   1 | batch  890 | loss=565.5897 | lr=0.01\nstep   4266/33750 | epoch   1 | batch  891 | loss=2.2273 | lr=0.01\nstep   4267/33750 | epoch   1 | batch  892 | loss=234.1927 | lr=0.01\nstep   4268/33750 | epoch   1 | batch  893 | loss=1707.6448 | lr=0.01\nstep   4269/33750 | epoch   1 | batch  894 | loss=610.9177 | lr=0.01\nstep   4270/33750 | epoch   1 | batch  895 | loss=0.4053 | lr=0.01\nstep   4271/33750 | epoch   1 | batch  896 | loss=0.1251 | lr=0.01\nstep   4272/33750 | epoch   1 | batch  897 | loss=2837.5190 | lr=0.01\nstep   4273/33750 | epoch   1 | batch  898 | loss=0.6571 | lr=0.01\nstep   4274/33750 | epoch   1 | batch  899 | loss=19.1581 | lr=0.01\nstep   4275/33750 | epoch   1 | batch  900 | loss=1341.7618 | lr=0.01\nstep   4276/33750 | epoch   1 | batch  901 | loss=8008.5859 | lr=0.01\nstep   4277/33750 | epoch   1 | batch  902 | loss=309.0493 | lr=0.01\nstep   4278/33750 | epoch   1 | batch  903 | loss=0.2247 | lr=0.01\nstep   4279/33750 | epoch   1 | batch  904 | loss=23.8555 | lr=0.01\nstep   4280/33750 | epoch   1 | batch  905 | loss=1.2299 | lr=0.01\nstep   4281/33750 | epoch   1 | batch  906 | loss=855.7167 | lr=0.01\nstep   4282/33750 | epoch   1 | batch  907 | loss=2056.4180 | lr=0.01\nstep   4283/33750 | epoch   1 | batch  908 | loss=38.7774 | lr=0.01\nstep   4284/33750 | epoch   1 | batch  909 | loss=11.9735 | lr=0.01\nstep   4285/33750 | epoch   1 | batch  910 | loss=2247.0837 | lr=0.01\nstep   4286/33750 | epoch   1 | batch  911 | loss=0.2563 | lr=0.01\nstep   4287/33750 | epoch   1 | batch  912 | loss=1074.6418 | lr=0.01\nstep   4288/33750 | epoch   1 | batch  913 | loss=707.1224 | lr=0.01\nstep   4289/33750 | epoch   1 | batch  914 | loss=154.5429 | lr=0.01\nstep   4290/33750 | epoch   1 | batch  915 | loss=0.0204 | lr=0.01\nstep   4291/33750 | epoch   1 | batch  916 | loss=2.9631 | lr=0.01\nstep   4292/33750 | epoch   1 | batch  917 | loss=23.2700 | lr=0.01\nstep   4293/33750 | epoch   1 | batch  918 | loss=717.9401 | lr=0.01\nstep   4294/33750 | epoch   1 | batch  919 | loss=168.1146 | lr=0.01\nstep   4295/33750 | epoch   1 | batch  920 | loss=1.3894 | lr=0.01\nstep   4296/33750 | epoch   1 | batch  921 | loss=1239.5162 | lr=0.01\nstep   4297/33750 | epoch   1 | batch  922 | loss=3140.3857 | lr=0.01\nstep   4298/33750 | epoch   1 | batch  923 | loss=547.0058 | lr=0.01\nstep   4299/33750 | epoch   1 | batch  924 | loss=157.1934 | lr=0.01\nstep   4300/33750 | epoch   1 | batch  925 | loss=462.0266 | lr=0.01\nstep   4301/33750 | epoch   1 | batch  926 | loss=27.3151 | lr=0.01\nstep   4302/33750 | epoch   1 | batch  927 | loss=0.2429 | lr=0.01\nstep   4303/33750 | epoch   1 | batch  928 | loss=2194.0710 | lr=0.01\nstep   4304/33750 | epoch   1 | batch  929 | loss=565.4254 | lr=0.01\nstep   4305/33750 | epoch   1 | batch  930 | loss=537.8278 | lr=0.01\nstep   4306/33750 | epoch   1 | batch  931 | loss=6524.8003 | lr=0.01\nstep   4307/33750 | epoch   1 | batch  932 | loss=0.9968 | lr=0.01\nstep   4308/33750 | epoch   1 | batch  933 | loss=10.6878 | lr=0.01\nstep   4309/33750 | epoch   1 | batch  934 | loss=787.3360 | lr=0.01\nstep   4310/33750 | epoch   1 | batch  935 | loss=386.5776 | lr=0.01\nstep   4311/33750 | epoch   1 | batch  936 | loss=912.8901 | lr=0.01\nstep   4312/33750 | epoch   1 | batch  937 | loss=6.6976 | lr=0.01\nstep   4313/33750 | epoch   1 | batch  938 | loss=3346.7297 | lr=0.01\nstep   4314/33750 | epoch   1 | batch  939 | loss=0.0246 | lr=0.01\nstep   4315/33750 | epoch   1 | batch  940 | loss=0.4316 | lr=0.01\nstep   4316/33750 | epoch   1 | batch  941 | loss=0.0005 | lr=0.01\nstep   4317/33750 | epoch   1 | batch  942 | loss=1816.2858 | lr=0.01\nstep   4318/33750 | epoch   1 | batch  943 | loss=1.9493 | lr=0.01\nstep   4319/33750 | epoch   1 | batch  944 | loss=15.4571 | lr=0.01\nstep   4320/33750 | epoch   1 | batch  945 | loss=2.6635 | lr=0.01\nstep   4321/33750 | epoch   1 | batch  946 | loss=784.4326 | lr=0.01\nstep   4322/33750 | epoch   1 | batch  947 | loss=189.0460 | lr=0.01\nstep   4323/33750 | epoch   1 | batch  948 | loss=497.3928 | lr=0.01\nstep   4324/33750 | epoch   1 | batch  949 | loss=0.5238 | lr=0.01\nstep   4325/33750 | epoch   1 | batch  950 | loss=2.4295 | lr=0.01\nstep   4326/33750 | epoch   1 | batch  951 | loss=2.4298 | lr=0.01\nstep   4327/33750 | epoch   1 | batch  952 | loss=0.0289 | lr=0.01\nstep   4328/33750 | epoch   1 | batch  953 | loss=675.5903 | lr=0.01\nstep   4329/33750 | epoch   1 | batch  954 | loss=2.1543 | lr=0.01\nstep   4330/33750 | epoch   1 | batch  955 | loss=0.0081 | lr=0.01\nstep   4331/33750 | epoch   1 | batch  956 | loss=1.6620 | lr=0.01\nstep   4332/33750 | epoch   1 | batch  957 | loss=6.1752 | lr=0.01\nstep   4333/33750 | epoch   1 | batch  958 | loss=944.7578 | lr=0.01\nstep   4334/33750 | epoch   1 | batch  959 | loss=712.8974 | lr=0.01\nstep   4335/33750 | epoch   1 | batch  960 | loss=741.5533 | lr=0.01\nstep   4336/33750 | epoch   1 | batch  961 | loss=3600.2512 | lr=0.01\nstep   4337/33750 | epoch   1 | batch  962 | loss=904.9790 | lr=0.01\nstep   4338/33750 | epoch   1 | batch  963 | loss=1.7769 | lr=0.01\nstep   4339/33750 | epoch   1 | batch  964 | loss=0.0163 | lr=0.01\nstep   4340/33750 | epoch   1 | batch  965 | loss=108.1452 | lr=0.01\nstep   4341/33750 | epoch   1 | batch  966 | loss=1552.0994 | lr=0.01\nstep   4342/33750 | epoch   1 | batch  967 | loss=0.0471 | lr=0.01\nstep   4343/33750 | epoch   1 | batch  968 | loss=213.5783 | lr=0.01\nstep   4344/33750 | epoch   1 | batch  969 | loss=166.3096 | lr=0.01\nstep   4345/33750 | epoch   1 | batch  970 | loss=180.1303 | lr=0.01\nstep   4346/33750 | epoch   1 | batch  971 | loss=19.1301 | lr=0.01\nstep   4347/33750 | epoch   1 | batch  972 | loss=392.0659 | lr=0.01\nstep   4348/33750 | epoch   1 | batch  973 | loss=0.0399 | lr=0.01\nstep   4349/33750 | epoch   1 | batch  974 | loss=5.5861 | lr=0.01\nstep   4350/33750 | epoch   1 | batch  975 | loss=25.0722 | lr=0.01\nstep   4351/33750 | epoch   1 | batch  976 | loss=158.8471 | lr=0.01\nstep   4352/33750 | epoch   1 | batch  977 | loss=2751.8965 | lr=0.01\nstep   4353/33750 | epoch   1 | batch  978 | loss=202.4005 | lr=0.01\nstep   4354/33750 | epoch   1 | batch  979 | loss=1.2538 | lr=0.01\nstep   4355/33750 | epoch   1 | batch  980 | loss=138.8732 | lr=0.01\nstep   4356/33750 | epoch   1 | batch  981 | loss=615.9593 | lr=0.01\nstep   4357/33750 | epoch   1 | batch  982 | loss=523.7993 | lr=0.01\nstep   4358/33750 | epoch   1 | batch  983 | loss=0.0010 | lr=0.01\nstep   4359/33750 | epoch   1 | batch  984 | loss=10.8563 | lr=0.01\nstep   4360/33750 | epoch   1 | batch  985 | loss=0.0478 | lr=0.01\nstep   4361/33750 | epoch   1 | batch  986 | loss=321.2728 | lr=0.01\nstep   4362/33750 | epoch   1 | batch  987 | loss=153.7247 | lr=0.01\nstep   4363/33750 | epoch   1 | batch  988 | loss=1252.7067 | lr=0.01\nstep   4364/33750 | epoch   1 | batch  989 | loss=316.6655 | lr=0.01\nstep   4365/33750 | epoch   1 | batch  990 | loss=927.2073 | lr=0.01\nstep   4366/33750 | epoch   1 | batch  991 | loss=0.8315 | lr=0.01\nstep   4367/33750 | epoch   1 | batch  992 | loss=958.2733 | lr=0.01\nstep   4368/33750 | epoch   1 | batch  993 | loss=244.6956 | lr=0.01\nstep   4369/33750 | epoch   1 | batch  994 | loss=5.1957 | lr=0.01\nstep   4370/33750 | epoch   1 | batch  995 | loss=3.1575 | lr=0.01\nstep   4371/33750 | epoch   1 | batch  996 | loss=408.7542 | lr=0.01\nstep   4372/33750 | epoch   1 | batch  997 | loss=45.5333 | lr=0.01\nstep   4373/33750 | epoch   1 | batch  998 | loss=1.7798 | lr=0.01\nstep   4374/33750 | epoch   1 | batch  999 | loss=794.1883 | lr=0.01\nstep   4375/33750 | epoch   1 | batch 1000 | loss=102.7352 | lr=0.01\nstep   4376/33750 | epoch   1 | batch 1001 | loss=982.9940 | lr=0.01\nstep   4377/33750 | epoch   1 | batch 1002 | loss=0.0686 | lr=0.01\nstep   4378/33750 | epoch   1 | batch 1003 | loss=2.5980 | lr=0.01\nstep   4379/33750 | epoch   1 | batch 1004 | loss=617.9573 | lr=0.01\nstep   4380/33750 | epoch   1 | batch 1005 | loss=2949.4128 | lr=0.01\nstep   4381/33750 | epoch   1 | batch 1006 | loss=870.1332 | lr=0.01\nstep   4382/33750 | epoch   1 | batch 1007 | loss=5.7568 | lr=0.01\nstep   4383/33750 | epoch   1 | batch 1008 | loss=2241.3286 | lr=0.01\nstep   4384/33750 | epoch   1 | batch 1009 | loss=1.4259 | lr=0.01\nstep   4385/33750 | epoch   1 | batch 1010 | loss=21.9572 | lr=0.01\nstep   4386/33750 | epoch   1 | batch 1011 | loss=3.1277 | lr=0.01\nstep   4387/33750 | epoch   1 | batch 1012 | loss=2388.0999 | lr=0.01\nstep   4388/33750 | epoch   1 | batch 1013 | loss=620.0998 | lr=0.01\nstep   4389/33750 | epoch   1 | batch 1014 | loss=13.4738 | lr=0.01\nstep   4390/33750 | epoch   1 | batch 1015 | loss=347.5136 | lr=0.01\nstep   4391/33750 | epoch   1 | batch 1016 | loss=1.1466 | lr=0.01\nstep   4392/33750 | epoch   1 | batch 1017 | loss=298.4989 | lr=0.01\nstep   4393/33750 | epoch   1 | batch 1018 | loss=0.0028 | lr=0.01\nstep   4394/33750 | epoch   1 | batch 1019 | loss=863.4352 | lr=0.01\nstep   4395/33750 | epoch   1 | batch 1020 | loss=1791.8787 | lr=0.01\nstep   4396/33750 | epoch   1 | batch 1021 | loss=217.5960 | lr=0.01\nstep   4397/33750 | epoch   1 | batch 1022 | loss=963.1812 | lr=0.01\nstep   4398/33750 | epoch   1 | batch 1023 | loss=271.3661 | lr=0.01\nstep   4399/33750 | epoch   1 | batch 1024 | loss=1.1615 | lr=0.01\nstep   4400/33750 | epoch   1 | batch 1025 | loss=521.5857 | lr=0.01\nstep   4401/33750 | epoch   1 | batch 1026 | loss=119.5136 | lr=0.01\nstep   4402/33750 | epoch   1 | batch 1027 | loss=628.6576 | lr=0.01\nstep   4403/33750 | epoch   1 | batch 1028 | loss=212.7877 | lr=0.01\nstep   4404/33750 | epoch   1 | batch 1029 | loss=1660.3021 | lr=0.01\nstep   4405/33750 | epoch   1 | batch 1030 | loss=322.3332 | lr=0.01\nstep   4406/33750 | epoch   1 | batch 1031 | loss=800.3381 | lr=0.01\nstep   4407/33750 | epoch   1 | batch 1032 | loss=1499.3600 | lr=0.01\nstep   4408/33750 | epoch   1 | batch 1033 | loss=101.0561 | lr=0.01\nstep   4409/33750 | epoch   1 | batch 1034 | loss=3062.7417 | lr=0.01\nstep   4410/33750 | epoch   1 | batch 1035 | loss=121.4013 | lr=0.01\nstep   4411/33750 | epoch   1 | batch 1036 | loss=1172.2469 | lr=0.01\nstep   4412/33750 | epoch   1 | batch 1037 | loss=11.2746 | lr=0.01\nstep   4413/33750 | epoch   1 | batch 1038 | loss=0.0110 | lr=0.01\nstep   4414/33750 | epoch   1 | batch 1039 | loss=302.1349 | lr=0.01\nstep   4415/33750 | epoch   1 | batch 1040 | loss=362.2903 | lr=0.01\nstep   4416/33750 | epoch   1 | batch 1041 | loss=320.3289 | lr=0.01\nstep   4417/33750 | epoch   1 | batch 1042 | loss=0.6574 | lr=0.01\nstep   4418/33750 | epoch   1 | batch 1043 | loss=1800.7451 | lr=0.01\nstep   4419/33750 | epoch   1 | batch 1044 | loss=795.4418 | lr=0.01\nstep   4420/33750 | epoch   1 | batch 1045 | loss=288.7955 | lr=0.01\nstep   4421/33750 | epoch   1 | batch 1046 | loss=0.1020 | lr=0.01\nstep   4422/33750 | epoch   1 | batch 1047 | loss=0.2022 | lr=0.01\nstep   4423/33750 | epoch   1 | batch 1048 | loss=18.1277 | lr=0.01\nstep   4424/33750 | epoch   1 | batch 1049 | loss=49.7011 | lr=0.01\nstep   4425/33750 | epoch   1 | batch 1050 | loss=268.2853 | lr=0.01\nstep   4426/33750 | epoch   1 | batch 1051 | loss=1621.3842 | lr=0.01\nstep   4427/33750 | epoch   1 | batch 1052 | loss=1.2091 | lr=0.01\nstep   4428/33750 | epoch   1 | batch 1053 | loss=190.0562 | lr=0.01\nstep   4429/33750 | epoch   1 | batch 1054 | loss=169.9705 | lr=0.01\nstep   4430/33750 | epoch   1 | batch 1055 | loss=245.5139 | lr=0.01\nstep   4431/33750 | epoch   1 | batch 1056 | loss=5813.9058 | lr=0.01\nstep   4432/33750 | epoch   1 | batch 1057 | loss=1.4004 | lr=0.01\nstep   4433/33750 | epoch   1 | batch 1058 | loss=818.0724 | lr=0.01\nstep   4434/33750 | epoch   1 | batch 1059 | loss=4.8124 | lr=0.01\nstep   4435/33750 | epoch   1 | batch 1060 | loss=808.6158 | lr=0.01\nstep   4436/33750 | epoch   1 | batch 1061 | loss=6.1685 | lr=0.01\nstep   4437/33750 | epoch   1 | batch 1062 | loss=23.9901 | lr=0.01\nstep   4438/33750 | epoch   1 | batch 1063 | loss=4261.1631 | lr=0.01\nstep   4439/33750 | epoch   1 | batch 1064 | loss=239.5637 | lr=0.01\nstep   4440/33750 | epoch   1 | batch 1065 | loss=0.3698 | lr=0.01\nstep   4441/33750 | epoch   1 | batch 1066 | loss=253.8703 | lr=0.01\nstep   4442/33750 | epoch   1 | batch 1067 | loss=465.4880 | lr=0.01\nstep   4443/33750 | epoch   1 | batch 1068 | loss=266.4553 | lr=0.01\nstep   4444/33750 | epoch   1 | batch 1069 | loss=94.1125 | lr=0.01\nstep   4445/33750 | epoch   1 | batch 1070 | loss=2753.2378 | lr=0.01\nstep   4446/33750 | epoch   1 | batch 1071 | loss=0.0016 | lr=0.01\nstep   4447/33750 | epoch   1 | batch 1072 | loss=0.3747 | lr=0.01\nstep   4448/33750 | epoch   1 | batch 1073 | loss=2.6554 | lr=0.01\nstep   4449/33750 | epoch   1 | batch 1074 | loss=2377.3203 | lr=0.01\nstep   4450/33750 | epoch   1 | batch 1075 | loss=478.4238 | lr=0.01\nstep   4451/33750 | epoch   1 | batch 1076 | loss=196.6438 | lr=0.01\nstep   4452/33750 | epoch   1 | batch 1077 | loss=0.0012 | lr=0.01\nstep   4453/33750 | epoch   1 | batch 1078 | loss=2047.1396 | lr=0.01\nstep   4454/33750 | epoch   1 | batch 1079 | loss=5.8984 | lr=0.01\nstep   4455/33750 | epoch   1 | batch 1080 | loss=2.8392 | lr=0.01\nstep   4456/33750 | epoch   1 | batch 1081 | loss=3.0701 | lr=0.01\nstep   4457/33750 | epoch   1 | batch 1082 | loss=6.4448 | lr=0.01\nstep   4458/33750 | epoch   1 | batch 1083 | loss=344.7503 | lr=0.01\nstep   4459/33750 | epoch   1 | batch 1084 | loss=3.4233 | lr=0.01\nstep   4460/33750 | epoch   1 | batch 1085 | loss=76.4632 | lr=0.01\nstep   4461/33750 | epoch   1 | batch 1086 | loss=91.2815 | lr=0.01\nstep   4462/33750 | epoch   1 | batch 1087 | loss=0.0412 | lr=0.01\nstep   4463/33750 | epoch   1 | batch 1088 | loss=1722.2909 | lr=0.01\nstep   4464/33750 | epoch   1 | batch 1089 | loss=0.0072 | lr=0.01\nstep   4465/33750 | epoch   1 | batch 1090 | loss=5.3541 | lr=0.01\nstep   4466/33750 | epoch   1 | batch 1091 | loss=3.9443 | lr=0.01\nstep   4467/33750 | epoch   1 | batch 1092 | loss=933.5933 | lr=0.01\nstep   4468/33750 | epoch   1 | batch 1093 | loss=794.9789 | lr=0.01\nstep   4469/33750 | epoch   1 | batch 1094 | loss=10.1430 | lr=0.01\nstep   4470/33750 | epoch   1 | batch 1095 | loss=277.8002 | lr=0.01\nstep   4471/33750 | epoch   1 | batch 1096 | loss=93.1356 | lr=0.01\nstep   4472/33750 | epoch   1 | batch 1097 | loss=235.4524 | lr=0.01\nstep   4473/33750 | epoch   1 | batch 1098 | loss=0.2456 | lr=0.01\nstep   4474/33750 | epoch   1 | batch 1099 | loss=2884.0212 | lr=0.01\nstep   4475/33750 | epoch   1 | batch 1100 | loss=0.1814 | lr=0.01\nstep   4476/33750 | epoch   1 | batch 1101 | loss=76.8196 | lr=0.01\nstep   4477/33750 | epoch   1 | batch 1102 | loss=3.6862 | lr=0.01\nstep   4478/33750 | epoch   1 | batch 1103 | loss=283.6214 | lr=0.01\nstep   4479/33750 | epoch   1 | batch 1104 | loss=1297.2764 | lr=0.01\nstep   4480/33750 | epoch   1 | batch 1105 | loss=0.2504 | lr=0.01\nstep   4481/33750 | epoch   1 | batch 1106 | loss=41.4843 | lr=0.01\nstep   4482/33750 | epoch   1 | batch 1107 | loss=17.8878 | lr=0.01\nstep   4483/33750 | epoch   1 | batch 1108 | loss=0.1954 | lr=0.01\nstep   4484/33750 | epoch   1 | batch 1109 | loss=736.0626 | lr=0.01\nstep   4485/33750 | epoch   1 | batch 1110 | loss=973.4373 | lr=0.01\nstep   4486/33750 | epoch   1 | batch 1111 | loss=7.5348 | lr=0.01\nstep   4487/33750 | epoch   1 | batch 1112 | loss=529.2547 | lr=0.01\nstep   4488/33750 | epoch   1 | batch 1113 | loss=5.9921 | lr=0.01\nstep   4489/33750 | epoch   1 | batch 1114 | loss=0.3227 | lr=0.01\nstep   4490/33750 | epoch   1 | batch 1115 | loss=2713.2642 | lr=0.01\nstep   4491/33750 | epoch   1 | batch 1116 | loss=3481.7000 | lr=0.01\nstep   4492/33750 | epoch   1 | batch 1117 | loss=0.0015 | lr=0.01\nstep   4493/33750 | epoch   1 | batch 1118 | loss=1972.4905 | lr=0.01\nstep   4494/33750 | epoch   1 | batch 1119 | loss=2502.0435 | lr=0.01\nstep   4495/33750 | epoch   1 | batch 1120 | loss=0.0396 | lr=0.01\nstep   4496/33750 | epoch   1 | batch 1121 | loss=622.9952 | lr=0.01\nstep   4497/33750 | epoch   1 | batch 1122 | loss=1978.6149 | lr=0.01\nstep   4498/33750 | epoch   1 | batch 1123 | loss=1566.7000 | lr=0.01\nstep   4499/33750 | epoch   1 | batch 1124 | loss=0.0008 | lr=0.01\nstep   4500/33750 | epoch   1 | batch 1125 | loss=1721.1237 | lr=0.01\nstep   4501/33750 | epoch   1 | batch 1126 | loss=2.6062 | lr=0.01\nstep   4502/33750 | epoch   1 | batch 1127 | loss=4.5848 | lr=0.01\nstep   4503/33750 | epoch   1 | batch 1128 | loss=1169.6854 | lr=0.01\nstep   4504/33750 | epoch   1 | batch 1129 | loss=7.9579 | lr=0.01\nstep   4505/33750 | epoch   1 | batch 1130 | loss=3.5779 | lr=0.01\nstep   4506/33750 | epoch   1 | batch 1131 | loss=639.5806 | lr=0.01\nstep   4507/33750 | epoch   1 | batch 1132 | loss=78.8616 | lr=0.01\nstep   4508/33750 | epoch   1 | batch 1133 | loss=3.7435 | lr=0.01\nstep   4509/33750 | epoch   1 | batch 1134 | loss=108.9246 | lr=0.01\nstep   4510/33750 | epoch   1 | batch 1135 | loss=35.3597 | lr=0.01\nstep   4511/33750 | epoch   1 | batch 1136 | loss=3.2668 | lr=0.01\nstep   4512/33750 | epoch   1 | batch 1137 | loss=0.0100 | lr=0.01\nstep   4513/33750 | epoch   1 | batch 1138 | loss=312.2038 | lr=0.01\nstep   4514/33750 | epoch   1 | batch 1139 | loss=63.1142 | lr=0.01\nstep   4515/33750 | epoch   1 | batch 1140 | loss=744.0500 | lr=0.01\nstep   4516/33750 | epoch   1 | batch 1141 | loss=795.2580 | lr=0.01\nstep   4517/33750 | epoch   1 | batch 1142 | loss=430.1954 | lr=0.01\nstep   4518/33750 | epoch   1 | batch 1143 | loss=783.0097 | lr=0.01\nstep   4519/33750 | epoch   1 | batch 1144 | loss=406.2811 | lr=0.01\nstep   4520/33750 | epoch   1 | batch 1145 | loss=3.1221 | lr=0.01\nstep   4521/33750 | epoch   1 | batch 1146 | loss=0.2246 | lr=0.01\nstep   4522/33750 | epoch   1 | batch 1147 | loss=0.0409 | lr=0.01\nstep   4523/33750 | epoch   1 | batch 1148 | loss=1687.3027 | lr=0.01\nstep   4524/33750 | epoch   1 | batch 1149 | loss=4.8572 | lr=0.01\nstep   4525/33750 | epoch   1 | batch 1150 | loss=1758.8687 | lr=0.01\nstep   4526/33750 | epoch   1 | batch 1151 | loss=0.0253 | lr=0.01\nstep   4527/33750 | epoch   1 | batch 1152 | loss=494.5575 | lr=0.01\nstep   4528/33750 | epoch   1 | batch 1153 | loss=51.0151 | lr=0.01\nstep   4529/33750 | epoch   1 | batch 1154 | loss=0.0051 | lr=0.01\nstep   4530/33750 | epoch   1 | batch 1155 | loss=2.1006 | lr=0.01\nstep   4531/33750 | epoch   1 | batch 1156 | loss=1165.6151 | lr=0.01\nstep   4532/33750 | epoch   1 | batch 1157 | loss=509.6451 | lr=0.01\nstep   4533/33750 | epoch   1 | batch 1158 | loss=180.4884 | lr=0.01\nstep   4534/33750 | epoch   1 | batch 1159 | loss=552.2101 | lr=0.01\nstep   4535/33750 | epoch   1 | batch 1160 | loss=17.1890 | lr=0.01\nstep   4536/33750 | epoch   1 | batch 1161 | loss=114.0237 | lr=0.01\nstep   4537/33750 | epoch   1 | batch 1162 | loss=2610.9609 | lr=0.01\nstep   4538/33750 | epoch   1 | batch 1163 | loss=1.9877 | lr=0.01\nstep   4539/33750 | epoch   1 | batch 1164 | loss=536.2753 | lr=0.01\nstep   4540/33750 | epoch   1 | batch 1165 | loss=2.5969 | lr=0.01\nstep   4541/33750 | epoch   1 | batch 1166 | loss=1879.0516 | lr=0.01\nstep   4542/33750 | epoch   1 | batch 1167 | loss=0.7763 | lr=0.01\nstep   4543/33750 | epoch   1 | batch 1168 | loss=5.5563 | lr=0.01\nstep   4544/33750 | epoch   1 | batch 1169 | loss=0.0009 | lr=0.01\nstep   4545/33750 | epoch   1 | batch 1170 | loss=1.0623 | lr=0.01\nstep   4546/33750 | epoch   1 | batch 1171 | loss=1.5049 | lr=0.01\nstep   4547/33750 | epoch   1 | batch 1172 | loss=0.0005 | lr=0.01\nstep   4548/33750 | epoch   1 | batch 1173 | loss=72.4219 | lr=0.01\nstep   4549/33750 | epoch   1 | batch 1174 | loss=5.2889 | lr=0.01\nstep   4550/33750 | epoch   1 | batch 1175 | loss=0.5136 | lr=0.01\nstep   4551/33750 | epoch   1 | batch 1176 | loss=2686.0413 | lr=0.01\nstep   4552/33750 | epoch   1 | batch 1177 | loss=1148.4089 | lr=0.01\nstep   4553/33750 | epoch   1 | batch 1178 | loss=167.3201 | lr=0.01\nstep   4554/33750 | epoch   1 | batch 1179 | loss=805.5539 | lr=0.01\nstep   4555/33750 | epoch   1 | batch 1180 | loss=2046.8773 | lr=0.01\nstep   4556/33750 | epoch   1 | batch 1181 | loss=0.0902 | lr=0.01\nstep   4557/33750 | epoch   1 | batch 1182 | loss=981.4391 | lr=0.01\nstep   4558/33750 | epoch   1 | batch 1183 | loss=4195.0903 | lr=0.01\nstep   4559/33750 | epoch   1 | batch 1184 | loss=14.6563 | lr=0.01\nstep   4560/33750 | epoch   1 | batch 1185 | loss=1989.5260 | lr=0.01\nstep   4561/33750 | epoch   1 | batch 1186 | loss=90.2370 | lr=0.01\nstep   4562/33750 | epoch   1 | batch 1187 | loss=20.5869 | lr=0.01\nstep   4563/33750 | epoch   1 | batch 1188 | loss=0.1143 | lr=0.01\nstep   4564/33750 | epoch   1 | batch 1189 | loss=0.2380 | lr=0.01\nstep   4565/33750 | epoch   1 | batch 1190 | loss=0.5093 | lr=0.01\nstep   4566/33750 | epoch   1 | batch 1191 | loss=275.8956 | lr=0.01\nstep   4567/33750 | epoch   1 | batch 1192 | loss=1567.7908 | lr=0.01\nstep   4568/33750 | epoch   1 | batch 1193 | loss=1406.5972 | lr=0.01\nstep   4569/33750 | epoch   1 | batch 1194 | loss=238.2305 | lr=0.01\nstep   4570/33750 | epoch   1 | batch 1195 | loss=0.0934 | lr=0.01\nstep   4571/33750 | epoch   1 | batch 1196 | loss=729.8975 | lr=0.01\nstep   4572/33750 | epoch   1 | batch 1197 | loss=7.4102 | lr=0.01\nstep   4573/33750 | epoch   1 | batch 1198 | loss=3.5613 | lr=0.01\nstep   4574/33750 | epoch   1 | batch 1199 | loss=3.5705 | lr=0.01\nstep   4575/33750 | epoch   1 | batch 1200 | loss=3530.3542 | lr=0.01\nstep   4576/33750 | epoch   1 | batch 1201 | loss=7.2491 | lr=0.01\nstep   4577/33750 | epoch   1 | batch 1202 | loss=904.1515 | lr=0.01\nstep   4578/33750 | epoch   1 | batch 1203 | loss=0.1555 | lr=0.01\nstep   4579/33750 | epoch   1 | batch 1204 | loss=3403.9478 | lr=0.01\nstep   4580/33750 | epoch   1 | batch 1205 | loss=16.7583 | lr=0.01\nstep   4581/33750 | epoch   1 | batch 1206 | loss=5198.8149 | lr=0.01\nstep   4582/33750 | epoch   1 | batch 1207 | loss=10.5068 | lr=0.01\nstep   4583/33750 | epoch   1 | batch 1208 | loss=5224.9731 | lr=0.01\nstep   4584/33750 | epoch   1 | batch 1209 | loss=2526.7432 | lr=0.01\nstep   4585/33750 | epoch   1 | batch 1210 | loss=2787.2109 | lr=0.01\nstep   4586/33750 | epoch   1 | batch 1211 | loss=592.4153 | lr=0.01\nstep   4587/33750 | epoch   1 | batch 1212 | loss=1184.5425 | lr=0.01\nstep   4588/33750 | epoch   1 | batch 1213 | loss=666.1356 | lr=0.01\nstep   4589/33750 | epoch   1 | batch 1214 | loss=0.1458 | lr=0.01\nstep   4590/33750 | epoch   1 | batch 1215 | loss=1212.4569 | lr=0.01\nstep   4591/33750 | epoch   1 | batch 1216 | loss=1.8631 | lr=0.01\nstep   4592/33750 | epoch   1 | batch 1217 | loss=480.0331 | lr=0.01\nstep   4593/33750 | epoch   1 | batch 1218 | loss=1673.4135 | lr=0.01\nstep   4594/33750 | epoch   1 | batch 1219 | loss=42.6123 | lr=0.01\nstep   4595/33750 | epoch   1 | batch 1220 | loss=2.6126 | lr=0.01\nstep   4596/33750 | epoch   1 | batch 1221 | loss=2.7379 | lr=0.01\nstep   4597/33750 | epoch   1 | batch 1222 | loss=629.2244 | lr=0.01\nstep   4598/33750 | epoch   1 | batch 1223 | loss=567.8957 | lr=0.01\nstep   4599/33750 | epoch   1 | batch 1224 | loss=3.0900 | lr=0.01\nstep   4600/33750 | epoch   1 | batch 1225 | loss=52.1987 | lr=0.01\nstep   4601/33750 | epoch   1 | batch 1226 | loss=0.2308 | lr=0.01\nstep   4602/33750 | epoch   1 | batch 1227 | loss=2727.3718 | lr=0.01\nstep   4603/33750 | epoch   1 | batch 1228 | loss=0.0703 | lr=0.01\nstep   4604/33750 | epoch   1 | batch 1229 | loss=6188.1465 | lr=0.01\nstep   4605/33750 | epoch   1 | batch 1230 | loss=295.2043 | lr=0.01\nstep   4606/33750 | epoch   1 | batch 1231 | loss=19.3173 | lr=0.01\nstep   4607/33750 | epoch   1 | batch 1232 | loss=4398.1890 | lr=0.01\nstep   4608/33750 | epoch   1 | batch 1233 | loss=3772.8027 | lr=0.01\nstep   4609/33750 | epoch   1 | batch 1234 | loss=8.3370 | lr=0.01\nstep   4610/33750 | epoch   1 | batch 1235 | loss=6945.4800 | lr=0.01\nstep   4611/33750 | epoch   1 | batch 1236 | loss=876.0875 | lr=0.01\nstep   4612/33750 | epoch   1 | batch 1237 | loss=430.3537 | lr=0.01\nstep   4613/33750 | epoch   1 | batch 1238 | loss=0.0315 | lr=0.01\nstep   4614/33750 | epoch   1 | batch 1239 | loss=3250.0227 | lr=0.01\nstep   4615/33750 | epoch   1 | batch 1240 | loss=19.6546 | lr=0.01\nstep   4616/33750 | epoch   1 | batch 1241 | loss=81.1319 | lr=0.01\nstep   4617/33750 | epoch   1 | batch 1242 | loss=577.2707 | lr=0.01\nstep   4618/33750 | epoch   1 | batch 1243 | loss=14.1670 | lr=0.01\nstep   4619/33750 | epoch   1 | batch 1244 | loss=1050.3835 | lr=0.01\nstep   4620/33750 | epoch   1 | batch 1245 | loss=210.0635 | lr=0.01\nstep   4621/33750 | epoch   1 | batch 1246 | loss=166.0813 | lr=0.01\nstep   4622/33750 | epoch   1 | batch 1247 | loss=30.8231 | lr=0.01\nstep   4623/33750 | epoch   1 | batch 1248 | loss=1120.0604 | lr=0.01\nstep   4624/33750 | epoch   1 | batch 1249 | loss=1428.4469 | lr=0.01\nstep   4625/33750 | epoch   1 | batch 1250 | loss=56.6101 | lr=0.01\nstep   4626/33750 | epoch   1 | batch 1251 | loss=1408.1068 | lr=0.01\nstep   4627/33750 | epoch   1 | batch 1252 | loss=103.4754 | lr=0.01\nstep   4628/33750 | epoch   1 | batch 1253 | loss=925.4170 | lr=0.01\nstep   4629/33750 | epoch   1 | batch 1254 | loss=204.7137 | lr=0.01\nstep   4630/33750 | epoch   1 | batch 1255 | loss=8.8772 | lr=0.01\nstep   4631/33750 | epoch   1 | batch 1256 | loss=8.4244 | lr=0.01\nstep   4632/33750 | epoch   1 | batch 1257 | loss=120.6624 | lr=0.01\nstep   4633/33750 | epoch   1 | batch 1258 | loss=30.9607 | lr=0.01\nstep   4634/33750 | epoch   1 | batch 1259 | loss=0.4456 | lr=0.01\nstep   4635/33750 | epoch   1 | batch 1260 | loss=19.1408 | lr=0.01\nstep   4636/33750 | epoch   1 | batch 1261 | loss=446.8542 | lr=0.01\nstep   4637/33750 | epoch   1 | batch 1262 | loss=9666.1396 | lr=0.01\nstep   4638/33750 | epoch   1 | batch 1263 | loss=38.2240 | lr=0.01\nstep   4639/33750 | epoch   1 | batch 1264 | loss=13.6042 | lr=0.01\nstep   4640/33750 | epoch   1 | batch 1265 | loss=1002.7350 | lr=0.01\nstep   4641/33750 | epoch   1 | batch 1266 | loss=1164.3551 | lr=0.01\nstep   4642/33750 | epoch   1 | batch 1267 | loss=24.0629 | lr=0.01\nstep   4643/33750 | epoch   1 | batch 1268 | loss=9.0874 | lr=0.01\nstep   4644/33750 | epoch   1 | batch 1269 | loss=0.3233 | lr=0.01\nstep   4645/33750 | epoch   1 | batch 1270 | loss=0.8929 | lr=0.01\nstep   4646/33750 | epoch   1 | batch 1271 | loss=428.1839 | lr=0.01\nstep   4647/33750 | epoch   1 | batch 1272 | loss=46.1151 | lr=0.01\nstep   4648/33750 | epoch   1 | batch 1273 | loss=134.9445 | lr=0.01\nstep   4649/33750 | epoch   1 | batch 1274 | loss=8.0131 | lr=0.01\nstep   4650/33750 | epoch   1 | batch 1275 | loss=534.1573 | lr=0.01\nstep   4651/33750 | epoch   1 | batch 1276 | loss=0.2988 | lr=0.01\nstep   4652/33750 | epoch   1 | batch 1277 | loss=237.4385 | lr=0.01\nstep   4653/33750 | epoch   1 | batch 1278 | loss=517.6293 | lr=0.01\nstep   4654/33750 | epoch   1 | batch 1279 | loss=1.6044 | lr=0.01\nstep   4655/33750 | epoch   1 | batch 1280 | loss=117.6064 | lr=0.01\nstep   4656/33750 | epoch   1 | batch 1281 | loss=348.8831 | lr=0.01\nstep   4657/33750 | epoch   1 | batch 1282 | loss=220.4168 | lr=0.01\nstep   4658/33750 | epoch   1 | batch 1283 | loss=1036.3813 | lr=0.01\nstep   4659/33750 | epoch   1 | batch 1284 | loss=795.8193 | lr=0.01\nstep   4660/33750 | epoch   1 | batch 1285 | loss=8536.3193 | lr=0.01\nstep   4661/33750 | epoch   1 | batch 1286 | loss=2.8479 | lr=0.01\nstep   4662/33750 | epoch   1 | batch 1287 | loss=3359.8611 | lr=0.01\nstep   4663/33750 | epoch   1 | batch 1288 | loss=1.7065 | lr=0.01\nstep   4664/33750 | epoch   1 | batch 1289 | loss=60.8911 | lr=0.01\nstep   4665/33750 | epoch   1 | batch 1290 | loss=53.2413 | lr=0.01\nstep   4666/33750 | epoch   1 | batch 1291 | loss=2830.9475 | lr=0.01\nstep   4667/33750 | epoch   1 | batch 1292 | loss=502.6211 | lr=0.01\nstep   4668/33750 | epoch   1 | batch 1293 | loss=1107.8475 | lr=0.01\nstep   4669/33750 | epoch   1 | batch 1294 | loss=2.4984 | lr=0.01\nstep   4670/33750 | epoch   1 | batch 1295 | loss=710.5237 | lr=0.01\nstep   4671/33750 | epoch   1 | batch 1296 | loss=16.7617 | lr=0.01\nstep   4672/33750 | epoch   1 | batch 1297 | loss=3497.8708 | lr=0.01\nstep   4673/33750 | epoch   1 | batch 1298 | loss=62.3083 | lr=0.01\nstep   4674/33750 | epoch   1 | batch 1299 | loss=206.5570 | lr=0.01\nstep   4675/33750 | epoch   1 | batch 1300 | loss=230.4739 | lr=0.01\nstep   4676/33750 | epoch   1 | batch 1301 | loss=510.4139 | lr=0.01\nstep   4677/33750 | epoch   1 | batch 1302 | loss=143.1722 | lr=0.01\nstep   4678/33750 | epoch   1 | batch 1303 | loss=22.0333 | lr=0.01\nstep   4679/33750 | epoch   1 | batch 1304 | loss=71.5570 | lr=0.01\nstep   4680/33750 | epoch   1 | batch 1305 | loss=445.5136 | lr=0.01\nstep   4681/33750 | epoch   1 | batch 1306 | loss=0.0805 | lr=0.01\nstep   4682/33750 | epoch   1 | batch 1307 | loss=1.6341 | lr=0.01\nstep   4683/33750 | epoch   1 | batch 1308 | loss=1155.7882 | lr=0.01\nstep   4684/33750 | epoch   1 | batch 1309 | loss=530.9991 | lr=0.01\nstep   4685/33750 | epoch   1 | batch 1310 | loss=569.8235 | lr=0.01\nstep   4686/33750 | epoch   1 | batch 1311 | loss=235.7852 | lr=0.01\nstep   4687/33750 | epoch   1 | batch 1312 | loss=46.5761 | lr=0.01\nstep   4688/33750 | epoch   1 | batch 1313 | loss=654.1328 | lr=0.01\nstep   4689/33750 | epoch   1 | batch 1314 | loss=187.3134 | lr=0.01\nstep   4690/33750 | epoch   1 | batch 1315 | loss=3.5628 | lr=0.01\nstep   4691/33750 | epoch   1 | batch 1316 | loss=8.2923 | lr=0.01\nstep   4692/33750 | epoch   1 | batch 1317 | loss=26.5272 | lr=0.01\nstep   4693/33750 | epoch   1 | batch 1318 | loss=25.0612 | lr=0.01\nstep   4694/33750 | epoch   1 | batch 1319 | loss=5.2992 | lr=0.01\nstep   4695/33750 | epoch   1 | batch 1320 | loss=12.4194 | lr=0.01\nstep   4696/33750 | epoch   1 | batch 1321 | loss=323.9063 | lr=0.01\nstep   4697/33750 | epoch   1 | batch 1322 | loss=3.0855 | lr=0.01\nstep   4698/33750 | epoch   1 | batch 1323 | loss=774.9539 | lr=0.01\nstep   4699/33750 | epoch   1 | batch 1324 | loss=330.6206 | lr=0.01\nstep   4700/33750 | epoch   1 | batch 1325 | loss=0.1079 | lr=0.01\nstep   4701/33750 | epoch   1 | batch 1326 | loss=379.9588 | lr=0.01\nstep   4702/33750 | epoch   1 | batch 1327 | loss=0.1438 | lr=0.01\nstep   4703/33750 | epoch   1 | batch 1328 | loss=1.8961 | lr=0.01\nstep   4704/33750 | epoch   1 | batch 1329 | loss=242.5456 | lr=0.01\nstep   4705/33750 | epoch   1 | batch 1330 | loss=0.4628 | lr=0.01\nstep   4706/33750 | epoch   1 | batch 1331 | loss=0.0583 | lr=0.01\nstep   4707/33750 | epoch   1 | batch 1332 | loss=1036.2030 | lr=0.01\nstep   4708/33750 | epoch   1 | batch 1333 | loss=4.2382 | lr=0.01\nstep   4709/33750 | epoch   1 | batch 1334 | loss=10.7847 | lr=0.01\nstep   4710/33750 | epoch   1 | batch 1335 | loss=0.0282 | lr=0.01\nstep   4711/33750 | epoch   1 | batch 1336 | loss=51.1561 | lr=0.01\nstep   4712/33750 | epoch   1 | batch 1337 | loss=344.6403 | lr=0.01\nstep   4713/33750 | epoch   1 | batch 1338 | loss=7.5110 | lr=0.01\nstep   4714/33750 | epoch   1 | batch 1339 | loss=167.3410 | lr=0.01\nstep   4715/33750 | epoch   1 | batch 1340 | loss=909.5467 | lr=0.01\nstep   4716/33750 | epoch   1 | batch 1341 | loss=2271.1050 | lr=0.01\nstep   4717/33750 | epoch   1 | batch 1342 | loss=312.2331 | lr=0.01\nstep   4718/33750 | epoch   1 | batch 1343 | loss=2.4626 | lr=0.01\nstep   4719/33750 | epoch   1 | batch 1344 | loss=260.1360 | lr=0.01\nstep   4720/33750 | epoch   1 | batch 1345 | loss=4.3098 | lr=0.01\nstep   4721/33750 | epoch   1 | batch 1346 | loss=895.9837 | lr=0.01\nstep   4722/33750 | epoch   1 | batch 1347 | loss=15.0086 | lr=0.01\nstep   4723/33750 | epoch   1 | batch 1348 | loss=2182.2468 | lr=0.01\nstep   4724/33750 | epoch   1 | batch 1349 | loss=51.1189 | lr=0.01\nstep   4725/33750 | epoch   1 | batch 1350 | loss=20.6331 | lr=0.01\nstep   4726/33750 | epoch   1 | batch 1351 | loss=746.4398 | lr=0.01\nstep   4727/33750 | epoch   1 | batch 1352 | loss=311.1446 | lr=0.01\nstep   4728/33750 | epoch   1 | batch 1353 | loss=4006.2747 | lr=0.01\nstep   4729/33750 | epoch   1 | batch 1354 | loss=0.0241 | lr=0.01\nstep   4730/33750 | epoch   1 | batch 1355 | loss=0.7316 | lr=0.01\nstep   4731/33750 | epoch   1 | batch 1356 | loss=1.9630 | lr=0.01\nstep   4732/33750 | epoch   1 | batch 1357 | loss=13.5661 | lr=0.01\nstep   4733/33750 | epoch   1 | batch 1358 | loss=581.4376 | lr=0.01\nstep   4734/33750 | epoch   1 | batch 1359 | loss=13.4596 | lr=0.01\nstep   4735/33750 | epoch   1 | batch 1360 | loss=589.4432 | lr=0.01\nstep   4736/33750 | epoch   1 | batch 1361 | loss=1310.9269 | lr=0.01\nstep   4737/33750 | epoch   1 | batch 1362 | loss=1437.5312 | lr=0.01\nstep   4738/33750 | epoch   1 | batch 1363 | loss=18.0579 | lr=0.01\nstep   4739/33750 | epoch   1 | batch 1364 | loss=1810.9990 | lr=0.01\nstep   4740/33750 | epoch   1 | batch 1365 | loss=3185.4763 | lr=0.01\nstep   4741/33750 | epoch   1 | batch 1366 | loss=1842.9059 | lr=0.01\nstep   4742/33750 | epoch   1 | batch 1367 | loss=2.3403 | lr=0.01\nstep   4743/33750 | epoch   1 | batch 1368 | loss=3.9064 | lr=0.01\nstep   4744/33750 | epoch   1 | batch 1369 | loss=850.4518 | lr=0.01\nstep   4745/33750 | epoch   1 | batch 1370 | loss=550.3965 | lr=0.01\nstep   4746/33750 | epoch   1 | batch 1371 | loss=99.2913 | lr=0.01\nstep   4747/33750 | epoch   1 | batch 1372 | loss=0.1176 | lr=0.01\nstep   4748/33750 | epoch   1 | batch 1373 | loss=209.9353 | lr=0.01\nstep   4749/33750 | epoch   1 | batch 1374 | loss=0.0147 | lr=0.01\nstep   4750/33750 | epoch   1 | batch 1375 | loss=0.0027 | lr=0.01\nstep   4751/33750 | epoch   1 | batch 1376 | loss=96.2624 | lr=0.01\nstep   4752/33750 | epoch   1 | batch 1377 | loss=24.4698 | lr=0.01\nstep   4753/33750 | epoch   1 | batch 1378 | loss=2.1988 | lr=0.01\nstep   4754/33750 | epoch   1 | batch 1379 | loss=26.6450 | lr=0.01\nstep   4755/33750 | epoch   1 | batch 1380 | loss=372.6693 | lr=0.01\nstep   4756/33750 | epoch   1 | batch 1381 | loss=0.0707 | lr=0.01\nstep   4757/33750 | epoch   1 | batch 1382 | loss=10.8872 | lr=0.01\nstep   4758/33750 | epoch   1 | batch 1383 | loss=0.3219 | lr=0.01\nstep   4759/33750 | epoch   1 | batch 1384 | loss=497.2239 | lr=0.01\nstep   4760/33750 | epoch   1 | batch 1385 | loss=1448.5082 | lr=0.01\nstep   4761/33750 | epoch   1 | batch 1386 | loss=0.2368 | lr=0.01\nstep   4762/33750 | epoch   1 | batch 1387 | loss=240.1041 | lr=0.01\nstep   4763/33750 | epoch   1 | batch 1388 | loss=1470.4146 | lr=0.01\nstep   4764/33750 | epoch   1 | batch 1389 | loss=964.0503 | lr=0.01\nstep   4765/33750 | epoch   1 | batch 1390 | loss=73.6407 | lr=0.01\nstep   4766/33750 | epoch   1 | batch 1391 | loss=4.8235 | lr=0.01\nstep   4767/33750 | epoch   1 | batch 1392 | loss=0.9461 | lr=0.01\nstep   4768/33750 | epoch   1 | batch 1393 | loss=206.6142 | lr=0.01\nstep   4769/33750 | epoch   1 | batch 1394 | loss=176.8861 | lr=0.01\nstep   4770/33750 | epoch   1 | batch 1395 | loss=80.3534 | lr=0.01\nstep   4771/33750 | epoch   1 | batch 1396 | loss=5307.4473 | lr=0.01\nstep   4772/33750 | epoch   1 | batch 1397 | loss=9.3005 | lr=0.01\nstep   4773/33750 | epoch   1 | batch 1398 | loss=0.2958 | lr=0.01\nstep   4774/33750 | epoch   1 | batch 1399 | loss=6.3889 | lr=0.01\nstep   4775/33750 | epoch   1 | batch 1400 | loss=158.2968 | lr=0.01\nstep   4776/33750 | epoch   1 | batch 1401 | loss=192.5791 | lr=0.01\nstep   4777/33750 | epoch   1 | batch 1402 | loss=821.5978 | lr=0.01\nstep   4778/33750 | epoch   1 | batch 1403 | loss=681.7814 | lr=0.01\nstep   4779/33750 | epoch   1 | batch 1404 | loss=354.4760 | lr=0.01\nstep   4780/33750 | epoch   1 | batch 1405 | loss=163.3732 | lr=0.01\nstep   4781/33750 | epoch   1 | batch 1406 | loss=0.0035 | lr=0.01\nstep   4782/33750 | epoch   1 | batch 1407 | loss=0.6244 | lr=0.01\nstep   4783/33750 | epoch   1 | batch 1408 | loss=0.8357 | lr=0.01\nstep   4784/33750 | epoch   1 | batch 1409 | loss=94.8762 | lr=0.01\nstep   4785/33750 | epoch   1 | batch 1410 | loss=0.0058 | lr=0.01\nstep   4786/33750 | epoch   1 | batch 1411 | loss=147.8196 | lr=0.01\nstep   4787/33750 | epoch   1 | batch 1412 | loss=0.0019 | lr=0.01\nstep   4788/33750 | epoch   1 | batch 1413 | loss=4940.3696 | lr=0.01\nstep   4789/33750 | epoch   1 | batch 1414 | loss=39.0392 | lr=0.01\nstep   4790/33750 | epoch   1 | batch 1415 | loss=0.4818 | lr=0.01\nstep   4791/33750 | epoch   1 | batch 1416 | loss=0.8723 | lr=0.01\nstep   4792/33750 | epoch   1 | batch 1417 | loss=306.7933 | lr=0.01\nstep   4793/33750 | epoch   1 | batch 1418 | loss=1776.8489 | lr=0.01\nstep   4794/33750 | epoch   1 | batch 1419 | loss=10.1432 | lr=0.01\nstep   4795/33750 | epoch   1 | batch 1420 | loss=0.2242 | lr=0.01\nstep   4796/33750 | epoch   1 | batch 1421 | loss=0.0011 | lr=0.01\nstep   4797/33750 | epoch   1 | batch 1422 | loss=874.3165 | lr=0.01\nstep   4798/33750 | epoch   1 | batch 1423 | loss=2.1777 | lr=0.01\nstep   4799/33750 | epoch   1 | batch 1424 | loss=0.0679 | lr=0.01\nstep   4800/33750 | epoch   1 | batch 1425 | loss=78.9113 | lr=0.01\nstep   4801/33750 | epoch   1 | batch 1426 | loss=1896.8971 | lr=0.01\nstep   4802/33750 | epoch   1 | batch 1427 | loss=63.1159 | lr=0.01\nstep   4803/33750 | epoch   1 | batch 1428 | loss=1246.2347 | lr=0.01\nstep   4804/33750 | epoch   1 | batch 1429 | loss=1011.7230 | lr=0.01\nstep   4805/33750 | epoch   1 | batch 1430 | loss=22.9115 | lr=0.01\nstep   4806/33750 | epoch   1 | batch 1431 | loss=1.2749 | lr=0.01\nstep   4807/33750 | epoch   1 | batch 1432 | loss=322.4721 | lr=0.01\nstep   4808/33750 | epoch   1 | batch 1433 | loss=584.8154 | lr=0.01\nstep   4809/33750 | epoch   1 | batch 1434 | loss=0.2971 | lr=0.01\nstep   4810/33750 | epoch   1 | batch 1435 | loss=21.9907 | lr=0.01\nstep   4811/33750 | epoch   1 | batch 1436 | loss=803.3015 | lr=0.01\nstep   4812/33750 | epoch   1 | batch 1437 | loss=111.5276 | lr=0.01\nstep   4813/33750 | epoch   1 | batch 1438 | loss=1.7061 | lr=0.01\nstep   4814/33750 | epoch   1 | batch 1439 | loss=4.2764 | lr=0.01\nstep   4815/33750 | epoch   1 | batch 1440 | loss=220.5463 | lr=0.01\nstep   4816/33750 | epoch   1 | batch 1441 | loss=11.4600 | lr=0.01\nstep   4817/33750 | epoch   1 | batch 1442 | loss=112.1031 | lr=0.01\nstep   4818/33750 | epoch   1 | batch 1443 | loss=705.0991 | lr=0.01\nstep   4819/33750 | epoch   1 | batch 1444 | loss=112.5920 | lr=0.01\nstep   4820/33750 | epoch   1 | batch 1445 | loss=951.7026 | lr=0.01\nstep   4821/33750 | epoch   1 | batch 1446 | loss=0.0736 | lr=0.01\nstep   4822/33750 | epoch   1 | batch 1447 | loss=0.0382 | lr=0.01\nstep   4823/33750 | epoch   1 | batch 1448 | loss=0.0788 | lr=0.01\nstep   4824/33750 | epoch   1 | batch 1449 | loss=123.3577 | lr=0.01\nstep   4825/33750 | epoch   1 | batch 1450 | loss=50.7119 | lr=0.01\nstep   4826/33750 | epoch   1 | batch 1451 | loss=253.1836 | lr=0.01\nstep   4827/33750 | epoch   1 | batch 1452 | loss=91.8302 | lr=0.01\nstep   4828/33750 | epoch   1 | batch 1453 | loss=284.1205 | lr=0.01\nstep   4829/33750 | epoch   1 | batch 1454 | loss=599.9788 | lr=0.01\nstep   4830/33750 | epoch   1 | batch 1455 | loss=23.7182 | lr=0.01\nstep   4831/33750 | epoch   1 | batch 1456 | loss=0.7871 | lr=0.01\nstep   4832/33750 | epoch   1 | batch 1457 | loss=281.1216 | lr=0.01\nstep   4833/33750 | epoch   1 | batch 1458 | loss=50.4589 | lr=0.01\nstep   4834/33750 | epoch   1 | batch 1459 | loss=0.9010 | lr=0.01\nstep   4835/33750 | epoch   1 | batch 1460 | loss=0.5612 | lr=0.01\nstep   4836/33750 | epoch   1 | batch 1461 | loss=281.7656 | lr=0.01\nstep   4837/33750 | epoch   1 | batch 1462 | loss=686.9242 | lr=0.01\nstep   4838/33750 | epoch   1 | batch 1463 | loss=290.7656 | lr=0.01\nstep   4839/33750 | epoch   1 | batch 1464 | loss=0.0067 | lr=0.01\nstep   4840/33750 | epoch   1 | batch 1465 | loss=448.0131 | lr=0.01\nstep   4841/33750 | epoch   1 | batch 1466 | loss=0.0168 | lr=0.01\nstep   4842/33750 | epoch   1 | batch 1467 | loss=64.9151 | lr=0.01\nstep   4843/33750 | epoch   1 | batch 1468 | loss=0.4978 | lr=0.01\nstep   4844/33750 | epoch   1 | batch 1469 | loss=0.0221 | lr=0.01\nstep   4845/33750 | epoch   1 | batch 1470 | loss=1.8072 | lr=0.01\nstep   4846/33750 | epoch   1 | batch 1471 | loss=0.1647 | lr=0.01\nstep   4847/33750 | epoch   1 | batch 1472 | loss=0.6515 | lr=0.01\nstep   4848/33750 | epoch   1 | batch 1473 | loss=653.1868 | lr=0.01\nstep   4849/33750 | epoch   1 | batch 1474 | loss=444.8955 | lr=0.01\nstep   4850/33750 | epoch   1 | batch 1475 | loss=0.0297 | lr=0.01\nstep   4851/33750 | epoch   1 | batch 1476 | loss=0.0739 | lr=0.01\nstep   4852/33750 | epoch   1 | batch 1477 | loss=53.0299 | lr=0.01\nstep   4853/33750 | epoch   1 | batch 1478 | loss=1125.2478 | lr=0.01\nstep   4854/33750 | epoch   1 | batch 1479 | loss=0.0051 | lr=0.01\nstep   4855/33750 | epoch   1 | batch 1480 | loss=278.1321 | lr=0.01\nstep   4856/33750 | epoch   1 | batch 1481 | loss=7.4045 | lr=0.01\nstep   4857/33750 | epoch   1 | batch 1482 | loss=124.6718 | lr=0.01\nstep   4858/33750 | epoch   1 | batch 1483 | loss=0.5955 | lr=0.01\nstep   4859/33750 | epoch   1 | batch 1484 | loss=21.0286 | lr=0.01\nstep   4860/33750 | epoch   1 | batch 1485 | loss=0.0109 | lr=0.01\nstep   4861/33750 | epoch   1 | batch 1486 | loss=7.0278 | lr=0.01\nstep   4862/33750 | epoch   1 | batch 1487 | loss=2170.1912 | lr=0.01\nstep   4863/33750 | epoch   1 | batch 1488 | loss=0.5793 | lr=0.01\nstep   4864/33750 | epoch   1 | batch 1489 | loss=519.1797 | lr=0.01\nstep   4865/33750 | epoch   1 | batch 1490 | loss=1599.5452 | lr=0.01\nstep   4866/33750 | epoch   1 | batch 1491 | loss=545.1320 | lr=0.01\nstep   4867/33750 | epoch   1 | batch 1492 | loss=39.7941 | lr=0.01\nstep   4868/33750 | epoch   1 | batch 1493 | loss=3.5861 | lr=0.01\nstep   4869/33750 | epoch   1 | batch 1494 | loss=12.2609 | lr=0.01\nstep   4870/33750 | epoch   1 | batch 1495 | loss=1371.5170 | lr=0.01\nstep   4871/33750 | epoch   1 | batch 1496 | loss=416.6605 | lr=0.01\nstep   4872/33750 | epoch   1 | batch 1497 | loss=0.0036 | lr=0.01\nstep   4873/33750 | epoch   1 | batch 1498 | loss=309.3819 | lr=0.01\nstep   4874/33750 | epoch   1 | batch 1499 | loss=549.3483 | lr=0.01\nstep   4875/33750 | epoch   1 | batch 1500 | loss=3.7128 | lr=0.01\nstep   4876/33750 | epoch   1 | batch 1501 | loss=2.0785 | lr=0.01\nstep   4877/33750 | epoch   1 | batch 1502 | loss=219.7607 | lr=0.01\nstep   4878/33750 | epoch   1 | batch 1503 | loss=0.3111 | lr=0.01\nstep   4879/33750 | epoch   1 | batch 1504 | loss=625.3317 | lr=0.01\nstep   4880/33750 | epoch   1 | batch 1505 | loss=0.0062 | lr=0.01\nstep   4881/33750 | epoch   1 | batch 1506 | loss=2.1431 | lr=0.01\nstep   4882/33750 | epoch   1 | batch 1507 | loss=2584.6687 | lr=0.01\nstep   4883/33750 | epoch   1 | batch 1508 | loss=6.7940 | lr=0.01\nstep   4884/33750 | epoch   1 | batch 1509 | loss=13.6267 | lr=0.01\nstep   4885/33750 | epoch   1 | batch 1510 | loss=0.4500 | lr=0.01\nstep   4886/33750 | epoch   1 | batch 1511 | loss=252.7391 | lr=0.01\nstep   4887/33750 | epoch   1 | batch 1512 | loss=1293.9969 | lr=0.01\nstep   4888/33750 | epoch   1 | batch 1513 | loss=665.3132 | lr=0.01\nstep   4889/33750 | epoch   1 | batch 1514 | loss=1.4994 | lr=0.01\nstep   4890/33750 | epoch   1 | batch 1515 | loss=245.8803 | lr=0.01\nstep   4891/33750 | epoch   1 | batch 1516 | loss=84.9182 | lr=0.01\nstep   4892/33750 | epoch   1 | batch 1517 | loss=41.7032 | lr=0.01\nstep   4893/33750 | epoch   1 | batch 1518 | loss=0.1485 | lr=0.01\nstep   4894/33750 | epoch   1 | batch 1519 | loss=24.3222 | lr=0.01\nstep   4895/33750 | epoch   1 | batch 1520 | loss=1081.0059 | lr=0.01\nstep   4896/33750 | epoch   1 | batch 1521 | loss=0.2316 | lr=0.01\nstep   4897/33750 | epoch   1 | batch 1522 | loss=12.8896 | lr=0.01\nstep   4898/33750 | epoch   1 | batch 1523 | loss=0.1347 | lr=0.01\nstep   4899/33750 | epoch   1 | batch 1524 | loss=3.2698 | lr=0.01\nstep   4900/33750 | epoch   1 | batch 1525 | loss=0.8566 | lr=0.01\nstep   4901/33750 | epoch   1 | batch 1526 | loss=1051.8090 | lr=0.01\nstep   4902/33750 | epoch   1 | batch 1527 | loss=2031.1829 | lr=0.01\nstep   4903/33750 | epoch   1 | batch 1528 | loss=0.0060 | lr=0.01\nstep   4904/33750 | epoch   1 | batch 1529 | loss=1128.0167 | lr=0.01\nstep   4905/33750 | epoch   1 | batch 1530 | loss=84.9727 | lr=0.01\nstep   4906/33750 | epoch   1 | batch 1531 | loss=0.0937 | lr=0.01\nstep   4907/33750 | epoch   1 | batch 1532 | loss=225.9274 | lr=0.01\nstep   4908/33750 | epoch   1 | batch 1533 | loss=3.0770 | lr=0.01\nstep   4909/33750 | epoch   1 | batch 1534 | loss=7.5557 | lr=0.01\nstep   4910/33750 | epoch   1 | batch 1535 | loss=227.7258 | lr=0.01\nstep   4911/33750 | epoch   1 | batch 1536 | loss=2.6225 | lr=0.01\nstep   4912/33750 | epoch   1 | batch 1537 | loss=4.7784 | lr=0.01\nstep   4913/33750 | epoch   1 | batch 1538 | loss=1307.4878 | lr=0.01\nstep   4914/33750 | epoch   1 | batch 1539 | loss=5.3278 | lr=0.01\nstep   4915/33750 | epoch   1 | batch 1540 | loss=6.6614 | lr=0.01\nstep   4916/33750 | epoch   1 | batch 1541 | loss=201.8691 | lr=0.01\nstep   4917/33750 | epoch   1 | batch 1542 | loss=16.4621 | lr=0.01\nstep   4918/33750 | epoch   1 | batch 1543 | loss=1.2802 | lr=0.01\nstep   4919/33750 | epoch   1 | batch 1544 | loss=0.7613 | lr=0.01\nstep   4920/33750 | epoch   1 | batch 1545 | loss=60.2854 | lr=0.01\nstep   4921/33750 | epoch   1 | batch 1546 | loss=1.8781 | lr=0.01\nstep   4922/33750 | epoch   1 | batch 1547 | loss=251.7439 | lr=0.01\nstep   4923/33750 | epoch   1 | batch 1548 | loss=605.9255 | lr=0.01\nstep   4924/33750 | epoch   1 | batch 1549 | loss=100.5429 | lr=0.01\nstep   4925/33750 | epoch   1 | batch 1550 | loss=441.4082 | lr=0.01\nstep   4926/33750 | epoch   1 | batch 1551 | loss=5084.2168 | lr=0.01\nstep   4927/33750 | epoch   1 | batch 1552 | loss=425.8166 | lr=0.01\nstep   4928/33750 | epoch   1 | batch 1553 | loss=844.5015 | lr=0.01\nstep   4929/33750 | epoch   1 | batch 1554 | loss=724.8705 | lr=0.01\nstep   4930/33750 | epoch   1 | batch 1555 | loss=489.1828 | lr=0.01\nstep   4931/33750 | epoch   1 | batch 1556 | loss=383.2862 | lr=0.01\nstep   4932/33750 | epoch   1 | batch 1557 | loss=89.9574 | lr=0.01\nstep   4933/33750 | epoch   1 | batch 1558 | loss=8.6977 | lr=0.01\nstep   4934/33750 | epoch   1 | batch 1559 | loss=771.7997 | lr=0.01\nstep   4935/33750 | epoch   1 | batch 1560 | loss=483.6841 | lr=0.01\nstep   4936/33750 | epoch   1 | batch 1561 | loss=556.9808 | lr=0.01\nstep   4937/33750 | epoch   1 | batch 1562 | loss=0.0824 | lr=0.01\nstep   4938/33750 | epoch   1 | batch 1563 | loss=0.0287 | lr=0.01\nstep   4939/33750 | epoch   1 | batch 1564 | loss=2.8349 | lr=0.01\nstep   4940/33750 | epoch   1 | batch 1565 | loss=392.6214 | lr=0.01\nstep   4941/33750 | epoch   1 | batch 1566 | loss=1265.6161 | lr=0.01\nstep   4942/33750 | epoch   1 | batch 1567 | loss=1291.5641 | lr=0.01\nstep   4943/33750 | epoch   1 | batch 1568 | loss=17.0568 | lr=0.01\nstep   4944/33750 | epoch   1 | batch 1569 | loss=58.4182 | lr=0.01\nstep   4945/33750 | epoch   1 | batch 1570 | loss=12.8021 | lr=0.01\nstep   4946/33750 | epoch   1 | batch 1571 | loss=1.0706 | lr=0.01\nstep   4947/33750 | epoch   1 | batch 1572 | loss=68.3471 | lr=0.01\nstep   4948/33750 | epoch   1 | batch 1573 | loss=889.6721 | lr=0.01\nstep   4949/33750 | epoch   1 | batch 1574 | loss=221.2919 | lr=0.01\nstep   4950/33750 | epoch   1 | batch 1575 | loss=0.5720 | lr=0.01\nstep   4951/33750 | epoch   1 | batch 1576 | loss=14.5846 | lr=0.01\nstep   4952/33750 | epoch   1 | batch 1577 | loss=0.5196 | lr=0.01\nstep   4953/33750 | epoch   1 | batch 1578 | loss=183.2922 | lr=0.01\nstep   4954/33750 | epoch   1 | batch 1579 | loss=3.2060 | lr=0.01\nstep   4955/33750 | epoch   1 | batch 1580 | loss=575.5480 | lr=0.01\nstep   4956/33750 | epoch   1 | batch 1581 | loss=208.9162 | lr=0.01\nstep   4957/33750 | epoch   1 | batch 1582 | loss=0.0025 | lr=0.01\nstep   4958/33750 | epoch   1 | batch 1583 | loss=0.0012 | lr=0.01\nstep   4959/33750 | epoch   1 | batch 1584 | loss=1481.0758 | lr=0.01\nstep   4960/33750 | epoch   1 | batch 1585 | loss=353.1152 | lr=0.01\nstep   4961/33750 | epoch   1 | batch 1586 | loss=3076.4500 | lr=0.01\nstep   4962/33750 | epoch   1 | batch 1587 | loss=9.5277 | lr=0.01\nstep   4963/33750 | epoch   1 | batch 1588 | loss=0.3661 | lr=0.01\nstep   4964/33750 | epoch   1 | batch 1589 | loss=32.2732 | lr=0.01\nstep   4965/33750 | epoch   1 | batch 1590 | loss=14.7415 | lr=0.01\nstep   4966/33750 | epoch   1 | batch 1591 | loss=177.7708 | lr=0.01\nstep   4967/33750 | epoch   1 | batch 1592 | loss=1.0984 | lr=0.01\nstep   4968/33750 | epoch   1 | batch 1593 | loss=846.6666 | lr=0.01\nstep   4969/33750 | epoch   1 | batch 1594 | loss=4.2333 | lr=0.01\nstep   4970/33750 | epoch   1 | batch 1595 | loss=3.8134 | lr=0.01\nstep   4971/33750 | epoch   1 | batch 1596 | loss=1.0181 | lr=0.01\nstep   4972/33750 | epoch   1 | batch 1597 | loss=620.7031 | lr=0.01\nstep   4973/33750 | epoch   1 | batch 1598 | loss=0.0870 | lr=0.01\nstep   4974/33750 | epoch   1 | batch 1599 | loss=834.7795 | lr=0.01\nstep   4975/33750 | epoch   1 | batch 1600 | loss=70.8745 | lr=0.01\nstep   4976/33750 | epoch   1 | batch 1601 | loss=0.7886 | lr=0.01\nstep   4977/33750 | epoch   1 | batch 1602 | loss=86.5460 | lr=0.01\nstep   4978/33750 | epoch   1 | batch 1603 | loss=1316.3989 | lr=0.01\nstep   4979/33750 | epoch   1 | batch 1604 | loss=0.0368 | lr=0.01\nstep   4980/33750 | epoch   1 | batch 1605 | loss=2007.6644 | lr=0.01\nstep   4981/33750 | epoch   1 | batch 1606 | loss=4038.2642 | lr=0.01\nstep   4982/33750 | epoch   1 | batch 1607 | loss=85.1985 | lr=0.01\nstep   4983/33750 | epoch   1 | batch 1608 | loss=7.8225 | lr=0.01\nstep   4984/33750 | epoch   1 | batch 1609 | loss=808.9749 | lr=0.01\nstep   4985/33750 | epoch   1 | batch 1610 | loss=42.1082 | lr=0.01\nstep   4986/33750 | epoch   1 | batch 1611 | loss=0.0986 | lr=0.01\nstep   4987/33750 | epoch   1 | batch 1612 | loss=634.0465 | lr=0.01\nstep   4988/33750 | epoch   1 | batch 1613 | loss=222.6360 | lr=0.01\nstep   4989/33750 | epoch   1 | batch 1614 | loss=451.1082 | lr=0.01\nstep   4990/33750 | epoch   1 | batch 1615 | loss=0.0783 | lr=0.01\nstep   4991/33750 | epoch   1 | batch 1616 | loss=0.1910 | lr=0.01\nstep   4992/33750 | epoch   1 | batch 1617 | loss=1268.1674 | lr=0.01\nstep   4993/33750 | epoch   1 | batch 1618 | loss=42.4279 | lr=0.01\nstep   4994/33750 | epoch   1 | batch 1619 | loss=7.9308 | lr=0.01\nstep   4995/33750 | epoch   1 | batch 1620 | loss=213.5295 | lr=0.01\nstep   4996/33750 | epoch   1 | batch 1621 | loss=29.5049 | lr=0.01\nstep   4997/33750 | epoch   1 | batch 1622 | loss=1847.9762 | lr=0.01\nstep   4998/33750 | epoch   1 | batch 1623 | loss=65.4705 | lr=0.01\nstep   4999/33750 | epoch   1 | batch 1624 | loss=648.4968 | lr=0.01\nstep   5000/33750 | epoch   1 | batch 1625 | loss=9.1902 | lr=0.01\nstep   5001/33750 | epoch   1 | batch 1626 | loss=0.1750 | lr=0.01\nstep   5002/33750 | epoch   1 | batch 1627 | loss=3325.2937 | lr=0.01\nstep   5003/33750 | epoch   1 | batch 1628 | loss=2292.1482 | lr=0.01\nstep   5004/33750 | epoch   1 | batch 1629 | loss=0.0473 | lr=0.01\nstep   5005/33750 | epoch   1 | batch 1630 | loss=1.8577 | lr=0.01\nstep   5006/33750 | epoch   1 | batch 1631 | loss=0.3716 | lr=0.01\nstep   5007/33750 | epoch   1 | batch 1632 | loss=8.7788 | lr=0.01\nstep   5008/33750 | epoch   1 | batch 1633 | loss=686.4521 | lr=0.01\nstep   5009/33750 | epoch   1 | batch 1634 | loss=584.4016 | lr=0.01\nstep   5010/33750 | epoch   1 | batch 1635 | loss=114.3832 | lr=0.01\nstep   5011/33750 | epoch   1 | batch 1636 | loss=1.3417 | lr=0.01\nstep   5012/33750 | epoch   1 | batch 1637 | loss=1.3037 | lr=0.01\nstep   5013/33750 | epoch   1 | batch 1638 | loss=0.1506 | lr=0.01\nstep   5014/33750 | epoch   1 | batch 1639 | loss=27.1251 | lr=0.01\nstep   5015/33750 | epoch   1 | batch 1640 | loss=139.3896 | lr=0.01\nstep   5016/33750 | epoch   1 | batch 1641 | loss=455.1095 | lr=0.01\nstep   5017/33750 | epoch   1 | batch 1642 | loss=509.1463 | lr=0.01\nstep   5018/33750 | epoch   1 | batch 1643 | loss=1233.9763 | lr=0.01\nstep   5019/33750 | epoch   1 | batch 1644 | loss=470.1402 | lr=0.01\nstep   5020/33750 | epoch   1 | batch 1645 | loss=0.0642 | lr=0.01\nstep   5021/33750 | epoch   1 | batch 1646 | loss=16.8040 | lr=0.01\nstep   5022/33750 | epoch   1 | batch 1647 | loss=0.0069 | lr=0.01\nstep   5023/33750 | epoch   1 | batch 1648 | loss=4.2781 | lr=0.01\nstep   5024/33750 | epoch   1 | batch 1649 | loss=1217.9044 | lr=0.01\nstep   5025/33750 | epoch   1 | batch 1650 | loss=8.1671 | lr=0.01\nstep   5026/33750 | epoch   1 | batch 1651 | loss=223.5747 | lr=0.01\nstep   5027/33750 | epoch   1 | batch 1652 | loss=1632.3751 | lr=0.01\nstep   5028/33750 | epoch   1 | batch 1653 | loss=0.2163 | lr=0.01\nstep   5029/33750 | epoch   1 | batch 1654 | loss=385.4995 | lr=0.01\nstep   5030/33750 | epoch   1 | batch 1655 | loss=952.0909 | lr=0.01\nstep   5031/33750 | epoch   1 | batch 1656 | loss=0.8295 | lr=0.01\nstep   5032/33750 | epoch   1 | batch 1657 | loss=0.0006 | lr=0.01\nstep   5033/33750 | epoch   1 | batch 1658 | loss=46.6778 | lr=0.01\nstep   5034/33750 | epoch   1 | batch 1659 | loss=14.6831 | lr=0.01\nstep   5035/33750 | epoch   1 | batch 1660 | loss=246.0653 | lr=0.01\nstep   5036/33750 | epoch   1 | batch 1661 | loss=0.1453 | lr=0.01\nstep   5037/33750 | epoch   1 | batch 1662 | loss=211.2618 | lr=0.01\nstep   5038/33750 | epoch   1 | batch 1663 | loss=668.3439 | lr=0.01\nstep   5039/33750 | epoch   1 | batch 1664 | loss=67.0543 | lr=0.01\nstep   5040/33750 | epoch   1 | batch 1665 | loss=485.3612 | lr=0.01\nstep   5041/33750 | epoch   1 | batch 1666 | loss=0.4870 | lr=0.01\nstep   5042/33750 | epoch   1 | batch 1667 | loss=6.1166 | lr=0.01\nstep   5043/33750 | epoch   1 | batch 1668 | loss=79.5860 | lr=0.01\nstep   5044/33750 | epoch   1 | batch 1669 | loss=421.1820 | lr=0.01\nstep   5045/33750 | epoch   1 | batch 1670 | loss=348.5101 | lr=0.01\nstep   5046/33750 | epoch   1 | batch 1671 | loss=5370.4453 | lr=0.01\nstep   5047/33750 | epoch   1 | batch 1672 | loss=261.9173 | lr=0.01\nstep   5048/33750 | epoch   1 | batch 1673 | loss=308.4740 | lr=0.01\nstep   5049/33750 | epoch   1 | batch 1674 | loss=398.0281 | lr=0.01\nstep   5050/33750 | epoch   1 | batch 1675 | loss=0.1473 | lr=0.01\nstep   5051/33750 | epoch   1 | batch 1676 | loss=33.1280 | lr=0.01\nstep   5052/33750 | epoch   1 | batch 1677 | loss=33.4144 | lr=0.01\nstep   5053/33750 | epoch   1 | batch 1678 | loss=16.7758 | lr=0.01\nstep   5054/33750 | epoch   1 | batch 1679 | loss=5.7304 | lr=0.01\nstep   5055/33750 | epoch   1 | batch 1680 | loss=82.2339 | lr=0.01\nstep   5056/33750 | epoch   1 | batch 1681 | loss=540.9525 | lr=0.01\nstep   5057/33750 | epoch   1 | batch 1682 | loss=2.4919 | lr=0.01\nstep   5058/33750 | epoch   1 | batch 1683 | loss=278.9057 | lr=0.01\nstep   5059/33750 | epoch   1 | batch 1684 | loss=1951.8672 | lr=0.01\nstep   5060/33750 | epoch   1 | batch 1685 | loss=460.3807 | lr=0.01\nstep   5061/33750 | epoch   1 | batch 1686 | loss=2477.5193 | lr=0.01\nstep   5062/33750 | epoch   1 | batch 1687 | loss=1684.3094 | lr=0.01\nstep   5063/33750 | epoch   1 | batch 1688 | loss=158.4696 | lr=0.01\nstep   5064/33750 | epoch   1 | batch 1689 | loss=2055.3960 | lr=0.01\nstep   5065/33750 | epoch   1 | batch 1690 | loss=1599.5703 | lr=0.01\nstep   5066/33750 | epoch   1 | batch 1691 | loss=340.7606 | lr=0.01\nstep   5067/33750 | epoch   1 | batch 1692 | loss=3.8513 | lr=0.01\nstep   5068/33750 | epoch   1 | batch 1693 | loss=45.4281 | lr=0.01\nstep   5069/33750 | epoch   1 | batch 1694 | loss=0.0399 | lr=0.01\nstep   5070/33750 | epoch   1 | batch 1695 | loss=1575.2151 | lr=0.01\nstep   5071/33750 | epoch   1 | batch 1696 | loss=33.3005 | lr=0.01\nstep   5072/33750 | epoch   1 | batch 1697 | loss=1143.4778 | lr=0.01\nstep   5073/33750 | epoch   1 | batch 1698 | loss=1026.5348 | lr=0.01\nstep   5074/33750 | epoch   1 | batch 1699 | loss=84.2806 | lr=0.01\nstep   5075/33750 | epoch   1 | batch 1700 | loss=345.9859 | lr=0.01\nstep   5076/33750 | epoch   1 | batch 1701 | loss=65.8183 | lr=0.01\nstep   5077/33750 | epoch   1 | batch 1702 | loss=0.2238 | lr=0.01\nstep   5078/33750 | epoch   1 | batch 1703 | loss=1.3840 | lr=0.01\nstep   5079/33750 | epoch   1 | batch 1704 | loss=319.6316 | lr=0.01\nstep   5080/33750 | epoch   1 | batch 1705 | loss=0.1075 | lr=0.01\nstep   5081/33750 | epoch   1 | batch 1706 | loss=1082.5398 | lr=0.01\nstep   5082/33750 | epoch   1 | batch 1707 | loss=112.5029 | lr=0.01\nstep   5083/33750 | epoch   1 | batch 1708 | loss=193.2252 | lr=0.01\nstep   5084/33750 | epoch   1 | batch 1709 | loss=396.5989 | lr=0.01\nstep   5085/33750 | epoch   1 | batch 1710 | loss=640.3466 | lr=0.01\nstep   5086/33750 | epoch   1 | batch 1711 | loss=179.9445 | lr=0.01\nstep   5087/33750 | epoch   1 | batch 1712 | loss=314.9226 | lr=0.01\nstep   5088/33750 | epoch   1 | batch 1713 | loss=0.2005 | lr=0.01\nstep   5089/33750 | epoch   1 | batch 1714 | loss=240.4178 | lr=0.01\nstep   5090/33750 | epoch   1 | batch 1715 | loss=15.5942 | lr=0.01\nstep   5091/33750 | epoch   1 | batch 1716 | loss=408.8820 | lr=0.01\nstep   5092/33750 | epoch   1 | batch 1717 | loss=1886.5393 | lr=0.01\nstep   5093/33750 | epoch   1 | batch 1718 | loss=0.0032 | lr=0.01\nstep   5094/33750 | epoch   1 | batch 1719 | loss=1.7087 | lr=0.01\nstep   5095/33750 | epoch   1 | batch 1720 | loss=0.0011 | lr=0.01\nstep   5096/33750 | epoch   1 | batch 1721 | loss=0.1717 | lr=0.01\nstep   5097/33750 | epoch   1 | batch 1722 | loss=2.3358 | lr=0.01\nstep   5098/33750 | epoch   1 | batch 1723 | loss=15.5098 | lr=0.01\nstep   5099/33750 | epoch   1 | batch 1724 | loss=2187.3970 | lr=0.01\nstep   5100/33750 | epoch   1 | batch 1725 | loss=0.9832 | lr=0.01\nstep   5101/33750 | epoch   1 | batch 1726 | loss=22.8105 | lr=0.01\nstep   5102/33750 | epoch   1 | batch 1727 | loss=0.0730 | lr=0.01\nstep   5103/33750 | epoch   1 | batch 1728 | loss=4.0042 | lr=0.01\nstep   5104/33750 | epoch   1 | batch 1729 | loss=2989.7878 | lr=0.01\nstep   5105/33750 | epoch   1 | batch 1730 | loss=386.2955 | lr=0.01\nstep   5106/33750 | epoch   1 | batch 1731 | loss=0.2928 | lr=0.01\nstep   5107/33750 | epoch   1 | batch 1732 | loss=0.0372 | lr=0.01\nstep   5108/33750 | epoch   1 | batch 1733 | loss=245.6175 | lr=0.01\nstep   5109/33750 | epoch   1 | batch 1734 | loss=16.0974 | lr=0.01\nstep   5110/33750 | epoch   1 | batch 1735 | loss=0.0940 | lr=0.01\nstep   5111/33750 | epoch   1 | batch 1736 | loss=0.6681 | lr=0.01\nstep   5112/33750 | epoch   1 | batch 1737 | loss=1304.2346 | lr=0.01\nstep   5113/33750 | epoch   1 | batch 1738 | loss=1138.9913 | lr=0.01\nstep   5114/33750 | epoch   1 | batch 1739 | loss=0.4330 | lr=0.01\nstep   5115/33750 | epoch   1 | batch 1740 | loss=582.2405 | lr=0.01\nstep   5116/33750 | epoch   1 | batch 1741 | loss=507.3275 | lr=0.01\nstep   5117/33750 | epoch   1 | batch 1742 | loss=99.1818 | lr=0.01\nstep   5118/33750 | epoch   1 | batch 1743 | loss=145.1351 | lr=0.01\nstep   5119/33750 | epoch   1 | batch 1744 | loss=0.0570 | lr=0.01\nstep   5120/33750 | epoch   1 | batch 1745 | loss=0.0985 | lr=0.01\nstep   5121/33750 | epoch   1 | batch 1746 | loss=14.8082 | lr=0.01\nstep   5122/33750 | epoch   1 | batch 1747 | loss=51.3837 | lr=0.01\nstep   5123/33750 | epoch   1 | batch 1748 | loss=109.9429 | lr=0.01\nstep   5124/33750 | epoch   1 | batch 1749 | loss=406.2061 | lr=0.01\nstep   5125/33750 | epoch   1 | batch 1750 | loss=561.2670 | lr=0.01\nstep   5126/33750 | epoch   1 | batch 1751 | loss=137.7219 | lr=0.01\nstep   5127/33750 | epoch   1 | batch 1752 | loss=314.0970 | lr=0.01\nstep   5128/33750 | epoch   1 | batch 1753 | loss=230.6146 | lr=0.01\nstep   5129/33750 | epoch   1 | batch 1754 | loss=1.7774 | lr=0.01\nstep   5130/33750 | epoch   1 | batch 1755 | loss=57.7663 | lr=0.01\nstep   5131/33750 | epoch   1 | batch 1756 | loss=1.4959 | lr=0.01\nstep   5132/33750 | epoch   1 | batch 1757 | loss=303.3800 | lr=0.01\nstep   5133/33750 | epoch   1 | batch 1758 | loss=116.2566 | lr=0.01\nstep   5134/33750 | epoch   1 | batch 1759 | loss=1264.8496 | lr=0.01\nstep   5135/33750 | epoch   1 | batch 1760 | loss=6061.7910 | lr=0.01\nstep   5136/33750 | epoch   1 | batch 1761 | loss=0.1057 | lr=0.01\nstep   5137/33750 | epoch   1 | batch 1762 | loss=142.5407 | lr=0.01\nstep   5138/33750 | epoch   1 | batch 1763 | loss=1.1798 | lr=0.01\nstep   5139/33750 | epoch   1 | batch 1764 | loss=116.2113 | lr=0.01\nstep   5140/33750 | epoch   1 | batch 1765 | loss=6.5778 | lr=0.01\nstep   5141/33750 | epoch   1 | batch 1766 | loss=0.1275 | lr=0.01\nstep   5142/33750 | epoch   1 | batch 1767 | loss=203.6909 | lr=0.01\nstep   5143/33750 | epoch   1 | batch 1768 | loss=26.7862 | lr=0.01\nstep   5144/33750 | epoch   1 | batch 1769 | loss=1203.2408 | lr=0.01\nstep   5145/33750 | epoch   1 | batch 1770 | loss=379.9974 | lr=0.01\nstep   5146/33750 | epoch   1 | batch 1771 | loss=996.9067 | lr=0.01\nstep   5147/33750 | epoch   1 | batch 1772 | loss=131.4057 | lr=0.01\nstep   5148/33750 | epoch   1 | batch 1773 | loss=4.7880 | lr=0.01\nstep   5149/33750 | epoch   1 | batch 1774 | loss=139.2306 | lr=0.01\nstep   5150/33750 | epoch   1 | batch 1775 | loss=651.4160 | lr=0.01\nstep   5151/33750 | epoch   1 | batch 1776 | loss=478.7895 | lr=0.01\nstep   5152/33750 | epoch   1 | batch 1777 | loss=168.2662 | lr=0.01\nstep   5153/33750 | epoch   1 | batch 1778 | loss=239.5246 | lr=0.01\nstep   5154/33750 | epoch   1 | batch 1779 | loss=174.6553 | lr=0.01\nstep   5155/33750 | epoch   1 | batch 1780 | loss=1840.4293 | lr=0.01\nstep   5156/33750 | epoch   1 | batch 1781 | loss=3.1541 | lr=0.01\nstep   5157/33750 | epoch   1 | batch 1782 | loss=672.1105 | lr=0.01\nstep   5158/33750 | epoch   1 | batch 1783 | loss=906.9331 | lr=0.01\nstep   5159/33750 | epoch   1 | batch 1784 | loss=16.2386 | lr=0.01\nstep   5160/33750 | epoch   1 | batch 1785 | loss=163.5982 | lr=0.01\nstep   5161/33750 | epoch   1 | batch 1786 | loss=0.1725 | lr=0.01\nstep   5162/33750 | epoch   1 | batch 1787 | loss=0.0631 | lr=0.01\nstep   5163/33750 | epoch   1 | batch 1788 | loss=12.0213 | lr=0.01\nstep   5164/33750 | epoch   1 | batch 1789 | loss=1.1383 | lr=0.01\nstep   5165/33750 | epoch   1 | batch 1790 | loss=1.0621 | lr=0.01\nstep   5166/33750 | epoch   1 | batch 1791 | loss=353.6020 | lr=0.01\nstep   5167/33750 | epoch   1 | batch 1792 | loss=731.7087 | lr=0.01\nstep   5168/33750 | epoch   1 | batch 1793 | loss=96.9327 | lr=0.01\nstep   5169/33750 | epoch   1 | batch 1794 | loss=1.4067 | lr=0.01\nstep   5170/33750 | epoch   1 | batch 1795 | loss=391.5634 | lr=0.01\nstep   5171/33750 | epoch   1 | batch 1796 | loss=736.5878 | lr=0.01\nstep   5172/33750 | epoch   1 | batch 1797 | loss=0.6013 | lr=0.01\nstep   5173/33750 | epoch   1 | batch 1798 | loss=260.1616 | lr=0.01\nstep   5174/33750 | epoch   1 | batch 1799 | loss=0.0386 | lr=0.01\nstep   5175/33750 | epoch   1 | batch 1800 | loss=100.4644 | lr=0.01\nstep   5176/33750 | epoch   1 | batch 1801 | loss=0.0564 | lr=0.01\nstep   5177/33750 | epoch   1 | batch 1802 | loss=78.2067 | lr=0.01\nstep   5178/33750 | epoch   1 | batch 1803 | loss=16.7214 | lr=0.01\nstep   5179/33750 | epoch   1 | batch 1804 | loss=4.4699 | lr=0.01\nstep   5180/33750 | epoch   1 | batch 1805 | loss=331.4366 | lr=0.01\nstep   5181/33750 | epoch   1 | batch 1806 | loss=3945.8250 | lr=0.01\nstep   5182/33750 | epoch   1 | batch 1807 | loss=1933.6243 | lr=0.01\nstep   5183/33750 | epoch   1 | batch 1808 | loss=0.3680 | lr=0.01\nstep   5184/33750 | epoch   1 | batch 1809 | loss=33.2128 | lr=0.01\nstep   5185/33750 | epoch   1 | batch 1810 | loss=1.6336 | lr=0.01\nstep   5186/33750 | epoch   1 | batch 1811 | loss=21.7079 | lr=0.01\nstep   5187/33750 | epoch   1 | batch 1812 | loss=136.5305 | lr=0.01\nstep   5188/33750 | epoch   1 | batch 1813 | loss=753.9929 | lr=0.01\nstep   5189/33750 | epoch   1 | batch 1814 | loss=29.7410 | lr=0.01\nstep   5190/33750 | epoch   1 | batch 1815 | loss=247.6601 | lr=0.01\nstep   5191/33750 | epoch   1 | batch 1816 | loss=805.0252 | lr=0.01\nstep   5192/33750 | epoch   1 | batch 1817 | loss=4624.6411 | lr=0.01\nstep   5193/33750 | epoch   1 | batch 1818 | loss=5925.9058 | lr=0.01\nstep   5194/33750 | epoch   1 | batch 1819 | loss=1.5538 | lr=0.01\nstep   5195/33750 | epoch   1 | batch 1820 | loss=164.0501 | lr=0.01\nstep   5196/33750 | epoch   1 | batch 1821 | loss=0.7021 | lr=0.01\nstep   5197/33750 | epoch   1 | batch 1822 | loss=1562.2825 | lr=0.01\nstep   5198/33750 | epoch   1 | batch 1823 | loss=1.9472 | lr=0.01\nstep   5199/33750 | epoch   1 | batch 1824 | loss=26.6602 | lr=0.01\nstep   5200/33750 | epoch   1 | batch 1825 | loss=0.0064 | lr=0.01\nstep   5201/33750 | epoch   1 | batch 1826 | loss=131.0397 | lr=0.01\nstep   5202/33750 | epoch   1 | batch 1827 | loss=135.6496 | lr=0.01\nstep   5203/33750 | epoch   1 | batch 1828 | loss=833.2263 | lr=0.01\nstep   5204/33750 | epoch   1 | batch 1829 | loss=0.4344 | lr=0.01\nstep   5205/33750 | epoch   1 | batch 1830 | loss=0.0027 | lr=0.01\nstep   5206/33750 | epoch   1 | batch 1831 | loss=388.5119 | lr=0.01\nstep   5207/33750 | epoch   1 | batch 1832 | loss=11.3608 | lr=0.01\nstep   5208/33750 | epoch   1 | batch 1833 | loss=1338.6627 | lr=0.01\nstep   5209/33750 | epoch   1 | batch 1834 | loss=0.0082 | lr=0.01\nstep   5210/33750 | epoch   1 | batch 1835 | loss=451.9183 | lr=0.01\nstep   5211/33750 | epoch   1 | batch 1836 | loss=0.0129 | lr=0.01\nstep   5212/33750 | epoch   1 | batch 1837 | loss=2.3551 | lr=0.01\nstep   5213/33750 | epoch   1 | batch 1838 | loss=5.5339 | lr=0.01\nstep   5214/33750 | epoch   1 | batch 1839 | loss=0.0039 | lr=0.01\nstep   5215/33750 | epoch   1 | batch 1840 | loss=2.8870 | lr=0.01\nstep   5216/33750 | epoch   1 | batch 1841 | loss=0.3334 | lr=0.01\nstep   5217/33750 | epoch   1 | batch 1842 | loss=243.7883 | lr=0.01\nstep   5218/33750 | epoch   1 | batch 1843 | loss=294.1321 | lr=0.01\nstep   5219/33750 | epoch   1 | batch 1844 | loss=273.0808 | lr=0.01\nstep   5220/33750 | epoch   1 | batch 1845 | loss=193.6075 | lr=0.01\nstep   5221/33750 | epoch   1 | batch 1846 | loss=672.3332 | lr=0.01\nstep   5222/33750 | epoch   1 | batch 1847 | loss=147.9633 | lr=0.01\nstep   5223/33750 | epoch   1 | batch 1848 | loss=130.2142 | lr=0.01\nstep   5224/33750 | epoch   1 | batch 1849 | loss=978.7101 | lr=0.01\nstep   5225/33750 | epoch   1 | batch 1850 | loss=346.0086 | lr=0.01\nstep   5226/33750 | epoch   1 | batch 1851 | loss=0.0263 | lr=0.01\nstep   5227/33750 | epoch   1 | batch 1852 | loss=4.5984 | lr=0.01\nstep   5228/33750 | epoch   1 | batch 1853 | loss=0.0124 | lr=0.01\nstep   5229/33750 | epoch   1 | batch 1854 | loss=138.4337 | lr=0.01\nstep   5230/33750 | epoch   1 | batch 1855 | loss=772.5616 | lr=0.01\nstep   5231/33750 | epoch   1 | batch 1856 | loss=390.1906 | lr=0.01\nstep   5232/33750 | epoch   1 | batch 1857 | loss=3.7647 | lr=0.01\nstep   5233/33750 | epoch   1 | batch 1858 | loss=3.3019 | lr=0.01\nstep   5234/33750 | epoch   1 | batch 1859 | loss=216.9405 | lr=0.01\nstep   5235/33750 | epoch   1 | batch 1860 | loss=1257.6860 | lr=0.01\nstep   5236/33750 | epoch   1 | batch 1861 | loss=0.0042 | lr=0.01\nstep   5237/33750 | epoch   1 | batch 1862 | loss=293.5101 | lr=0.01\nstep   5238/33750 | epoch   1 | batch 1863 | loss=0.5719 | lr=0.01\nstep   5239/33750 | epoch   1 | batch 1864 | loss=2.4528 | lr=0.01\nstep   5240/33750 | epoch   1 | batch 1865 | loss=2725.5320 | lr=0.01\nstep   5241/33750 | epoch   1 | batch 1866 | loss=3.4913 | lr=0.01\nstep   5242/33750 | epoch   1 | batch 1867 | loss=106.2446 | lr=0.01\nstep   5243/33750 | epoch   1 | batch 1868 | loss=0.0017 | lr=0.01\nstep   5244/33750 | epoch   1 | batch 1869 | loss=183.5898 | lr=0.01\nstep   5245/33750 | epoch   1 | batch 1870 | loss=4495.8257 | lr=0.01\nstep   5246/33750 | epoch   1 | batch 1871 | loss=751.3997 | lr=0.01\nstep   5247/33750 | epoch   1 | batch 1872 | loss=96.5404 | lr=0.01\nstep   5248/33750 | epoch   1 | batch 1873 | loss=0.0010 | lr=0.01\nstep   5249/33750 | epoch   1 | batch 1874 | loss=3435.7285 | lr=0.01\nstep   5250/33750 | epoch   1 | batch 1875 | loss=23.8314 | lr=0.01\nstep   5251/33750 | epoch   1 | batch 1876 | loss=38.1869 | lr=0.01\nstep   5252/33750 | epoch   1 | batch 1877 | loss=55.8439 | lr=0.01\nstep   5253/33750 | epoch   1 | batch 1878 | loss=251.1536 | lr=0.01\nstep   5254/33750 | epoch   1 | batch 1879 | loss=378.9717 | lr=0.01\nstep   5255/33750 | epoch   1 | batch 1880 | loss=0.8827 | lr=0.01\nstep   5256/33750 | epoch   1 | batch 1881 | loss=333.1137 | lr=0.01\nstep   5257/33750 | epoch   1 | batch 1882 | loss=107.4983 | lr=0.01\nstep   5258/33750 | epoch   1 | batch 1883 | loss=0.1533 | lr=0.01\nstep   5259/33750 | epoch   1 | batch 1884 | loss=76.4914 | lr=0.01\nstep   5260/33750 | epoch   1 | batch 1885 | loss=351.0253 | lr=0.01\nstep   5261/33750 | epoch   1 | batch 1886 | loss=0.8982 | lr=0.01\nstep   5262/33750 | epoch   1 | batch 1887 | loss=510.8013 | lr=0.01\nstep   5263/33750 | epoch   1 | batch 1888 | loss=357.5905 | lr=0.01\nstep   5264/33750 | epoch   1 | batch 1889 | loss=121.9812 | lr=0.01\nstep   5265/33750 | epoch   1 | batch 1890 | loss=1897.6223 | lr=0.01\nstep   5266/33750 | epoch   1 | batch 1891 | loss=2.8707 | lr=0.01\nstep   5267/33750 | epoch   1 | batch 1892 | loss=0.0005 | lr=0.01\nstep   5268/33750 | epoch   1 | batch 1893 | loss=65.7354 | lr=0.01\nstep   5269/33750 | epoch   1 | batch 1894 | loss=669.9614 | lr=0.01\nstep   5270/33750 | epoch   1 | batch 1895 | loss=0.2370 | lr=0.01\nstep   5271/33750 | epoch   1 | batch 1896 | loss=18.1314 | lr=0.01\nstep   5272/33750 | epoch   1 | batch 1897 | loss=794.3736 | lr=0.01\nstep   5273/33750 | epoch   1 | batch 1898 | loss=0.0292 | lr=0.01\nstep   5274/33750 | epoch   1 | batch 1899 | loss=192.1707 | lr=0.01\nstep   5275/33750 | epoch   1 | batch 1900 | loss=0.0110 | lr=0.01\nstep   5276/33750 | epoch   1 | batch 1901 | loss=6.4422 | lr=0.01\nstep   5277/33750 | epoch   1 | batch 1902 | loss=0.7886 | lr=0.01\nstep   5278/33750 | epoch   1 | batch 1903 | loss=4.0258 | lr=0.01\nstep   5279/33750 | epoch   1 | batch 1904 | loss=21.8177 | lr=0.01\nstep   5280/33750 | epoch   1 | batch 1905 | loss=0.3268 | lr=0.01\nstep   5281/33750 | epoch   1 | batch 1906 | loss=1386.6036 | lr=0.01\nstep   5282/33750 | epoch   1 | batch 1907 | loss=645.7319 | lr=0.01\nstep   5283/33750 | epoch   1 | batch 1908 | loss=0.0165 | lr=0.01\nstep   5284/33750 | epoch   1 | batch 1909 | loss=7.8247 | lr=0.01\nstep   5285/33750 | epoch   1 | batch 1910 | loss=0.1635 | lr=0.01\nstep   5286/33750 | epoch   1 | batch 1911 | loss=826.9062 | lr=0.01\nstep   5287/33750 | epoch   1 | batch 1912 | loss=0.0225 | lr=0.01\nstep   5288/33750 | epoch   1 | batch 1913 | loss=0.0116 | lr=0.01\nstep   5289/33750 | epoch   1 | batch 1914 | loss=14.7190 | lr=0.01\nstep   5290/33750 | epoch   1 | batch 1915 | loss=192.3752 | lr=0.01\nstep   5291/33750 | epoch   1 | batch 1916 | loss=1267.8568 | lr=0.01\nstep   5292/33750 | epoch   1 | batch 1917 | loss=1.7274 | lr=0.01\nstep   5293/33750 | epoch   1 | batch 1918 | loss=0.0054 | lr=0.01\nstep   5294/33750 | epoch   1 | batch 1919 | loss=1508.9758 | lr=0.01\nstep   5295/33750 | epoch   1 | batch 1920 | loss=2.8678 | lr=0.01\nstep   5296/33750 | epoch   1 | batch 1921 | loss=39.3120 | lr=0.01\nstep   5297/33750 | epoch   1 | batch 1922 | loss=3.6769 | lr=0.01\nstep   5298/33750 | epoch   1 | batch 1923 | loss=0.0009 | lr=0.01\nstep   5299/33750 | epoch   1 | batch 1924 | loss=0.8712 | lr=0.01\nstep   5300/33750 | epoch   1 | batch 1925 | loss=0.7800 | lr=0.01\nstep   5301/33750 | epoch   1 | batch 1926 | loss=1.8212 | lr=0.01\nstep   5302/33750 | epoch   1 | batch 1927 | loss=1.6155 | lr=0.01\nstep   5303/33750 | epoch   1 | batch 1928 | loss=165.7525 | lr=0.01\nstep   5304/33750 | epoch   1 | batch 1929 | loss=45.6668 | lr=0.01\nstep   5305/33750 | epoch   1 | batch 1930 | loss=33.1567 | lr=0.01\nstep   5306/33750 | epoch   1 | batch 1931 | loss=2.6372 | lr=0.01\nstep   5307/33750 | epoch   1 | batch 1932 | loss=819.1505 | lr=0.01\nstep   5308/33750 | epoch   1 | batch 1933 | loss=149.9006 | lr=0.01\nstep   5309/33750 | epoch   1 | batch 1934 | loss=620.1880 | lr=0.01\nstep   5310/33750 | epoch   1 | batch 1935 | loss=209.9675 | lr=0.01\nstep   5311/33750 | epoch   1 | batch 1936 | loss=0.0424 | lr=0.01\nstep   5312/33750 | epoch   1 | batch 1937 | loss=1205.4708 | lr=0.01\nstep   5313/33750 | epoch   1 | batch 1938 | loss=27.9309 | lr=0.01\nstep   5314/33750 | epoch   1 | batch 1939 | loss=39.4534 | lr=0.01\nstep   5315/33750 | epoch   1 | batch 1940 | loss=4076.6721 | lr=0.01\nstep   5316/33750 | epoch   1 | batch 1941 | loss=141.6317 | lr=0.01\nstep   5317/33750 | epoch   1 | batch 1942 | loss=66.9627 | lr=0.01\nstep   5318/33750 | epoch   1 | batch 1943 | loss=0.2933 | lr=0.01\nstep   5319/33750 | epoch   1 | batch 1944 | loss=91.7920 | lr=0.01\nstep   5320/33750 | epoch   1 | batch 1945 | loss=0.0059 | lr=0.01\nstep   5321/33750 | epoch   1 | batch 1946 | loss=0.1204 | lr=0.01\nstep   5322/33750 | epoch   1 | batch 1947 | loss=266.6797 | lr=0.01\nstep   5323/33750 | epoch   1 | batch 1948 | loss=1325.7097 | lr=0.01\nstep   5324/33750 | epoch   1 | batch 1949 | loss=718.1105 | lr=0.01\nstep   5325/33750 | epoch   1 | batch 1950 | loss=917.9675 | lr=0.01\nstep   5326/33750 | epoch   1 | batch 1951 | loss=233.5248 | lr=0.01\nstep   5327/33750 | epoch   1 | batch 1952 | loss=745.1652 | lr=0.01\nstep   5328/33750 | epoch   1 | batch 1953 | loss=1.1412 | lr=0.01\nstep   5329/33750 | epoch   1 | batch 1954 | loss=516.4466 | lr=0.01\nstep   5330/33750 | epoch   1 | batch 1955 | loss=162.1854 | lr=0.01\nstep   5331/33750 | epoch   1 | batch 1956 | loss=0.3267 | lr=0.01\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/329162507.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlast_completed_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m history, best_val_loss = train_until_total_iters(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtotal_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_iters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_completed_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/khrm-for-openpolymer/src/training_loops.py\u001b[0m in \u001b[0;36mtrain_until_total_iters\u001b[0;34m(total_iters, start_epoch, train_state, train_loader, val_loader, config, train_batch_fn, validate_fn, save_checkpoint_fn, checkpoint_every_n, print_every_n)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mbatch_size_effective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_graphs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/2160885409.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(epoch, train_state, batch_data, config)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhrm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhrm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhrm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q_policy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/khrm-for-openpolymer/src/kmeans_hrm_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, carry, data)\u001b[0m\n\u001b[1;32m   1497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1498\u001b[0m         \u001b[0;31m# Main run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1499\u001b[0;31m         \u001b[0mnew_inner_carry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq_halt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_continue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minner_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_inner_carry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_current_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_current_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m         output = KMeansHRMOutput(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/khrm-for-openpolymer/src/kmeans_hrm_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, kmeans_carry, inputs, batch)\u001b[0m\n\u001b[1;32m   1255\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_H_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mK_cycles\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_L_step\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL_cycles\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m                         \u001b[0;31m# Update the node features based on current clustering\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m                         \u001b[0mnew_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_per_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_carry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m                         current_carry = KMeansCarry(\n\u001b[1;32m   1259\u001b[0m                             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/khrm-for-openpolymer/src/kmeans_hrm_model.py\u001b[0m in \u001b[0;36m_forward_per_graph\u001b[0;34m(self, kmeans_carry, inputs, batch)\u001b[0m\n\u001b[1;32m   1125\u001b[0m             \u001b[0minputs_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmasked_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans_carry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1127\u001b[0;31m         all_subgraph_features = [\n\u001b[0m\u001b[1;32m   1128\u001b[0m             self._forward_per_subgraph(\n\u001b[1;32m   1129\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasked_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/khrm-for-openpolymer/src/kmeans_hrm_model.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m         all_subgraph_features = [\n\u001b[0;32m-> 1128\u001b[0;31m             self._forward_per_subgraph(\n\u001b[0m\u001b[1;32m   1129\u001b[0m                 \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasked_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 \u001b[0medge_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmasked_edges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/khrm-for-openpolymer/src/kmeans_hrm_model.py\u001b[0m in \u001b[0;36m_forward_per_subgraph\u001b[0;34m(self, x, edge_index, input_nodes, input_edges, input_edge_attr, carry_batch, batch)\u001b[0m\n\u001b[1;32m   1043\u001b[0m             \u001b[0mneg_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1045\u001b[0;31m                 \u001b[0mneg_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatched_negative_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombined_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neg_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1046\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m                 \u001b[0mneg_edges\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnegative_sampling\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_edge_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_nodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_nodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_neg_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_negative_sampling.py\u001b[0m in \u001b[0;36mbatched_negative_sampling\u001b[0;34m(edge_index, batch, num_neg_samples, method, force_undirected)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0medge_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0medge_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0medge_index\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mptr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 200\u001b[0;31m         neg_edge_index = negative_sampling(edge_index, num_nodes[i],\n\u001b[0m\u001b[1;32m    201\u001b[0m                                            \u001b[0mnum_neg_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m                                            force_undirected)\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_negative_sampling.py\u001b[0m in \u001b[0;36mnegative_sampling\u001b[0;34m(edge_index, num_nodes, num_neg_samples, method, force_undirected)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Number of tries to sample negative indices.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m             \u001b[0mrnd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m             \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mneg_idx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch_geometric/utils/_negative_sampling.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(population, k, device)\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    471\u001b[0m             \u001b[0mselected_add\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 473\u001b[0;31m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    474\u001b[0m                 \u001b[0;32mwhile\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m                     \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/random.py\u001b[0m in \u001b[0;36m_randbelow_with_getrandbits\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0mgetrandbits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrandbits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# don't use (n-1) here because n can be 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0 <= r < 2**k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":17},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nmetrics_path = os.path.join(CHECKPOINT_DIR, \"history.json\")\nwith open(metrics_path, \"w\") as f:\n    json.dump(history, f, indent=2)\nprint(f\"Saved metrics to {metrics_path}\")\n\n# Plot\ndef plot_curves(history):\n    epochs = [e[\"epoch\"] for e in history[\"train\"]]\n    train_losses = [e[\"loss\"] for e in history[\"train\"]]\n    val_losses = [e[\"loss\"] for e in history[\"val\"]]\n\n    plt.figure(figsize=(8,4))\n    plt.plot(epochs, train_losses, label=\"train_loss\")\n    plt.plot(epochs, val_losses, label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_curves(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T20:15:31.722169Z","iopub.status.idle":"2025-09-15T20:15:31.722389Z","shell.execute_reply.started":"2025-09-15T20:15:31.722285Z","shell.execute_reply":"2025-09-15T20:15:31.722295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from dataset_helpers import smiles_to_graph_data\nimport pandas as pd\nfrom torch_geometric.loader import DataLoader\n\ndf = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n\nCOLS = ([\"SMILES\", \"id\"] + PROPERTIES)\n\nsubmission_df = pd.DataFrame(columns=COLS)\nsubmission_df = pd.concat([submission_df, df], ignore_index=True)\n\ninputs = df[\"SMILES\"].tolist()\nid = df[\"id\"].tolist()\n\ngraph_data_list = [smiles_to_graph_data(smiles, None, [info]) for smiles, info in zip(inputs, id)]\nprint(graph_data_list)\n\ntest_loader = DataLoader(graph_data_list, batch_size=1, shuffle=False)\n\nsubmission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T22:41:55.750616Z","iopub.execute_input":"2025-09-15T22:41:55.751196Z","iopub.status.idle":"2025-09-15T22:41:55.794671Z","shell.execute_reply.started":"2025-09-15T22:41:55.751171Z","shell.execute_reply":"2025-09-15T22:41:55.794112Z"}},"outputs":[{"name":"stdout","text":"[Data(x=[39, 6], edge_index=[2, 84], edge_attr=[84, 4], aux_info=[1]), Data(x=[39, 6], edge_index=[2, 86], edge_attr=[86, 4], aux_info=[1]), Data(x=[44, 6], edge_index=[2, 98], edge_attr=[98, 4], aux_info=[1])]\n","output_type":"stream"},{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"                                              SMILES          id   Tg  FFV  \\\n0  *Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C(c4ccc(*)cc4)(C(F)...  1109053969  NaN  NaN   \n1  *Oc1ccc(C(C)(C)c2ccc(Oc3ccc(C(=O)c4cccc(C(=O)c...  1422188626  NaN  NaN   \n2  *c1cccc(OCCCCCCCCOc2cccc(N3C(=O)c4ccc(-c5cccc6...  2032016830  NaN  NaN   \n\n    Tc Density   Rg  \n0  NaN     NaN  NaN  \n1  NaN     NaN  NaN  \n2  NaN     NaN  NaN  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>SMILES</th>\n      <th>id</th>\n      <th>Tg</th>\n      <th>FFV</th>\n      <th>Tc</th>\n      <th>Density</th>\n      <th>Rg</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>*Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C(c4ccc(*)cc4)(C(F)...</td>\n      <td>1109053969</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>*Oc1ccc(C(C)(C)c2ccc(Oc3ccc(C(=O)c4cccc(C(=O)c...</td>\n      <td>1422188626</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>*c1cccc(OCCCCCCCCOc2cccc(N3C(=O)c4ccc(-c5cccc6...</td>\n      <td>2032016830</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"model.eval()\n\nmax_iters = 10\n\n\nwith torch.no_grad():\n    for idx, datum in enumerate(test_loader):\n        preds = [0, 0, 0, 0, 0]\n        carry = model.initial_carry(datum, device)\n        while max_iters > 0:\n            carry, hrm_output = model(carry, datum)\n\n            preds = hrm_output['y_pred']\n            q_policy = hrm_output['q_policy']\n\n            if q_policy[0] > q_policy[1]:\n                break\n            max_iters -= 1\n        max_iters = 10\n        print(preds.cpu().numpy(), i)\n\n        submission_df.loc[idx, [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]] = preds.cpu().numpy().ravel()\n        \nprint('submission_df', submission_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T22:52:11.229807Z","iopub.execute_input":"2025-09-15T22:52:11.230518Z","iopub.status.idle":"2025-09-15T22:52:13.638499Z","shell.execute_reply.started":"2025-09-15T22:52:11.230494Z","shell.execute_reply":"2025-09-15T22:52:13.637860Z"}},"outputs":[{"name":"stdout","text":"[[ 3.0364149e+01  2.7716428e-01 -2.2286460e-02  7.6941609e-01\n   1.2645821e+01]] 2\n[[ 2.7746019e+01  2.7507359e-01 -8.0580926e-03  7.6476061e-01\n   1.2556776e+01]] 2\n[[20.416775    0.2680679   0.03186549  0.7492246  12.277412  ]] 2\nsubmission_df                                               SMILES          id         Tg  \\\n0  *Oc1ccc(C=NN=Cc2ccc(Oc3ccc(C(c4ccc(*)cc4)(C(F)...  1109053969  30.364149   \n1  *Oc1ccc(C(C)(C)c2ccc(Oc3ccc(C(=O)c4cccc(C(=O)c...  1422188626  27.746019   \n2  *c1cccc(OCCCCCCCCOc2cccc(N3C(=O)c4ccc(-c5cccc6...  2032016830  20.416775   \n\n        FFV        Tc   Density         Rg  \n0  0.277164 -0.022286  0.769416  12.645821  \n1  0.275074 -0.008058  0.764761  12.556776  \n2  0.268068  0.031865  0.749225  12.277412  \n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"submission_df.to_csv('/kaggle/working/submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-15T22:52:37.850370Z","iopub.execute_input":"2025-09-15T22:52:37.850670Z","iopub.status.idle":"2025-09-15T22:52:37.871014Z","shell.execute_reply.started":"2025-09-15T22:52:37.850648Z","shell.execute_reply":"2025-09-15T22:52:37.870460Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}