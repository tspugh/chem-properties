{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":74608,"databundleVersionId":12966160,"sourceType":"competition"},{"sourceId":7876159,"sourceType":"datasetVersion","datasetId":1640734},{"sourceId":13058873,"sourceType":"datasetVersion","datasetId":8264295}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!cp -r /kaggle/input/khrm-for-openpolymer /kaggle/working/.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:18:23.062508Z","iopub.execute_input":"2025-09-14T21:18:23.063159Z","iopub.status.idle":"2025-09-14T21:18:23.394717Z","shell.execute_reply.started":"2025-09-14T21:18:23.063127Z","shell.execute_reply":"2025-09-14T21:18:23.393685Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"!cp -r /kaggle/input/khrm-for-openpolymer /kaggle/working/.\n!cd /kaggle/working/khrm-for-openpolymer\n!pip install --no-cache-dir torch-geometric -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n!pip install --no-cache-dir torch-cluster -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n!pip install --no-cache-dir torch-scatter -f https://data.pyg.org/whl/torch-2.6.0+cu124.html\n#!pip install /kaggle/input/torch-geometric/torch_sparse-0.6.18-cp310-cp310-linux_x86_64.whl\n!pip install  \"pyscf>=2.10.0\" \"pyscf-semiempirical>=0.1.1\" \"pysmiles>=2.0.0\" \"rdkit>=2025.3.5\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:18:23.396404Z","iopub.execute_input":"2025-09-14T21:18:23.396678Z","iopub.status.idle":"2025-09-14T21:18:53.356584Z","shell.execute_reply.started":"2025-09-14T21:18:23.396648Z","shell.execute_reply":"2025-09-14T21:18:53.355750Z"},"_kg_hide-output":true},"outputs":[{"name":"stdout","text":"Looking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\nCollecting torch-geometric\n  Downloading torch_geometric-2.6.1-py3-none-any.whl.metadata (63 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.1/63.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.12.13)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2025.5.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.1.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (7.0.0)\nRequirement already satisfied: pyparsing in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (3.0.9)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (2.32.4)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from torch-geometric) (4.67.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->torch-geometric) (1.20.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch-geometric) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torch-geometric) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->torch-geometric) (2025.6.15)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torch-geometric) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torch-geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torch-geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torch-geometric) (2024.2.0)\nDownloading torch_geometric-2.6.1-py3-none-any.whl (1.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-geometric\nSuccessfully installed torch-geometric-2.6.1\nLooking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\nCollecting torch-cluster\n  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_cluster-1.6.3%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (3.4 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from torch-cluster) (1.15.3)\nRequirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy->torch-cluster) (1.26.4)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<2.5,>=1.23.5->scipy->torch-cluster) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<2.5,>=1.23.5->scipy->torch-cluster) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<2.5,>=1.23.5->scipy->torch-cluster) (2024.2.0)\nInstalling collected packages: torch-cluster\nSuccessfully installed torch-cluster-1.6.3+pt26cu124\nLooking in links: https://data.pyg.org/whl/torch-2.6.0+cu124.html\nCollecting torch-scatter\n  Downloading https://data.pyg.org/whl/torch-2.6.0%2Bcu124/torch_scatter-2.1.2%2Bpt26cu124-cp311-cp311-linux_x86_64.whl (10.8 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m171.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: torch-scatter\nSuccessfully installed torch-scatter-2.1.2+pt26cu124\nCollecting pyscf>=2.10.0\n  Downloading pyscf-2.10.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\nCollecting pyscf-semiempirical>=0.1.1\n  Downloading pyscf-semiempirical-0.1.1.tar.gz (30 kB)\n  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nCollecting pysmiles>=2.0.0\n  Downloading pysmiles-2.0.0-py2.py3-none-any.whl.metadata (14 kB)\nCollecting rdkit>=2025.3.5\n  Downloading rdkit-2025.3.6-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: numpy!=1.16,!=1.17,>=1.13 in /usr/local/lib/python3.11/dist-packages (from pyscf>=2.10.0) (1.26.4)\nRequirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from pyscf>=2.10.0) (1.15.3)\nRequirement already satisfied: h5py>=2.7 in /usr/local/lib/python3.11/dist-packages (from pyscf>=2.10.0) (3.14.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyscf>=2.10.0) (75.2.0)\nCollecting pbr (from pysmiles>=2.0.0)\n  Downloading pbr-7.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pysmiles>=2.0.0) (3.5)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit>=2025.3.5) (11.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy!=1.16,!=1.17,>=1.13->pyscf>=2.10.0) (2024.2.0)\nDownloading pyscf-2.10.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (51.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.2/51.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pysmiles-2.0.0-py2.py3-none-any.whl (37 kB)\nDownloading rdkit-2025.3.6-cp311-cp311-manylinux_2_28_x86_64.whl (36.2 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.2/36.2 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading pbr-7.0.1-py2.py3-none-any.whl (126 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.1/126.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hBuilding wheels for collected packages: pyscf-semiempirical\n  Building wheel for pyscf-semiempirical (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for pyscf-semiempirical: filename=pyscf_semiempirical-0.1.1-cp311-cp311-linux_x86_64.whl size=57467 sha256=1eabb1329ad54cfc584a324dd2ab9567638c93c6c26156e5376e261d5e23108f\n  Stored in directory: /root/.cache/pip/wheels/68/ed/27/b9089a75f0fc2374f29a237630f068dec796312ccc2bafd6b3\nSuccessfully built pyscf-semiempirical\nInstalling collected packages: pbr, pysmiles, pyscf, rdkit, pyscf-semiempirical\nSuccessfully installed pbr-7.0.1 pyscf-2.10.0 pyscf-semiempirical-0.1.1 pysmiles-2.0.0 rdkit-2025.3.6\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import os\nimport sys\nimport math\nimport json\nfrom typing import List, Tuple, Dict\nfrom datetime import datetime\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\n\n# Data props\nPROPERTIES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\nINPUTS = [\"SMILES\"]\nAUX_INFO = [\"monomer_count\", \"original_atoms\", \"final_atoms\", \"SMILES\"]\n\n# y output\nTARGET_DIM = len(PROPERTIES)\n\n# Params\nEPOCHS = 2\nBATCH_SIZE = 16\nLR = 1e-3\nWEIGHT_DECAY = 1e-4\nCHECKPOINT_EVERY_N_STEPS = 100\nK_HEADS = 16\n\n# data params\nVAL_RATIO = 0.1\nMIN_LENGTH = 100\nMAX_OUTPUT = 10\n\n\n# Random seeding\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:18:53.357737Z","iopub.execute_input":"2025-09-14T21:18:53.357999Z","iopub.status.idle":"2025-09-14T21:19:02.149969Z","shell.execute_reply.started":"2025-09-14T21:18:53.357973Z","shell.execute_reply":"2025-09-14T21:19:02.149290Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"import os\n\nPROJECT_ROOT='/kaggle/working/'\nCHECKPOINT_DIR = os.path.join(PROJECT_ROOT, \"checkpoints\", \"hrm\")\nos.makedirs(CHECKPOINT_DIR, exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:02.151468Z","iopub.execute_input":"2025-09-14T21:19:02.151871Z","iopub.status.idle":"2025-09-14T21:19:02.156283Z","shell.execute_reply.started":"2025-09-14T21:19:02.151848Z","shell.execute_reply":"2025-09-14T21:19:02.155682Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Load data\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\nLOAD_COLS = (INPUTS + PROPERTIES)\n\nraw_df = pd.DataFrame(columns=LOAD_COLS)\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        if ('dataset' in filename and '.csv' in filename) or 'train.csv' in filename:\n            df_temp = pd.read_csv(os.path.join(dirname, filename))\n            df_temp.reindex(columns=LOAD_COLS)\n            raw_df = pd.concat([raw_df, df_temp], ignore_index=True)\n\nprint(len(raw_df))\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:02.156960Z","iopub.execute_input":"2025-09-14T21:19:02.157180Z","iopub.status.idle":"2025-09-14T21:19:02.332962Z","shell.execute_reply.started":"2025-09-14T21:19:02.157145Z","shell.execute_reply":"2025-09-14T21:19:02.331996Z"}},"outputs":[{"name":"stdout","text":"16963\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_36/1308728939.py:15: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n  raw_df = pd.concat([raw_df, df_temp], ignore_index=True)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":" # !pip install -e /kaggle/input/khrm-for-openpolymer --no-deps --no-build-isolation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:02.333713Z","iopub.execute_input":"2025-09-14T21:19:02.333972Z","iopub.status.idle":"2025-09-14T21:19:02.337545Z","shell.execute_reply.started":"2025-09-14T21:19:02.333954Z","shell.execute_reply":"2025-09-14T21:19:02.336889Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"!pip install pysmiles\n!pip install rdkit","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:02.338510Z","iopub.execute_input":"2025-09-14T21:19:02.339127Z","iopub.status.idle":"2025-09-14T21:19:08.914213Z","shell.execute_reply.started":"2025-09-14T21:19:02.339100Z","shell.execute_reply":"2025-09-14T21:19:08.913460Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: pysmiles in /usr/local/lib/python3.11/dist-packages (2.0.0)\nRequirement already satisfied: pbr in /usr/local/lib/python3.11/dist-packages (from pysmiles) (7.0.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pysmiles) (3.5)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pbr->pysmiles) (75.2.0)\nRequirement already satisfied: rdkit in /usr/local/lib/python3.11/dist-packages (2025.3.6)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit) (1.26.4)\nRequirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit) (11.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit) (2024.2.0)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import sys\nsys.path.append(\"/kaggle/input/khrm-for-openpolymer/src\")\nimport data_gen_helpers\nfrom data_gen_helpers import iterative_extend_smiles, count_non_hydrogen_atoms\n\nimport logging\nfrom datetime import datetime\n\nfrom multiprocessing import Pool, cpu_count\nN_PROCESSES = max(cpu_count()-1, 1)\n\ndef process_row(args):\n    idx, row = args\n    original_smiles = row[\"SMILES\"]\n    original_atoms = count_non_hydrogen_atoms(original_smiles)\n\n    try:\n        extensions = list(iterative_extend_smiles(\n            original_smiles,\n            min_length=MIN_LENGTH,\n            max_output=MAX_OUTPUT\n        ))\n\n        results = []\n        if extensions:\n            for extended_smiles, monomer_count in extensions:\n                final_atoms = count_non_hydrogen_atoms(extended_smiles)\n                new_row = row.copy()\n                new_row[\"SMILES\"] = extended_smiles\n                new_row[\"monomer_count\"] = monomer_count\n                new_row[\"original_smiles\"] = original_smiles\n                new_row[\"original_atoms\"] = original_atoms\n                new_row[\"final_atoms\"] = final_atoms\n                results.append((\"success\", new_row))\n        else:\n            return [(\"fail\", (idx, original_smiles, \"No extensions generated\"))]\n\n        # FIX: don't double-wrap\n        return results\n\n    except Exception as e:\n        return [(\"fail\", (idx, original_smiles, str(e)))]\n        \n# ---- Parallel Execution ----\ndef parallel_extend(raw_df):\n    extended_data = []\n    failed_extensions = []\n\n    with Pool(N_PROCESSES) as pool:\n        for results in pool.imap_unordered(process_row, raw_df.iterrows(), chunksize=10):\n            for status, payload in results:\n                if status == \"success\":\n                    extended_data.append(payload)\n                else:\n                    failed_extensions.append(payload)\n\n    # Build DataFrame\n    mixed_fields=([\"SMILES\", \"monomer_count\", \"original_smiles\", \"original_atoms\", \"final_atoms\"]+PROPERTIES)\n    extended_df = pd.DataFrame(extended_data, columns = mixed_fields)\n    print(f\"\\nSuccessfully extended: {len(extended_df)} molecules\")\n    print(f\"Failed extensions: {len(failed_extensions)}\")\n\n    if failed_extensions:\n        print(\"\\n\\nFailed molecules:\\n\\n\")\n        for idx, smiles, error in failed_extensions[:5]:\n            print(f\"  {idx}: {smiles} - {error}\")\n\n    if len(extended_df) == 0:\n        raise RuntimeError(\"No molecules could be extended. Check your data and extension logic.\")\n\n    return extended_df, failed_extensions\n\n# ---- Run ----\nextended_df, failed_extensions = parallel_extend(raw_df[:2])\nif len(failed_extensions) > 0:\n    print(\"\\n\\nFailed molecules:\\n\\n\")\n    for idx, smiles, error in failed_extensions:  # Show first 5 errors\n        print(f\"  {idx}: {smiles} - {error}\")\n\nprint(extended_df.columns)\n\n# Use extended data for training\nif len(extended_df) == 0:\n    raise RuntimeError(\"No molecules could be extended. Check your data and extension logic.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:08.915211Z","iopub.execute_input":"2025-09-14T21:19:08.915522Z","iopub.status.idle":"2025-09-14T21:19:10.641096Z","shell.execute_reply.started":"2025-09-14T21:19:08.915487Z","shell.execute_reply":"2025-09-14T21:19:10.640310Z"}},"outputs":[{"name":"stdout","text":"\nSuccessfully extended: 11 molecules\nFailed extensions: 0\nIndex(['SMILES', 'monomer_count', 'original_smiles', 'original_atoms',\n       'final_atoms', 'Tg', 'FFV', 'Tc', 'Density', 'Rg'],\n      dtype='object')\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!pip install geometric","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:10.642035Z","iopub.execute_input":"2025-09-14T21:19:10.642388Z","iopub.status.idle":"2025-09-14T21:19:16.138920Z","shell.execute_reply.started":"2025-09-14T21:19:10.642366Z","shell.execute_reply":"2025-09-14T21:19:16.137904Z"}},"outputs":[{"name":"stdout","text":"Collecting geometric\n  Downloading geometric-1.1.tar.gz (386 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.0/386.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.11/dist-packages (from geometric) (1.26.4)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from geometric) (3.5)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from geometric) (1.17.0)\nRequirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from geometric) (1.15.3)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.11->geometric) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.11->geometric) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.11->geometric) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.11->geometric) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.11->geometric) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.11->geometric) (2024.2.0)\nBuilding wheels for collected packages: geometric\n  Building wheel for geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for geometric: filename=geometric-1.1-py3-none-any.whl size=402087 sha256=58f54fad9a76230fa357fd4a6da020d07da44756316fc1a085637258f7c1d1a3\n  Stored in directory: /root/.cache/pip/wheels/c5/77/0c/96a54539fe0560749fdbe283f92582d09bddb212856407a4cb\nSuccessfully built geometric\nInstalling collected packages: geometric\nSuccessfully installed geometric-1.1\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from dataset_helpers import smiles_iter_to_graph_dataset\n\nimport numpy as np\nfrom torch_geometric.loader import DataLoader\n\n# Split into Train/Val\nnum_rows = len(extended_df)\nperm = np.random.RandomState(SEED).permutation(num_rows)\ntrain_count = int((1.0 - VAL_RATIO) * num_rows)\ntrain_idx, val_idx = perm[:train_count], perm[train_count:]\ntrain_df = extended_df.iloc[train_idx].reset_index(drop=True)\nval_df = extended_df.iloc[val_idx].reset_index(drop=True)\n\n# Create graph datasets\ntrain_dataset = smiles_iter_to_graph_dataset(train_df[\"SMILES\"], torch.tensor(train_df[PROPERTIES].to_numpy(), dtype=torch.float32, device=device), aux_info=train_df[AUX_INFO])\nval_dataset = smiles_iter_to_graph_dataset(val_df[\"SMILES\"], torch.tensor(val_df[PROPERTIES].to_numpy(), dtype=torch.float32, device=device), aux_info=val_df[AUX_INFO])\n\nprint(f\"example aux_info: {train_dataset[0].aux_info}\")\nprint(f\"Train graphs: {len(train_dataset)} | Val graphs: {len(val_dataset)}\")\n\n# DataListLoader (batched lists of Data objects)\ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n# Derive input dimension from first training graph\nif len(train_dataset) == 0:\n    raise RuntimeError(\"Training dataset is empty after preprocessing.\")\nINPUT_DIM = train_dataset[0].x.shape[1]\nprint(f\"Input dim: {INPUT_DIM}, Target dim: {TARGET_DIM}\")\n\nEDGE_DIM = train_dataset[0].edge_attr.shape[1]\nprint(f\"Edge dim: {EDGE_DIM}\")\n\n# Show some examples of the extensions\nprint(f\"\\nExamples of SMILES extensions:\")\nfor i in range(min(3, len(train_df))):\n    row = train_df.iloc[i]\n    print(f\"Original ({row.get('original_atoms', 'N/A')} atoms): {row.get('original_smiles', 'N/A')}\")\n    print(f\"Extended ({row.get('final_atoms', 'N/A')} atoms): {row['SMILES']}\")\n    print()\n\nTOTAL_BATCH_COUNT = len(train_loader)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:16.142173Z","iopub.execute_input":"2025-09-14T21:19:16.142460Z","iopub.status.idle":"2025-09-14T21:19:29.510094Z","shell.execute_reply.started":"2025-09-14T21:19:16.142436Z","shell.execute_reply":"2025-09-14T21:19:29.509178Z"}},"outputs":[{"name":"stdout","text":"example aux_info: [2 17 104\n 'CCCCCCOC(=O)c1ccccc1C(C)CC(CCC(CC(CC(CC(C)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC']\nTrain graphs: 9 | Val graphs: 2\nInput dim: 6, Target dim: 5\nEdge dim: 4\n\nExamples of SMILES extensions:\nOriginal (17 atoms): *CC(*)c1ccccc1C(=O)OCCCCCC\nExtended (104 atoms): CCCCCCOC(=O)c1ccccc1C(C)CC(CCC(CC(CC(CC(C)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC\n\nOriginal (17 atoms): *CC(*)c1ccccc1C(=O)OCCCCCC\nExtended (36 atoms): CCCCCCOC(=O)c1ccccc1C(C)CCC(C)c1ccccc1C(=O)OCCCCCC\n\nOriginal (45 atoms): *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5ccc(N*)cc5)cc4)CCC(CCCCC)CC3)cc2)cc1\nExtended (92 atoms): CCCCCC1CCC(c2ccc(C(CCC)c3ccc(NC)cc3)cc2)(c2ccc(C(CCC)c3ccc(NNc4ccc(C(CCC)c5ccc(C6(c7ccc(C(CCC)c8ccc(NC)cc8)cc7)CCC(CCCCC)CC6)cc5)cc4)cc3)cc2)CC1\n\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Use KMeansHRMModule from kmeans_hrm_model.py\nfrom kmeans_hrm_model import (\n    KMeansHRMModule, KMeansHRMConfig, KMeansHRMInnerModuleConfig, KMeansHRMInitialCarry,\n    KMeansConfig, KMeansHeadConfig, OutputHeadConfig,\n    SpectralWeighting, SpectralWeightingConfig,\n    DiscreteMeanCenter, DiscreteMeanCenterConfig,\n    RadiusAttentionWeights, RadiusMaskConfig\n)\nfrom torch.optim import Adam\nfrom torch_geometric.nn import GATConv\n\ndef count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\ndef create_kmeans_hrm_config(input_dim: int, edge_dim: int, hidden_dim: int = 128, latent_dim: int = 128, output_dim: int = TARGET_DIM, k: int = K_HEADS) -> KMeansHRMConfig:\n    \n    # Spectral Weighting Configuration\n    spectral_config = SpectralWeightingConfig(\n        node_channels=latent_dim,\n        K=3,  # Chebyshev polynomial order\n        num_layers=3,\n        normalization='sym',\n        bias=True,\n        dropout=0.2,\n        norm='batch',\n        norm_kwargs={'in_channels': latent_dim}\n    )\n    \n    # Center Module Configuration\n    center_config = DiscreteMeanCenterConfig(\n        distance_metric='euclidean'\n    )\n    \n    # Radius Mask Configuration (simplified weighting module)\n    radius_weighting = GATConv(latent_dim, latent_dim)\n    radius_config = RadiusMaskConfig(\n        max_num_neighbors=50,\n        radius=20,\n        weighting_module=radius_weighting,\n        threshold=0.1,\n        node_dim=latent_dim\n    )\n    \n    # KMeans Head Configuration\n    kmeans_head_config = KMeansHeadConfig(\n        node_count=k,\n        node_dim=latent_dim,\n        max_nodes=100,  \n        num_layers=5,\n        dropout=0.2,\n        weighting_module=SpectralWeighting(spectral_config),\n        center_module=DiscreteMeanCenter(center_config),\n        mask_module=RadiusAttentionWeights(radius_config),\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # KMeans Configuration\n    kmeans_config = KMeansConfig(\n        k=k,\n        max_iter=15,\n        thresh=1e-6,\n        max_overlap=2,\n        head_module=kmeans_head_config,\n        excluded_is_cluster=True\n    )\n    \n    # Output Head Configuration\n    output_head_config = OutputHeadConfig(\n        node_dim=latent_dim,\n        hidden_dim=hidden_dim,\n        output_dim=output_dim,\n        pooling_type='mean',\n        norm='batch',\n        norm_kwargs={'in_channels': hidden_dim},\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # Policy Module Configuration\n    policy_config = OutputHeadConfig(\n        node_dim=latent_dim,\n        hidden_dim=hidden_dim//2,\n        output_dim=2,  # halt=0, continue=1\n        pooling_type='mean',\n        norm='batch',\n        norm_kwargs={'in_channels': hidden_dim//2},\n        act='relu',\n        act_kwargs={}\n    )\n    \n    # Inner Module Configuration\n    inner_config = KMeansHRMInnerModuleConfig(\n        add_self_loops=True,\n        add_negative_edges=True,\n        dropout=0.2,\n        hidden_dim=hidden_dim,        # inner-side hidden size, reused\n        node_dim=latent_dim,          # must equal vgae_latent_dim\n        attention_dim=4,           # bigger in final\n        edge_dim=edge_dim,\n        layers=3,\n        kmeans_config=kmeans_config,\n        output_head_config=output_head_config,\n        policy_module_config=policy_config,\n        K_cycles=8,\n        L_cycles=30,\n        batch_size=BATCH_SIZE,\n        halt_max_steps=10,\n        halt_exploration_prob=0.1,\n    )\n    \n    config = KMeansHRMConfig(\n        inner_module=inner_config,\n        explore_steps_prob=0.1,\n        halt_max_steps=10,\n        pre_encoder_conv_layers=2,\n        vgae_encoder_type=\"cheb\",\n        input_dim=input_dim,\n        edge_attr_dim=edge_dim,\n        vgae_latent_dim=latent_dim,           # must equal inner.node_dim\n        vgae_encoder_layers=2,\n        vgae_encoder_dropout=0.1,\n        vgae_decoder_type=None,\n        vgae_kl_weight=1.0,\n    )\n    \n    return config\n\n# Modell initialisieren\n#hrm_config = create_kmeans_hrm_config(INPUT_DIM, EDGE_DIM, latent_dim=256, hidden_dim=128, output_dim=TARGET_DIM, k=16)\nhrm_config = create_kmeans_hrm_config(INPUT_DIM, EDGE_DIM, latent_dim=16, hidden_dim=16, output_dim=TARGET_DIM, k=2)\nmodel = KMeansHRMModule(hrm_config, training=True).to(device)\noptimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n\n# Print number of parameters\nnum_params = count_parameters(model)\nprint(f\"KMeansHRMModule created with {num_params:,} trainable parameters\")\nprint(f\"Model size: {num_params * 4 / 1024 / 1024:.2f} MB (float32)\")\nprint(model)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:29.511155Z","iopub.execute_input":"2025-09-14T21:19:29.511693Z","iopub.status.idle":"2025-09-14T21:19:29.718895Z","shell.execute_reply.started":"2025-09-14T21:19:29.511673Z","shell.execute_reply":"2025-09-14T21:19:29.718223Z"}},"outputs":[{"name":"stdout","text":"KMeansHRMModule created with 11,236 trainable parameters\nModel size: 0.04 MB (float32)\nKMeansHRMModule(\n  (inner_module): KMeansHRMInnerModule(\n    (kmeans_module): KMeans(\n      (heads): ModuleList(\n        (0-1): 2 x KMeansHead(\n          (weighting_module): SpectralWeighting(\n            (cheb_convs): ModuleList(\n              (0-2): 3 x ChebConv(16, 16, K=3, normalization=sym)\n            )\n            (norms): ModuleList(\n              (0-2): 3 x BatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n            )\n          )\n          (center_module): DiscreteMeanCenter(\n            (distance_module): PairwiseDistance()\n          )\n          (mask_module): RadiusAttentionWeights(\n            (weighting_module): GATConv(16, 16, heads=1)\n            (_mask_linear): Linear(in_features=16, out_features=1, bias=True)\n          )\n          (act): ReLU()\n        )\n      )\n    )\n    (vgae_encoder): VGAEEncoder(\n      (norms): ModuleList(\n        (0-2): 3 x LayerNorm(4, affine=True, mode=graph)\n      )\n      (convs): ModuleList(\n        (0): GCNConv(16, 4)\n        (1-2): 2 x GCNConv(4, 4)\n        (3): GCNConv(4, 16)\n      )\n      (edge_attr_linear): Linear(in_features=4, out_features=1, bias=True)\n    )\n    (vgae): VGAE(\n      (encoder): VGAEEncoder(\n        (norms): ModuleList(\n          (0-2): 3 x LayerNorm(4, affine=True, mode=graph)\n        )\n        (convs): ModuleList(\n          (0): GCNConv(16, 4)\n          (1-2): 2 x GCNConv(4, 4)\n          (3): GCNConv(4, 16)\n        )\n        (edge_attr_linear): Linear(in_features=4, out_features=1, bias=True)\n      )\n      (decoder): InnerProductDecoder()\n    )\n    (linear_post_attention): Sequential(\n      (0): Linear(in_features=16, out_features=128, bias=True)\n      (1): ReLU()\n      (2): Dropout(p=0.2, inplace=False)\n      (3): Linear(in_features=128, out_features=16, bias=True)\n      (4): LayerNorm((16,), eps=1e-05, elementwise_affine=True)\n    )\n    (norm): LayerNorm(16, affine=True, mode=graph)\n    (dropout_layer): Dropout(p=0.2, inplace=False)\n    (output_head): OutputHead(\n      (linear1): Linear(16, 16, bias=True)\n      (linear2): Linear(16, 5, bias=True)\n      (norm): BatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act): ReLU()\n    )\n    (policy_module): OutputHead(\n      (linear1): Linear(16, 8, bias=True)\n      (linear2): Linear(8, 2, bias=True)\n      (norm): BatchNorm(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act): ReLU()\n    )\n  )\n  (pre_encoder_conv): NNConv(6, 6, aggr=mean, nn=Sequential(\n    (0): Linear(in_features=4, out_features=24, bias=True)\n    (1): Linear(in_features=24, out_features=24, bias=True)\n    (2): ReLU()\n    (3): Dropout(p=0.1, inplace=False)\n    (4): Linear(in_features=24, out_features=24, bias=True)\n    (5): ReLU()\n    (6): Dropout(p=0.1, inplace=False)\n    (7): Linear(in_features=24, out_features=36, bias=True)\n  ))\n  (vgae_encoder): VGAEEncoder(\n    (norms): ModuleList(\n      (0): LayerNorm(16, affine=True, mode=graph)\n    )\n    (convs): ModuleList(\n      (0): ChebConv(6, 16, K=3, normalization=sym)\n      (1): ChebConv(16, 16, K=3, normalization=sym)\n    )\n    (edge_attr_linear): Linear(in_features=4, out_features=1, bias=True)\n  )\n  (vgae): VGAE(\n    (encoder): VGAEEncoder(\n      (norms): ModuleList(\n        (0): LayerNorm(16, affine=True, mode=graph)\n      )\n      (convs): ModuleList(\n        (0): ChebConv(6, 16, K=3, normalization=sym)\n        (1): ChebConv(16, 16, K=3, normalization=sym)\n      )\n      (edge_attr_linear): Linear(in_features=4, out_features=1, bias=True)\n    )\n    (decoder): InnerProductDecoder()\n  )\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"from training_loops import composite_loss, compute_mae_in_bounds, init_property_bounds\n\ninit_property_bounds(PROPERTIES, extended_df)\n\nfrom typing import TypedDict, Dict, Any\nfrom dataclasses import dataclass\n\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, random_split\nfrom torch.optim import Adam\n\nfrom torch_geometric.data import Data, Batch\n\ndef save_checkpoint(state: Dict, step: int, is_best: bool = False):\n    path = os.path.join(CHECKPOINT_DIR, f\"step_{step}.pt\")\n    torch.save(state, path)\n    if is_best:\n        best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n        torch.save(state, best_path)\n\n\nclass PretrainConfig(TypedDict):\n    # Data\n    data_path: str\n\n    # Hyperparams\n    global_batch_size: int\n    epochs: int\n    total_iters: int\n\n    lr: float\n    lr_min_ratio: float\n    lr_warmup_steps: int\n\n    weight_decay: float\n    beta1: float\n    beta2: float\n\n\n@dataclass\nclass TrainState:\n    model: nn.Module\n    optimizer: torch.optim.Optimizer\n    carry: KMeansHRMInitialCarry | None\n\n    step: int\n    total_steps: int\n\n\ndef compute_warmup_weight(step: int, warmup_steps: int, min_ratio: float) -> float:\n    if warmup_steps <= 0:\n        return 1.0\n    if step < warmup_steps:\n        # Linear warmup from min_ratio -> 1.0\n        return float(min_ratio + (1.0 - min_ratio) * (step / max(1, warmup_steps)))\n    return 1.0\n\n\ndef pack_train_state_for_save(ts: TrainState) -> Dict[str, Any]:\n    return {\n        \"step\": int(ts.step),\n        \"total_steps\": int(ts.total_steps),\n        # Carry can be large; still useful for exact resume within the same batch sequence\n        \"carry\": ts.carry,\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:29.719703Z","iopub.execute_input":"2025-09-14T21:19:29.719949Z","iopub.status.idle":"2025-09-14T21:19:29.735888Z","shell.execute_reply.started":"2025-09-14T21:19:29.719921Z","shell.execute_reply":"2025-09-14T21:19:29.735258Z"}},"outputs":[{"name":"stdout","text":"Loss and metrics initialized.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def train_batch(epoch: int, train_state: TrainState, batch_data: Batch, config: PretrainConfig) -> Dict[str, float]:\n    model = train_state.model\n    optimizer = train_state.optimizer\n\n    model.train()\n\n    # Targets [B,5]\n    y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n    batch_data = batch_data.to(device)\n\n    related_info = [torch.as_tensor(d[0]).unsqueeze(-1).to(device) for d in batch_data.aux_info]\n    #related_info = batch_data.aux_info\n    \n    # Reuse existing carry if provided; otherwise initialize\n    if train_state.carry is None:\n        print(f\"Initializing new carry for batch {batch_data.batch.shape[0]}\")\n        train_state.carry = model.initial_carry(batch_data)\n\n    print(train_state.carry.steps)\n    print(batch_data)\n    print(train_state.carry.halted)\n    print()\n    # Forward\n    train_state.carry, hrm_output = model(train_state.carry, batch_data)\n    preds = hrm_output['y_pred'].to(device)\n    policy = hrm_output['q_policy']\n    target_policy = hrm_output.get('target_q_policy', None)\n\n    targets = y\n    properties = PROPERTIES\n\n    if target_policy is not None and target_policy[0].size == policy[0].size:\n\n        preds = torch.cat([preds, policy[0].unsqueeze(-1), policy[1].unsqueeze(-1)], dim=1)\n        targets = torch.cat([y, target_policy[0].unsqueeze(-1), target_policy[1].unsqueeze(-1)], dim=1)\n\n        properties += ['q_halt', 'q_cont']\n\n    assert preds.size(0) == targets.size(0), f\"batch mismatch: preds {preds.shape}, targets {targets.shape}\"\n    assert len(properties) == targets.size(1), f\"mismatch {y.size} {properties}\"\n\n    loss = composite_loss(0, properties, preds, targets, related_info)\n\n    train_state.step += 1\n    warmup_w = compute_warmup_weight(train_state.step, config[\"lr_warmup_steps\"], config[\"lr_min_ratio\"])  # type: ignore[index]\n    scaled_loss = loss * warmup_w * (1.0 / TOTAL_BATCH_COUNT)\n\n    # Backward/update\n    optimizer.zero_grad()\n    scaled_loss.backward()\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n    optimizer.step()\n\n    # Metrics (unscaled loss for logging)\n    with torch.no_grad():\n        metrics = compute_mae_in_bounds(0, properties, preds, targets, related_info)\n        metrics.update({\n            \"loss\": loss.item(),\n            \"warmup_weight\": float(warmup_w),\n        })\n\n\n    # Periodic checkpoint\n    global_step = train_state.step\n    if global_step % CHECKPOINT_EVERY_N_STEPS == 0:\n        save_checkpoint({\n            \"epoch\": epoch,\n            \"global_step\": global_step,\n            \"model_state\": model.state_dict(),\n            \"optimizer_state\": optimizer.state_dict(),\n            \"model_class\": model.__class__.__name__,\n            \"model_module\": model.__class__.__module__,\n            \"train_state\": pack_train_state_for_save(train_state),\n        }, step=global_step)\n\n    return metrics\n\n\n@torch.no_grad()\ndef validate(epoch: int, model: nn.Module, loader: DataLoader) -> Dict[str, float]:\n    model.eval()\n    running_loss = 0.0\n    mae_accum = {f\"mae_{p}\": 0.0 for p in PROPERTIES + ['q_halt', 'q_cont']}\n    count_samples = 0\n\n    for batch_data in loader:\n        # Targets [B,5]\n        y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n        batch_data = batch_data.to(device)\n\n        related_info = [torch.as_tensor(d[0]).unsqueeze(-1).to(device) for d in batch_data.aux_info]\n        #related_info = batch_data.aux_info\n        \n        carry = model.initial_carry(batch_data)\n        \n        train_state.carry, hrm_output = model(train_state.carry, batch_data)\n        preds = hrm_output['y_pred'].to(device)\n        policy = hrm_output['q_policy']\n        target_policy = hrm_output.get('target_q_policy', None)\n        \n        targets = y\n        properties = PROPERTIES\n\n        if target_policy is not None and target_policy[0].size == policy[0].size:\n    \n            preds = torch.cat([preds, policy[0].unsqueeze(-1), policy[1].unsqueeze(-1)], dim=1)\n            targets = torch.cat([y, target_policy[0].unsqueeze(-1), target_policy[1].unsqueeze(-1)], dim=1)\n    \n            properties += ['q_halt', 'q_cont']\n\n        assert preds.size(0) == y.size(0), f\"batch mismatch: preds {preds.shape}, targets {targets.shape}\"\n        assert len(properties) == targets.size(1), f\"mismatch {y.size} {properties}\"\n    \n        loss = composite_loss(0, properties, preds, targets, related_info)\n\n        metrics = compute_mae_in_bounds(0, properties, preds, targets, related_info)\n        for k, v in metrics.items():\n            if not math.isnan(v):\n                mae_accum[k] += v * preds.size(0)\n        running_loss += loss.item() * preds.size(0)\n        count_samples += preds.size(0)\n\n    avg_loss = running_loss / max(1, count_samples)\n    avg_mae = {k: (v / max(1, count_samples)) for k, v in mae_accum.items()}\n\n    return {\"loss\": avg_loss, **avg_mae}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:29.736746Z","iopub.execute_input":"2025-09-14T21:19:29.737035Z","iopub.status.idle":"2025-09-14T21:19:29.754782Z","shell.execute_reply.started":"2025-09-14T21:19:29.737010Z","shell.execute_reply":"2025-09-14T21:19:29.754054Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from training_loops import train_until_total_iters\n\n# Build a minimal config for warmup from existing hyperparams\nCONFIG: PretrainConfig = {\n    \"data_path\": \"\",\n    \"global_batch_size\": BATCH_SIZE,\n    \"epochs\": EPOCHS,\n    # Step-based training support; used by helper loop\n    \"total_iters\": int(EPOCHS * len(train_loader)),\n    \"lr\": LR,\n    \"lr_min_ratio\": 0.1,\n    \"lr_warmup_steps\": max(1, len(train_loader) * 2),  # warm up first ~2 epochs of steps\n    \"weight_decay\": WEIGHT_DECAY,\n    \"beta1\": 0.9,\n    \"beta2\": 0.999,\n}\n\nhistory = {\"train\": [], \"val\": []}\nbest_val_loss = float(\"inf\")\n\n# Initialize TrainState with total steps estimated from loader length and epochs\ntrain_state = TrainState(\n    model=model,\n    optimizer=optimizer,\n    carry=None,\n    step=0,\n    total_steps=EPOCHS * len(train_loader)\n)\n\nlast_completed_epoch=0\n\ntrain_until_total_iters(\n    total_iters=CONFIG[\"total_iters\"],\n    start_epoch=last_completed_epoch,\n    train_state=train_state,\n    train_loader=train_loader,\n    val_loader=val_loader,\n    config=CONFIG,\n    train_batch_fn=train_batch,\n    validate_fn=validate,\n    save_checkpoint_fn=save_checkpoint,\n    checkpoint_every_n=10,\n    print_every_n=1\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:29.755307Z","iopub.execute_input":"2025-09-14T21:19:29.755501Z","iopub.status.idle":"2025-09-14T21:19:42.161844Z","shell.execute_reply.started":"2025-09-14T21:19:29.755486Z","shell.execute_reply":"2025-09-14T21:19:42.160815Z"}},"outputs":[{"name":"stdout","text":"Initializing new carry for batch 787\ntensor([0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0', dtype=torch.int32)\nDataBatch(x=[787, 6], edge_index=[2, 1670], edge_attr=[1670, 4], y=[45], aux_info=[9], batch=[787], ptr=[10])\ntensor([False, False, False, False, False, False, False, False, False],\n       device='cuda:0')\n\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/4029097517.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mlast_completed_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m train_until_total_iters(\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0mtotal_iters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mCONFIG\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total_iters\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mstart_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlast_completed_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/khrm-for-openpolymer/src/training_loops.py\u001b[0m in \u001b[0;36mtrain_until_total_iters\u001b[0;34m(total_iters, start_epoch, train_state, train_loader, val_loader, config, train_batch_fn, validate_fn, save_checkpoint_fn, checkpoint_every_n, print_every_n)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_batch_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0mbatch_size_effective\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"num_graphs\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_36/1035917375.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(epoch, train_state, batch_data, config)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;31m# Forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhrm_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcarry\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhrm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'y_pred'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhrm_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q_policy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/kaggle/input/khrm-for-openpolymer/src/kmeans_hrm_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, carry, data)\u001b[0m\n\u001b[1;32m   1500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1501\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalt_max_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1502\u001b[0;31m                 \u001b[0mhalted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhalted\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mq_halt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mq_continue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1504\u001b[0m                 \u001b[0mmin_halt_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_halt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexplore_steps_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhigh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhalt_max_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (9) must match the size of tensor b (2) at non-singleton dimension 0"],"ename":"RuntimeError","evalue":"The size of tensor a (9) must match the size of tensor b (2) at non-singleton dimension 0","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n\nmetrics_path = os.path.join(CHECKPOINT_DIR, \"history.json\")\nwith open(metrics_path, \"w\") as f:\n    json.dump(history, f, indent=2)\nprint(f\"Saved metrics to {metrics_path}\")\n\n# Plot\ndef plot_curves(history):\n    epochs = [e[\"epoch\"] for e in history[\"train\"]]\n    train_losses = [e[\"loss\"] for e in history[\"train\"]]\n    val_losses = [e[\"loss\"] for e in history[\"val\"]]\n\n    plt.figure(figsize=(8,4))\n    plt.plot(epochs, train_losses, label=\"train_loss\")\n    plt.plot(epochs, val_losses, label=\"val_loss\")\n    plt.xlabel(\"Epoch\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.grid(True)\n    plt.show()\n\nplot_curves(history)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:42.162410Z","iopub.status.idle":"2025-09-14T21:19:42.162718Z","shell.execute_reply.started":"2025-09-14T21:19:42.162563Z","shell.execute_reply":"2025-09-14T21:19:42.162575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from smiles_iter_to_graph_dataset import smiles_to_graph_dataset\n\nsubmission_df = pd.read_csv('/kaggle/input/open-polymer-challenge/test.csv')\ninputs = submissions_df[\"SMILES\"]\n\ninputs = []\nfor smiles in inputs:\n    submission_dataset = smiles_iter_to_graph_dataset(smiles, {}, None)\n\nsubmissions_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:42.163565Z","iopub.status.idle":"2025-09-14T21:19:42.163903Z","shell.execute_reply.started":"2025-09-14T21:19:42.163737Z","shell.execute_reply":"2025-09-14T21:19:42.163753Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.eval()\n\nmax_iters = 10\n\nin_loader = DataLoader(inputs, batch_size=1, shuffle=False)\nfor datum in inputs:\n    preds = [0, 0, 0, 0, 0]\n    while max_iters > 0:\n        carry = model.initial_carry(datum)\n        og_carry, hrm_output = model(carry, batch_data)\n        preds = hrm_output['y_pred']\n        q_policy = hrm_output['q_policy']\n\n        if q_policy[0] > q_policy[1]:\n            break\n        max_iters -= 1\n\n    submissions[PROPERTIES] = preds\n    \nprint('submission_df', submission_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:42.165415Z","iopub.status.idle":"2025-09-14T21:19:42.165755Z","shell.execute_reply.started":"2025-09-14T21:19:42.165564Z","shell.execute_reply":"2025-09-14T21:19:42.165575Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission_df.to_csv('submission.csv', index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-09-14T21:19:42.166896Z","iopub.status.idle":"2025-09-14T21:19:42.167137Z","shell.execute_reply.started":"2025-09-14T21:19:42.167035Z","shell.execute_reply":"2025-09-14T21:19:42.167044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}