{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05ad503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "696835c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       id                                             SMILES  Tg       FFV  \\\n",
      "0   87817                         *CC(*)c1ccccc1C(=O)OCCCCCC NaN  0.374645   \n",
      "1  106919  *Nc1ccc([C@H](CCC)c2ccc(C3(c4ccc([C@@H](CCC)c5... NaN  0.370410   \n",
      "2  388772  *Oc1ccc(S(=O)(=O)c2ccc(Oc3ccc(C4(c5ccc(Oc6ccc(... NaN  0.378860   \n",
      "3  519416  *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)... NaN  0.387324   \n",
      "4  539187  *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N... NaN  0.355470   \n",
      "\n",
      "         Tc  Density  Rg  \n",
      "0  0.205667      NaN NaN  \n",
      "1       NaN      NaN NaN  \n",
      "2       NaN      NaN NaN  \n",
      "3       NaN      NaN NaN  \n",
      "4       NaN      NaN NaN  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "full_train_data = pd.read_csv(\"../data/train.csv\")\n",
    "print(full_train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2737b1af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import json\n",
    "from typing import List, Tuple, Dict\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torch.optim import Adam\n",
    "\n",
    "from torch_geometric.data import Data, Batch\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "from kmeans_hrm_model import KMeansCarry\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Projektpfade und Trainingshyperparameter\n",
    "PROJECT_ROOT = \"/home/thomaspugh/projects/chem-properties\"\n",
    "DATA_CSV = os.path.join(PROJECT_ROOT, \"data\", \"train.csv\")\n",
    "CHECKPOINT_DIR = os.path.join(PROJECT_ROOT, \"checkpoints\", \"hrm\")\n",
    "os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n",
    "\n",
    "PROPERTIES = [\"Tg\", \"FFV\", \"Tc\", \"Density\", \"Rg\"]\n",
    "TARGET_DIM = len(PROPERTIES)\n",
    "\n",
    "# Trainingseinstellungen\n",
    "EPOCHS = 20\n",
    "BATCH_SIZE = 16\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-4\n",
    "CHECKPOINT_EVERY_N_STEPS = 100\n",
    "K_HEADS = 16  # Size of the KMeansCarry.mask feature dimension (only interface, not used here)\n",
    "\n",
    "# Optional: Limit the number of samples (None = all)\n",
    "MAX_SAMPLES = 5\n",
    "\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fac75a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 5\n",
      "Processing reached 1/5: *CC(*)c1ccccc1C(=O)OCCCCCC (17 atoms)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The SMILES writer does not write stereochemical information\n",
      "The SMILES writer does not write stereochemical information\n",
      "The SMILES writer does not write stereochemical information\n",
      "The SMILES writer does not write stereochemical information\n",
      "The SMILES writer does not write stereochemical information\n",
      "The SMILES writer does not write stereochemical information\n",
      "The SMILES writer does not write stereochemical information\n",
      "The SMILES writer does not write stereochemical information\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing reached 5/5: *Oc1ccc(OC(=O)c2cc(OCCCCCCCCCOCC3CCCN3c3ccc([N+](=O)[O-])cc3)c(C(*)=O)cc2OCCCCCCCCCOCC2CCCN2c2ccc([N+](=O)[O-])cc2)cc1 (70 atoms)\n",
      "\n",
      "Successfully extended: 24 molecules\n",
      "Failed extensions: 0\n",
      "Train graphs: 21 | Val graphs: 3\n",
      "Input dim: 6, Target dim: 5\n",
      "Edge dim: 4\n",
      "\n",
      "Examples of SMILES extensions:\n",
      "Original (17 atoms): *CC(*)c1ccccc1C(=O)OCCCCCC\n",
      "Extended (87 atoms): CCCCCCOC(=O)c1ccccc1C(C)CC(CCC(CC(CC(C)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC)c1ccccc1C(=O)OCCCCCC\n",
      "\n",
      "Original (42 atoms): *Nc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(N*)c(-c3ccc(C)cc3)c2-c2ccc(C)cc2)cc1\n",
      "Extended (128 atoms): CNc1ccc(-c2c(-c3ccc(C)cc3)c(-c3ccc(C)cc3)c(NNc3c(-c4ccc(C)cc4)c(-c4ccc(C)cc4)c(-c4ccc(NNc5ccc(-c6c(-c7ccc(C)cc7)c(-c7ccc(C)cc7)c(NC)c(-c7ccc(C)cc7)c6-c6ccc(C)cc6)cc5)cc4)c(-c4ccc(C)cc4)c3-c3ccc(C)cc3)c(-c3ccc(C)cc3)c2-c2ccc(C)cc2)cc1\n",
      "\n",
      "Original (17 atoms): *CC(*)c1ccccc1C(=O)OCCCCCC\n",
      "Extended (36 atoms): CCCCCCOC(=O)c1ccccc1C(C)CCC(C)c1ccccc1C(=O)OCCCCCC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset_helpers import smiles_iter_to_graph_dataset\n",
    "from data_gen_helpers import iterative_extend_smiles, count_non_hydrogen_atoms\n",
    "from data_gen_helpers import logger as data_gen_logger\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "data_gen_logger.setLevel(logging.DEBUG)\n",
    "\n",
    "logger = logging.getLogger(\"main\")\n",
    "logger.setLevel(logging.DEBUG)\n",
    "handler = logging.FileHandler(f\"main_{datetime.now().strftime('%Y%m%d%H%M%S')}.log\", mode=\"a\")\n",
    "handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(handler)\n",
    "data_gen_logger.addHandler(handler)\n",
    "\n",
    "\n",
    "# Load data\n",
    "VAL_RATIO = 0.1\n",
    "\n",
    "# Load CSV\n",
    "raw_df = pd.read_csv(DATA_CSV)\n",
    "raw_df = raw_df[[\"SMILES\"] + PROPERTIES].dropna(subset=[\"SMILES\"])\n",
    "if MAX_SAMPLES is not None:\n",
    "    raw_df = raw_df.iloc[:MAX_SAMPLES].reset_index(drop=True)\n",
    "\n",
    "print(f\"Original dataset size: {len(raw_df)}\")\n",
    "\n",
    "# Extend SMILES to reach at least 25 atoms (excluding hydrogen)\n",
    "extended_data = []\n",
    "failed_extensions = []\n",
    "\n",
    "for idx, row in raw_df.iterrows():\n",
    "    original_smiles = row[\"SMILES\"]\n",
    "    original_atoms = count_non_hydrogen_atoms(original_smiles)\n",
    "    \n",
    "    if idx % 50 == 0 or idx == len(raw_df) - 1:\n",
    "        print(f\"Processing reached {idx+1}/{len(raw_df)}: {original_smiles} ({original_atoms} atoms)\")\n",
    "    \n",
    "    try:\n",
    "        # Generate extended SMILES with at least 100 atoms (realistic value)\n",
    "        extensions = list(iterative_extend_smiles(\n",
    "            original_smiles, \n",
    "            min_length=100, \n",
    "            max_output=10,  # 10 variants\n",
    "        ))\n",
    "        \n",
    "        if extensions:\n",
    "            # Use the first successful extension\n",
    "            for extended_smiles, _ in extensions:\n",
    "                final_atoms = count_non_hydrogen_atoms(extended_smiles)\n",
    "                \n",
    "                # Create new row with extended SMILES\n",
    "                new_row = row.copy()\n",
    "                new_row[\"SMILES\"] = extended_smiles\n",
    "                new_row[\"original_smiles\"] = original_smiles\n",
    "                new_row[\"original_atoms\"] = original_atoms\n",
    "                new_row[\"final_atoms\"] = final_atoms\n",
    "                extended_data.append(new_row)\n",
    "            logger.debug(f\"  -> Success: {len(extensions)} molecules for {original_smiles} ({final_atoms} atoms)\")\n",
    "        else:\n",
    "            failed_extensions.append((idx, original_smiles, \"No extensions generated\"))\n",
    "            logger.debug(f\"  -> Failed: No extensions generated\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_extensions.append((idx, original_smiles, str(e)))\n",
    "        logger.debug(f\"  -> Failed: {e}\")\n",
    "\n",
    "# Create new DataFrame with extended SMILES\n",
    "extended_df = pd.DataFrame(extended_data)\n",
    "print(f\"\\nSuccessfully extended: {len(extended_df)} molecules\")\n",
    "print(f\"Failed extensions: {len(failed_extensions)}\")\n",
    "\n",
    "if len(failed_extensions) > 0:\n",
    "    logger.debug(\"\\n\\nFailed molecules:\\n\\n\")\n",
    "    for idx, smiles, error in failed_extensions:  # Show first 5 errors\n",
    "        logger.debug(f\"  {idx}: {smiles} - {error}\")\n",
    "\n",
    "# Use extended data for training\n",
    "if len(extended_df) == 0:\n",
    "    raise RuntimeError(\"No molecules could be extended. Check your data and extension logic.\")\n",
    "\n",
    "# Split into Train/Val\n",
    "num_rows = len(extended_df)\n",
    "perm = np.random.RandomState(SEED).permutation(num_rows)\n",
    "train_count = int((1.0 - VAL_RATIO) * num_rows)\n",
    "train_idx, val_idx = perm[:train_count], perm[train_count:]\n",
    "train_df = extended_df.iloc[train_idx].reset_index(drop=True)\n",
    "val_df = extended_df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "# Create graph datasets\n",
    "train_dataset = smiles_iter_to_graph_dataset(train_df[\"SMILES\"], torch.tensor(train_df[PROPERTIES].to_numpy(), dtype=torch.float32, device=device))\n",
    "val_dataset = smiles_iter_to_graph_dataset(val_df[\"SMILES\"], torch.tensor(val_df[PROPERTIES].to_numpy(), dtype=torch.float32, device=device))\n",
    "\n",
    "print(f\"Train graphs: {len(train_dataset)} | Val graphs: {len(val_dataset)}\")\n",
    "\n",
    "# DataListLoader (batched lists of Data objects)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Derive input dimension from first training graph\n",
    "if len(train_dataset) == 0:\n",
    "    raise RuntimeError(\"Training dataset is empty after preprocessing.\")\n",
    "INPUT_DIM = train_dataset[0].x.shape[1]\n",
    "print(f\"Input dim: {INPUT_DIM}, Target dim: {TARGET_DIM}\")\n",
    "\n",
    "EDGE_DIM = train_dataset[0].edge_attr.shape[1]\n",
    "print(f\"Edge dim: {EDGE_DIM}\")\n",
    "\n",
    "# Show some examples of the extensions\n",
    "print(f\"\\nExamples of SMILES extensions:\")\n",
    "for i in range(min(3, len(train_df))):\n",
    "    row = train_df.iloc[i]\n",
    "    print(f\"Original ({row.get('original_atoms', 'N/A')} atoms): {row.get('original_smiles', 'N/A')}\")\n",
    "    print(f\"Extended ({row.get('final_atoms', 'N/A')} atoms): {row['SMILES']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "42fea702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KMeansHRMModule created with 5,430 trainable parameters\n",
      "Model size: 0.02 MB (float32)\n",
      "KMeansHRMModule(\n",
      "  (inner_module): KMeansHRMInnerModule(\n",
      "    (kmeans_module): KMeans(\n",
      "      (heads): ModuleList(\n",
      "        (0-15): 16 x KMeansHead(\n",
      "          (weighting_module): SpectralWeighting(\n",
      "            (cheb_convs): ModuleList(\n",
      "              (0-1): 2 x ChebConv(6, 6, K=3, normalization=sym)\n",
      "            )\n",
      "            (norms): ModuleList(\n",
      "              (0-1): 2 x BatchNorm(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "            )\n",
      "          )\n",
      "          (center_module): DiscreteMeanCenter(\n",
      "            (distance_module): PairwiseDistance()\n",
      "          )\n",
      "          (mask_module): RadiusAttentionWeights(\n",
      "            (weighting_module): GATConv(6, 6, heads=1)\n",
      "            (_mask_linear): Linear(in_features=6, out_features=1, bias=True)\n",
      "          )\n",
      "          (act): ReLU()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (prevgae_attention_layers): Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=16, out_features=96, bias=True)\n",
      "    )\n",
      "    (encode_conv): NNConv(6, 16, aggr=mean, nn=Sequential(\n",
      "      (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Linear(in_features=16, out_features=16, bias=True)\n",
      "      (3): ReLU()\n",
      "      (4): Linear(in_features=16, out_features=96, bias=True)\n",
      "    ))\n",
      "    (linear_post_attention): Sequential(\n",
      "      (0): Linear(in_features=16, out_features=48, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.2, inplace=False)\n",
      "      (3): Linear(in_features=48, out_features=6, bias=True)\n",
      "      (4): LayerNorm((6,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (vgae_encoder): VGAEEncoder(\n",
      "      (convs): ModuleList(\n",
      "        (0-1): 2 x ChebConv(16, 16, K=3, normalization=sym)\n",
      "      )\n",
      "    )\n",
      "    (vgae): VGAE(\n",
      "      (encoder): VGAEEncoder(\n",
      "        (convs): ModuleList(\n",
      "          (0-1): 2 x ChebConv(16, 16, K=3, normalization=sym)\n",
      "        )\n",
      "      )\n",
      "      (decoder): InnerProductDecoder()\n",
      "    )\n",
      "    (norm): LayerNorm(6, affine=True, mode=graph)\n",
      "    (dropout_layer): Dropout(p=0.2, inplace=False)\n",
      "    (output_head): OutputHead(\n",
      "      (linear1): Linear(6, 16, bias=True)\n",
      "      (linear2): Linear(16, 5, bias=True)\n",
      "      (norm): BatchNorm(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "    (policy_module): OutputHead(\n",
      "      (linear1): Linear(6, 8, bias=True)\n",
      "      (linear2): Linear(8, 2, bias=True)\n",
      "      (norm): BatchNorm(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (act): ReLU()\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Use KMeansHRMModule from kmeans_hrm_model.py\n",
    "from kmeans_hrm_model import (\n",
    "    KMeansHRMModule, KMeansHRMConfig, KMeansHRMInnerModuleConfig, KMeansHRMInitialCarry,\n",
    "    KMeansConfig, KMeansHeadConfig, OutputHeadConfig,\n",
    "    SpectralWeighting, SpectralWeightingConfig,\n",
    "    DiscreteMeanCenter, DiscreteMeanCenterConfig,\n",
    "    RadiusAttentionWeights, RadiusMaskConfig\n",
    ")\n",
    "\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Anzahl der trainierbaren Parameter im Modell zählen\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# Konfiguration für KMeansHRM erstellen\n",
    "def create_kmeans_hrm_config(input_dim: int, edge_dim: int, hidden_dim: int = 128, output_dim: int = TARGET_DIM, k: int = K_HEADS) -> KMeansHRMConfig:\n",
    "    \n",
    "    # Spectral Weighting Configuration\n",
    "    spectral_config = SpectralWeightingConfig(\n",
    "        node_channels=input_dim,\n",
    "        K=3,  # Chebyshev polynomial order\n",
    "        num_layers=2,\n",
    "        normalization='sym',\n",
    "        bias=True,\n",
    "        dropout=0.2,\n",
    "        norm='batch',\n",
    "        norm_kwargs={'in_channels': input_dim}\n",
    "    )\n",
    "    \n",
    "    # Center Module Configuration\n",
    "    center_config = DiscreteMeanCenterConfig(\n",
    "        distance_metric='euclidean'\n",
    "    )\n",
    "    \n",
    "    # Radius Mask Configuration (simplified weighting module)\n",
    "    radius_weighting = GATConv(input_dim, input_dim)\n",
    "    radius_config = RadiusMaskConfig(\n",
    "        max_num_neighbors=25,\n",
    "        radius=20,\n",
    "        weighting_module=radius_weighting,\n",
    "        threshold=0.1,\n",
    "        node_dim=input_dim\n",
    "    )\n",
    "    \n",
    "    # KMeans Head Configuration\n",
    "    kmeans_head_config = KMeansHeadConfig(\n",
    "        node_count=k,\n",
    "        node_dim=input_dim,\n",
    "        max_nodes=100,  # Maximale Anzahl Knoten pro Graph\n",
    "        num_layers=5,\n",
    "        dropout=0.2,\n",
    "        weighting_module=SpectralWeighting(spectral_config),\n",
    "        center_module=DiscreteMeanCenter(center_config),\n",
    "        mask_module=RadiusAttentionWeights(radius_config),\n",
    "        act='relu',\n",
    "        act_kwargs={}\n",
    "    )\n",
    "    \n",
    "    # KMeans Configuration\n",
    "    kmeans_config = KMeansConfig(\n",
    "        k=k,\n",
    "        max_iter=10,\n",
    "        thresh=1e-6,\n",
    "        max_overlap=2,\n",
    "        head_module=kmeans_head_config,\n",
    "        excluded_is_cluster=True\n",
    "    )\n",
    "    \n",
    "    # Output Head Configuration\n",
    "    output_head_config = OutputHeadConfig(\n",
    "        node_dim=input_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        output_dim=output_dim,\n",
    "        pooling_type='mean',\n",
    "        norm='batch',\n",
    "        norm_kwargs={'in_channels': hidden_dim},\n",
    "        act='relu',\n",
    "        act_kwargs={}\n",
    "    )\n",
    "    \n",
    "    # Policy Module Configuration (für Halt-Entscheidungen)\n",
    "    policy_config = OutputHeadConfig(\n",
    "        node_dim=input_dim,\n",
    "        hidden_dim=hidden_dim//2,\n",
    "        output_dim=2,  # halt=0, continue=1\n",
    "        pooling_type='mean',\n",
    "        norm='batch',\n",
    "        norm_kwargs={'in_channels': hidden_dim//2},\n",
    "        act='relu',\n",
    "        act_kwargs={}\n",
    "    )\n",
    "    \n",
    "    # Inner Module Configuration\n",
    "    inner_config = KMeansHRMInnerModuleConfig(\n",
    "        add_self_loops=True,\n",
    "        dropout=0.2,\n",
    "        hidden_dim=16,                # Increase in final\n",
    "        node_dim=input_dim,\n",
    "        edge_dim=edge_dim,\n",
    "        layers=3,\n",
    "        kmeans_config=kmeans_config,\n",
    "        output_head_config=output_head_config,\n",
    "        policy_module_config=policy_config,\n",
    "        add_negative_edges=False,\n",
    "        K_cycles=2,\n",
    "        L_cycles=2,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        halt_max_steps=5,\n",
    "        halt_exploration_prob=0.1,\n",
    "        vgae_encoder_type='cheb',\n",
    "        vgae_latent_dim=16,        # Increase in final\n",
    "        vgae_encoder_layers=2,\n",
    "        vgae_encoder_dropout=0.1,\n",
    "        vgae_decoder_type=None,\n",
    "        vgae_kl_weight=1.0\n",
    "    )\n",
    "    \n",
    "    # Hauptkonfiguration\n",
    "    config = KMeansHRMConfig(\n",
    "        inner_module=inner_config,\n",
    "        explore_steps_prob=0.1,\n",
    "        halt_max_steps=5\n",
    "    )\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Modell initialisieren\n",
    "hrm_config = create_kmeans_hrm_config(INPUT_DIM, EDGE_DIM, hidden_dim=16, output_dim=TARGET_DIM)\n",
    "model = KMeansHRMModule(hrm_config, training=True).to(device)\n",
    "optimizer = Adam(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "# Print number of parameters\n",
    "num_params = count_parameters(model)\n",
    "print(f\"KMeansHRMModule created with {num_params:,} trainable parameters\")\n",
    "print(f\"Model size: {num_params * 4 / 1024 / 1024:.2f} MB (float32)\")\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "15e7e6e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss and metrics initialized.\n"
     ]
    }
   ],
   "source": [
    "# Loss: Range-violation for missing, otherwise MSE\n",
    "# Note: HRM-repo inspiration (loss splitting, early checkpoints), see links below\n",
    "# - losses.py and pretrain.py from HRM (only as idea source)\n",
    "\n",
    "# Automatically compute PROPERTY_BOUNDS from the data in extended_df\n",
    "# The bounds are determined for each property as a (min, max) tuple\n",
    "PROPERTY_BOUNDS: Dict[str, Tuple[float, float]] = {}\n",
    "for prop in PROPERTIES:\n",
    "    values = extended_df[prop].dropna()\n",
    "    if len(values) > 0:\n",
    "        min_val = float(values.min())\n",
    "        max_val = float(values.max())\n",
    "        PROPERTY_BOUNDS[prop] = (min_val, max_val)\n",
    "    else:\n",
    "        # Fallback to a default range if no values are present\n",
    "        PROPERTY_BOUNDS[prop] = (0.0, 1.0)\n",
    "\n",
    "bounds_tensor = torch.tensor([PROPERTY_BOUNDS[p] for p in PROPERTIES], dtype=torch.float32, device=device)  # [5,2]\n",
    "\n",
    "\n",
    "def range_violation_loss(pred: torch.Tensor, prop_idx: int) -> torch.Tensor:\n",
    "    lo, hi = bounds_tensor[prop_idx, 0], bounds_tensor[prop_idx, 1]\n",
    "    below = F.relu(lo - pred)\n",
    "    above = F.relu(pred - hi)\n",
    "    return torch.pow(below + above, 2)\n",
    "\n",
    "\n",
    "def composite_loss(preds: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    # preds: [B, 5], targets: [B, 5] with NaNs if missing\n",
    "    loss_items = []\n",
    "    for j in range(TARGET_DIM):\n",
    "        t = targets[:, j]\n",
    "        p = preds[:, j]\n",
    "        print(f\"p: {p}\")\n",
    "        print(f\"t: {t}\")\n",
    "        mask_present = torch.isfinite(t)\n",
    "        if mask_present.any():\n",
    "            # MSE for available labels\n",
    "            mse = F.mse_loss(p[mask_present], t[mask_present])\n",
    "            loss_items.append(mse)\n",
    "        # Range-violation for all (including available): no loss if within bounds\n",
    "        rv = range_violation_loss(p, j).mean()\n",
    "        loss_items.append(rv)\n",
    "    return torch.stack(loss_items).mean()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_mae_in_bounds(preds: torch.Tensor, targets: torch.Tensor) -> Dict[str, float]:\n",
    "    out = {}\n",
    "    for j, name in enumerate(PROPERTIES):\n",
    "        t = targets[:, j]\n",
    "        p = preds[:, j]\n",
    "        mask_present = torch.isfinite(t)\n",
    "        if mask_present.any():\n",
    "            out[f\"mae_{name}\"] = (p[mask_present] - t[mask_present]).abs().mean().item()\n",
    "        else:\n",
    "            out[f\"mae_{name}\"] = float(\"nan\")\n",
    "    return out\n",
    "\n",
    "print(\"Loss and metrics initialized.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "d87fb36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_data: DataBatch(x=[1810, 6], edge_index=[2, 3936], edge_attr=[3936, 4], y=[80], batch=[1810], ptr=[17])\n",
      "unique batches: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "pooled: torch.Size([16, 6])\n",
      "unique batches: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15])\n",
      "pooled: torch.Size([16, 6])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "_unique2(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 102\u001b[39m\n\u001b[32m     99\u001b[39m best_val_loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33minf\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    101\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, EPOCHS + \u001b[32m1\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m102\u001b[39m     train_stats = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m     val_stats = validate(epoch, model, val_loader)\n\u001b[32m    105\u001b[39m     history[\u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m].append({\u001b[33m\"\u001b[39m\u001b[33mepoch\u001b[39m\u001b[33m\"\u001b[39m: epoch, **train_stats})\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(epoch, model, loader, optimizer)\u001b[39m\n\u001b[32m     25\u001b[39m carry = model.initial_carry(batch_data)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Korrigiert: forward Methode braucht carry und batch_data als Parameter\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m new_carry, hrm_output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcarry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m preds = hrm_output[\u001b[33m'\u001b[39m\u001b[33my_pred\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mpred shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreds.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/src/kmeans_hrm_model.py:22\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, carry, data)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Batch, Data\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ChebConv,\n\u001b[32m     11\u001b[39m     GATConv,\n\u001b[32m     12\u001b[39m     GCNConv,\n\u001b[32m     13\u001b[39m     GlobalAttention,\n\u001b[32m     14\u001b[39m     InnerProductDecoder,\n\u001b[32m     15\u001b[39m     LayerNorm,\n\u001b[32m     16\u001b[39m     NNConv,\n\u001b[32m     17\u001b[39m     MessagePassing,\n\u001b[32m     18\u001b[39m     VGAE,\n\u001b[32m     19\u001b[39m     global_add_pool,\n\u001b[32m     20\u001b[39m     global_max_pool,\n\u001b[32m     21\u001b[39m     global_mean_pool,\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     Linear\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m radius\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresolver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activation_resolver, normalization_resolver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/src/kmeans_hrm_model.py:15\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, kmeans_carry, batch)\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Batch, Data\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     10\u001b[39m     ChebConv,\n\u001b[32m     11\u001b[39m     GATConv,\n\u001b[32m     12\u001b[39m     GCNConv,\n\u001b[32m     13\u001b[39m     GlobalAttention,\n\u001b[32m     14\u001b[39m     InnerProductDecoder,\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     LayerNorm,\n\u001b[32m     16\u001b[39m     NNConv,\n\u001b[32m     17\u001b[39m     MessagePassing,\n\u001b[32m     18\u001b[39m     VGAE,\n\u001b[32m     19\u001b[39m     global_add_pool,\n\u001b[32m     20\u001b[39m     global_max_pool,\n\u001b[32m     21\u001b[39m     global_mean_pool,\n\u001b[32m     22\u001b[39m     Linear\n\u001b[32m     23\u001b[39m )\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpool\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m radius\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch_geometric\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mresolver\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activation_resolver, normalization_resolver\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/_jit_internal.py:627\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/_jit_internal.py:627\u001b[39m, in \u001b[36mboolean_dispatch.<locals>.fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    625\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(*args, **kwargs)\n\u001b[32m    626\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m627\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mif_false\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/functional.py:1102\u001b[39m, in \u001b[36m_return_output\u001b[39m\u001b[34m(input, sorted, return_inverse, return_counts, dim)\u001b[39m\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28minput\u001b[39m):\n\u001b[32m   1100\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _unique_impl(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28msorted\u001b[39m, return_inverse, return_counts, dim)\n\u001b[32m-> \u001b[39m\u001b[32m1102\u001b[39m output, _, _ = \u001b[43m_unique_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/functional.py:995\u001b[39m, in \u001b[36m_unique_impl\u001b[39m\u001b[34m(input, sorted, return_inverse, return_counts, dim)\u001b[39m\n\u001b[32m    987\u001b[39m     output, inverse_indices, counts = _VF.unique_dim(\n\u001b[32m    988\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m    989\u001b[39m         dim,\n\u001b[32m   (...)\u001b[39m\u001b[32m    992\u001b[39m         return_counts=return_counts,\n\u001b[32m    993\u001b[39m     )\n\u001b[32m    994\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m995\u001b[39m     output, inverse_indices, counts = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_unique2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    996\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    997\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    998\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_inverse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    999\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_counts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1000\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output, inverse_indices, counts\n",
      "\u001b[31mTypeError\u001b[39m: _unique2(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "# Trainings- und Validierungsschleifen mit Checkpoints\n",
    "\n",
    "def save_checkpoint(state: Dict, step: int, is_best: bool = False):\n",
    "    path = os.path.join(CHECKPOINT_DIR, f\"step_{step}.pt\")\n",
    "    torch.save(state, path)\n",
    "    if is_best:\n",
    "        best_path = os.path.join(CHECKPOINT_DIR, \"best.pt\")\n",
    "        torch.save(state, best_path)\n",
    "\n",
    "\n",
    "def train_one_epoch(epoch: int, model: nn.Module, loader: DataLoader, optimizer: torch.optim.Optimizer) -> Dict[str, float]:\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    mae_accum = {f\"mae_{p}\": 0.0 for p in PROPERTIES}\n",
    "    count_samples = 0\n",
    "\n",
    "    step = 0\n",
    "    for batch_data in loader:\n",
    "        # Targets als Tensor [B,5]\n",
    "        print(f\"batch_data: {batch_data}\")\n",
    "        y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n",
    "            \n",
    "        batch_data = batch_data.to(device)\n",
    "\n",
    "        carry = model.initial_carry(batch_data)\n",
    "\n",
    "        # Korrigiert: forward Methode braucht carry und batch_data als Parameter\n",
    "        new_carry, hrm_output = model(carry, batch_data)\n",
    "        \n",
    "        preds = hrm_output['y_pred']\n",
    "        print(f\"pred shape: {preds.shape}\")\n",
    "        loss = composite_loss(preds, y)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            metrics = compute_mae_in_bounds(preds, y)\n",
    "            for k, v in metrics.items():\n",
    "                if not math.isnan(v):\n",
    "                    mae_accum[k] += v * preds.size(0)\n",
    "            running_loss += loss.item() * preds.size(0)\n",
    "            count_samples += preds.size(0)\n",
    "\n",
    "        step += 1\n",
    "        global_step = epoch * len(loader) + step\n",
    "        if global_step % CHECKPOINT_EVERY_N_STEPS == 0:\n",
    "            save_checkpoint({\n",
    "                \"epoch\": epoch,\n",
    "                \"global_step\": global_step,\n",
    "                \"model_state\": model.state_dict(),\n",
    "                \"optimizer_state\": optimizer.state_dict(),\n",
    "            }, step=global_step)\n",
    "\n",
    "    avg_loss = running_loss / max(1, count_samples)\n",
    "    avg_mae = {k: (v / max(1, count_samples)) for k, v in mae_accum.items()}\n",
    "    return {\"loss\": avg_loss, **avg_mae}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate(epoch: int, model: nn.Module, loader: DataLoader) -> Dict[str, float]:\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    mae_accum = {f\"mae_{p}\": 0.0 for p in PROPERTIES}\n",
    "    count_samples = 0\n",
    "\n",
    "    for batch_data in loader:\n",
    "        # Korrigiert: Konsistente Batch-Verarbeitung wie in train_one_epoch\n",
    "        y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n",
    "        \n",
    "        batch_data = batch_data.to(device)\n",
    "        \n",
    "        # Korrigiert: initial_carry braucht nur batch_data als Parameter\n",
    "        carry = model.initial_carry(batch_data)\n",
    "        \n",
    "        # Korrigiert: forward Methode braucht carry und batch_data als Parameter\n",
    "        new_carry, hrm_output = model(carry, batch_data)\n",
    "        \n",
    "        # Extrahiere Vorhersagen aus HRM-Output\n",
    "        preds = hrm_output['y_pred']\n",
    "\n",
    "        loss = composite_loss(preds, y)\n",
    "\n",
    "        metrics = compute_mae_in_bounds(preds, y)\n",
    "        for k, v in metrics.items():\n",
    "            if not math.isnan(v):\n",
    "                mae_accum[k] += v * preds.size(0)\n",
    "        running_loss += loss.item() * preds.size(0)\n",
    "        count_samples += preds.size(0)\n",
    "\n",
    "    avg_loss = running_loss / max(1, count_samples)\n",
    "    avg_mae = {k: (v / max(1, count_samples)) for k, v in mae_accum.items()}\n",
    "    return {\"loss\": avg_loss, **avg_mae}\n",
    "\n",
    "\n",
    "history = {\"train\": [], \"val\": []}\n",
    "best_val_loss = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train_stats = train_one_epoch(epoch, model, train_loader, optimizer)\n",
    "    val_stats = validate(epoch, model, val_loader)\n",
    "\n",
    "    history[\"train\"].append({\"epoch\": epoch, **train_stats})\n",
    "    history[\"val\"].append({\"epoch\": epoch, **val_stats})\n",
    "\n",
    "    is_best = val_stats[\"loss\"] < best_val_loss\n",
    "    if is_best:\n",
    "        best_val_loss = val_stats[\"loss\"]\n",
    "\n",
    "    save_checkpoint({\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"optimizer_state\": optimizer.state_dict(),\n",
    "        \"train_stats\": train_stats,\n",
    "        \"val_stats\": val_stats,\n",
    "    }, step=epoch, is_best=is_best)\n",
    "\n",
    "    print(f\"Epoch {epoch:03d} | train_loss={train_stats['loss']:.4f} | val_loss={val_stats['loss']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9ae24e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4048, 4])\n",
      "16\n",
      "torch.Size([16, 1841, 6])\n",
      "tensor([[2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 4., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        ...,\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.]])\n",
      "KMeansHRMInitialCarry(inner_carry=KMeansCarry(x=[1841, 6], edge_index=[2, 4048], mask=[1841, 16], none_selected=[1841], batch=[1841]), steps=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), halted=tensor([False, False, False, False, False, False, False, False, False, False,\n",
      "        False, False, False, False, False, False]), current_data=DataBatch(x=[1841, 6], edge_index=[2, 4048], edge_attr=[4048, 4], y=[80], batch=[1841], ptr=[17]))\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch_data = next(iter(train_loader))\n",
    "y = torch.stack([g.y for g in batch_data.to_data_list()], dim=0).to(device)\n",
    "batch_data = batch_data.to(device)\n",
    "\n",
    "# Korrigiert: initial_carry braucht nur batch_data als Parameter\n",
    "carry = model.initial_carry(batch_data)\n",
    "print(batch_data.edge_attr.shape)\n",
    "print(model.inner_module.k)\n",
    "print(carry.inner_carry.masked_x().shape)\n",
    "print(carry.inner_carry.masked_x(1))\n",
    "print(carry)\n",
    "print(EDGE_DIM)\n",
    "\n",
    "new_carry, hrm_output = model(carry, batch_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3b0f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metriken speichern und Lernkurven plotten\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Speichern\n",
    "metrics_path = os.path.join(CHECKPOINT_DIR, \"history.json\")\n",
    "with open(metrics_path, \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "print(f\"Saved metrics to {metrics_path}\")\n",
    "\n",
    "# Plot\n",
    "def plot_curves(history):\n",
    "    epochs = [e[\"epoch\"] for e in history[\"train\"]]\n",
    "    train_losses = [e[\"loss\"] for e in history[\"train\"]]\n",
    "    val_losses = [e[\"loss\"] for e in history[\"val\"]]\n",
    "\n",
    "    plt.figure(figsize=(8,4))\n",
    "    plt.plot(epochs, train_losses, label=\"train_loss\")\n",
    "    plt.plot(epochs, val_losses, label=\"val_loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_curves(history)\n",
    "\n",
    "# Hinweis auf Quellen (Inspiration):\n",
    "# - HRM Losses und Training (nur als Anregung, nicht 1:1):\n",
    "#   https://github.com/sapientinc/HRM/blob/05dd4ef795a98c20110e380a330d0b3ec159a46b/models/losses.py\n",
    "#   https://github.com/sapientinc/HRM/blob/05dd4ef795a98c20110e380a330d0b3ec159a46b/pretrain.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433464f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
