{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 05 – NN-Energieberechnung (ANI2x) für Polymerfragmente\n",
        "\n",
        "Dieses Notebook nutzt ein neuronales Netz (TorchANI, z. B. ANI2x) für Single-Point-Energien von aus Monomeren aufgebauten Fragmenten. Die Polymer-Konfigurationen werden mit `iterative_extend_smiles` erzeugt und die Berechnungen parallel (ähnlich zu `opt_helpers.py`) ausgeführt.\n",
        "\n",
        "Referenz: Sammlung relevanter NN-Modelle in der Chemie: [Neural-Network-Models-for-Chemistry](https://github.com/Eipgen/Neural-Network-Models-for-Chemistry)\n",
        "\n",
        "Hinweise:\n",
        "- Es wird TorchANI verwendet; Installation erfolgt unten im Notebook falls erforderlich."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K\u001b[37m⠧\u001b[0m \u001b[2mtyping-extensions==4.15.0                                                     \u001b[0m\u001b[37m⠋\u001b[0m \u001b[2mResolving dependencies...                                                     \u001b[0m\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: Failed to fetch: `https://pypi.org/simple/aiohttp/`\n",
            "  \u001b[1m\u001b[31mCaused by\u001b[39m\u001b[0m: Request failed after 3 retries\n",
            "  \u001b[1m\u001b[31mCaused by\u001b[39m\u001b[0m: error sending request for url (https://pypi.org/simple/aiohttp/)\n",
            "  \u001b[1m\u001b[31mCaused by\u001b[39m\u001b[0m: operation timed out\n",
            "\u001b[2mResolved \u001b[1m109 packages\u001b[0m \u001b[2min 0.84ms\u001b[0m\u001b[0m\n",
            "\u001b[1m\u001b[31merror\u001b[39m\u001b[0m: Distribution `\u001b[36mgpu4pyscf-cuda12x==1.4.3 @ registry+https://pypi.org/simple\u001b[39m` can't be installed because it doesn't have a source distribution or wheel for the current platform\n",
            "\n",
            "\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m You're on \u001b[36mLinux\u001b[39m (`\u001b[36mmanylinux_2_36_aarch64\u001b[39m`), but `\u001b[36mgpu4pyscf-cuda12x\u001b[39m` (\u001b[36mv1.4.3\u001b[39m) only has wheels for the following platforms: `\u001b[36mmanylinux_2_17_x86_64\u001b[39m`, `\u001b[36mmanylinux2014_x86_64\u001b[39m`; consider adding your platform to `\u001b[32mtool.uv.required-environments\u001b[39m` to ensure uv resolves to a version with compatible wheels\n"
          ]
        }
      ],
      "source": [
        "import importlib\n",
        "import os\n",
        "import shutil\n",
        "import sys\n",
        "\n",
        "# Install TorchANI using uv if missing\n",
        "try:\n",
        "    import torchani  # noqa: F401\n",
        "    print(\"TorchANI already installed.\")\n",
        "except Exception:\n",
        "    print(\"Installing torchani via uv ...\")\n",
        "    os.system(\"uv pip install --system torchani | cat\")\n",
        "    importlib.invalidate_caches()\n",
        "    try:\n",
        "        import torchani  # noqa: F401\n",
        "        print(\"TorchANI ready.\")\n",
        "    except Exception as e:\n",
        "        raise RuntimeError(f\"TorchANI installation failed: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Installing torchani ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/thomaspugh/projects/chem-properties/.venv/bin/python: No module named pip\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'torchani'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchani\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTorchANI already installed.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchani'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\n\u001b[32m     12\u001b[39m importlib.invalidate_caches()\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchani\u001b[39;00m  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTorchANI installation attempted.\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchani'"
          ]
        }
      ],
      "source": [
        "# (ersetzt durch uv-Installation in der vorherigen Zelle)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "xtb binary: None\n"
          ]
        },
        {
          "ename": "OSError",
          "evalue": "xtb was not found. Please install and make it available in PATH. See docs: https://xtb-docs.readthedocs.io/en/latest/",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mxtb binary:\u001b[39m\u001b[33m\"\u001b[39m, XTB_BIN)\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m XTB_BIN \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mxtb was not found. Please install and make it available in PATH. See docs: https://xtb-docs.readthedocs.io/en/latest/\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[31mOSError\u001b[39m: xtb was not found. Please install and make it available in PATH. See docs: https://xtb-docs.readthedocs.io/en/latest/"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import List, Tuple, Optional\n",
        "\n",
        "import torch\n",
        "import torchani\n",
        "import pandas as pd\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import rdDistGeom\n",
        "\n",
        "from data_gen_helpers import iterative_extend_smiles\n",
        "\n",
        "# lade ANI2x\n",
        "device = torch.device(\"cpu\")\n",
        "ani_model = torchani.models.ANI2x().to(device)\n",
        "ani_model.eval()\n",
        "print(\"Loaded ANI2x on\", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_asterisk(smiles: str) -> str:\n",
        "    return smiles.replace('*', '').replace('()', '')\n",
        "\n",
        "\n",
        "def smiles_to_species_coords(smiles: str, add_hydrogen: bool = True, max_embed_attempts: int = 10):\n",
        "    mol = Chem.MolFromSmiles(remove_asterisk(smiles))\n",
        "    if mol is None:\n",
        "        raise RuntimeError(\"Molecule failed to generate from smiles\")\n",
        "\n",
        "    if add_hydrogen:\n",
        "        mol = Chem.AddHs(mol)\n",
        "\n",
        "    success = False\n",
        "    for i in range(max_embed_attempts):\n",
        "        params = rdDistGeom.ETKDGv3()\n",
        "        params.randomSeed = i * 777\n",
        "        params.useRandomCoords = True\n",
        "        params.maxAttempts = 1000\n",
        "        code = rdDistGeom.EmbedMolecule(mol, params)\n",
        "        if code == 0:\n",
        "            success = True\n",
        "            break\n",
        "\n",
        "    if not success:\n",
        "        for i in range(max_embed_attempts):\n",
        "            code = rdDistGeom.EmbedMolecule(mol, randomSeed=i * 17)\n",
        "            if code == 0:\n",
        "                success = True\n",
        "                break\n",
        "\n",
        "    if not success or mol.GetNumConformers() == 0:\n",
        "        raise RuntimeError(\"Failed to embed molecule geometry\")\n",
        "\n",
        "    conf = mol.GetConformer()\n",
        "    species = []\n",
        "    coords = []\n",
        "    for atom in mol.GetAtoms():\n",
        "        species.append(atom.GetSymbol())\n",
        "        pos = conf.GetAtomPosition(atom.GetIdx())\n",
        "        coords.append([pos.x, pos.y, pos.z])\n",
        "\n",
        "    import numpy as np\n",
        "    species = ''.join(species)\n",
        "    coords = np.array([coords])\n",
        "    return species, coords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def run_ani_singlepoint(species: str, coords) -> float:\n",
        "    \"\"\"Berechnet die Energie (Hartree) mit ANI2x bei gegebenen Species und Koordinaten.\"\"\"\n",
        "    # TorchANI expects species tensor and coordinates tensor\n",
        "    converter = torchani.utils.ChemicalSymbolsToInts(ani_model.aev_computer.species)\n",
        "    species_tensor = torch.tensor([converter(list(species))], device=device)\n",
        "    coordinates = torch.tensor(coords, dtype=torch.float32, device=device)\n",
        "    energy = ani_model((species_tensor, coordinates)).energies\n",
        "    # energies shape: (batch,)\n",
        "    return float(energy.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tempfile\n",
        "from pathlib import Path\n",
        "\n",
        "def run_xtb_singlepoint_xyz(xyz_block: str, charge: int = 0, uhf: int = 0, level: str = \"GFN2-xTB\", solvation: Optional[str] = None) -> Tuple[Optional[float], str]:\n",
        "    \"\"\"\n",
        "    Runs an xTB single-point calculation for an XYZ (without header lines) and returns (energy in Eh, raw log).\n",
        "    level: e.g. \"GFN2-xTB\", \"GFN1-xTB\" – mapped to --gfn 2/1.\n",
        "    \"\"\"\n",
        "    if XTB_BIN is None:\n",
        "        raise EnvironmentError(\"xtb binary not found\")\n",
        "\n",
        "    level_map = {\"GFN2-xTB\": \"2\", \"GFN1-xTB\": \"1\"}\n",
        "    gfn = level_map.get(level, \"2\")\n",
        "\n",
        "    with tempfile.TemporaryDirectory() as tmpd:\n",
        "        tmp = Path(tmpd)\n",
        "        xyz_path = tmp / \"mol.xyz\"\n",
        "        # Write with header lines (number of atoms, comment) + coordinates\n",
        "        coords = xyz_block.strip().splitlines()\n",
        "        with xyz_path.open(\"w\") as f:\n",
        "            f.write(str(len(coords)) + \"\\n\")\n",
        "            f.write(\"generated by 05-xtb-calc-test\\n\")\n",
        "            f.write(\"\\n\".join(coords) + \"\\n\")\n",
        "\n",
        "        cmd = [XTB_BIN, str(xyz_path), \"--gfn\", gfn, \"--uhf\", str(uhf), \"--charge\", str(charge)]\n",
        "        if solvation:\n",
        "            cmd += [\"--alpb\", solvation]\n",
        "\n",
        "        try:\n",
        "            proc = subprocess.run(cmd, cwd=tmp, capture_output=True, text=True, check=False)\n",
        "            out = proc.stdout + \"\\n\" + proc.stderr\n",
        "        except Exception as e:\n",
        "            return None, f\"xTB call failed: {e}\"\n",
        "\n",
        "        energy = None\n",
        "        # Search for total energy line in Eh\n",
        "        # Example output (see xTB docs): \"TOTAL ENERGY       -40.123456 Eh\"\n",
        "        for line in out.splitlines():\n",
        "            ls = line.strip().lower()\n",
        "            if \"total energy\" in ls and \"eh\" in ls:\n",
        "                # robust extraction\n",
        "                parts = line.replace(\"=\", \" \").replace(\":\", \" \").split()\n",
        "                vals = []\n",
        "                for i, p in enumerate(parts):\n",
        "                    if p.lower() in (\"eh\", \"hartree\") and i > 0:\n",
        "                        try:\n",
        "                            vals.append(float(parts[i-1]))\n",
        "                        except:\n",
        "                            pass\n",
        "                if vals:\n",
        "                    energy = vals[-1]\n",
        "                    break\n",
        "        return energy, out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from multiprocessing import Pool, cpu_count, get_context\n",
        "import time\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(processName)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "def process_single_polymer_xtb(args) -> Tuple[str, str, Optional[float], str, float, int]:\n",
        "    monomer_smiles, polymer_smiles, level, monomer_count = args\n",
        "    start = time.time()\n",
        "    try:\n",
        "        xyz = smiles_to_xyz(polymer_smiles, add_hydrogen=True)\n",
        "        energy, raw = run_xtb_singlepoint_xyz(xyz, charge=0, uhf=0, level=level)\n",
        "        dur = time.time() - start\n",
        "        return monomer_smiles, polymer_smiles, energy, level, dur, monomer_count\n",
        "    except Exception as e:\n",
        "        dur = time.time() - start\n",
        "        logging.exception(f\"Fehler f\u0013r {polymer_smiles}: {e}\")\n",
        "        return monomer_smiles, polymer_smiles, None, level+\"_error\", dur, monomer_count\n",
        "\n",
        "\n",
        "def calculate_polymer_energies_xtb(monomer_smiles_list: List[str],\n",
        "                                  max_chain_length: int = 60,\n",
        "                                  level: str = \"GFN2-xTB\",\n",
        "                                  max_polymers_per_monomer: Optional[int] = None,\n",
        "                                  n_processes: Optional[int] = None) -> pd.DataFrame:\n",
        "    if n_processes is None:\n",
        "        n_processes = min(4, cpu_count())\n",
        "\n",
        "    tasks = []\n",
        "    for monomer in monomer_smiles_list:\n",
        "        pairs = list(iterative_extend_smiles(monomer, max_chain_length, max_polymers_per_monomer))\n",
        "        for polymer_smiles, monomer_count in pairs:\n",
        "            tasks.append((monomer, polymer_smiles, level, monomer_count))\n",
        "\n",
        "    results = []\n",
        "    if not tasks:\n",
        "        return pd.DataFrame(columns=[\"monomer_smiles\", \"polymer_smiles\", \"zero_energy\", \"method\", \"calc_time\", \"monomer_count\"])    \n",
        "\n",
        "    # Spawn-Kontext wie in opt_helpers\n",
        "    with get_context(\"spawn\").Pool(n_processes) as pool:\n",
        "        for out in pool.imap_unordered(process_single_polymer_xtb, tasks):\n",
        "            results.append(out)\n",
        "\n",
        "    df = pd.DataFrame(results, columns=[\n",
        "        'monomer_smiles', 'polymer_smiles', 'zero_energy', 'method', 'calc_time', 'monomer_count'\n",
        "    ])\n",
        "    df = df.dropna(subset=['zero_energy'])\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from multiprocessing import cpu_count, get_context\n",
        "import time\n",
        "import logging\n",
        "\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(processName)s - %(levelname)s - %(message)s')\n",
        "\n",
        "\n",
        "def process_single_polymer_ani(args) -> Tuple[str, str, Optional[float], str, float, int]:\n",
        "    monomer_smiles, polymer_smiles, level, monomer_count = args\n",
        "    start = time.time()\n",
        "    try:\n",
        "        species, coords = smiles_to_species_coords(polymer_smiles, add_hydrogen=True)\n",
        "        energy = run_ani_singlepoint(species, coords)\n",
        "        dur = time.time() - start\n",
        "        return monomer_smiles, polymer_smiles, energy, level, dur, monomer_count\n",
        "    except Exception as e:\n",
        "        dur = time.time() - start\n",
        "        logging.exception(f\"Fehler für {polymer_smiles}: {e}\")\n",
        "        return monomer_smiles, polymer_smiles, None, level+\"_error\", dur, monomer_count\n",
        "\n",
        "\n",
        "def calculate_polymer_energies_ani(monomer_smiles_list: List[str],\n",
        "                                  max_chain_length: int = 60,\n",
        "                                  level: str = \"ANI2x\",\n",
        "                                  max_polymers_per_monomer: Optional[int] = None,\n",
        "                                  n_processes: Optional[int] = None) -> pd.DataFrame:\n",
        "    if n_processes is None:\n",
        "        n_processes = min(4, cpu_count())\n",
        "\n",
        "    tasks = []\n",
        "    for monomer in monomer_smiles_list:\n",
        "        pairs = list(iterative_extend_smiles(monomer, max_chain_length, max_polymers_per_monomer))\n",
        "        for polymer_smiles, monomer_count in pairs:\n",
        "            tasks.append((monomer, polymer_smiles, level, monomer_count))\n",
        "\n",
        "    results = []\n",
        "    if not tasks:\n",
        "        return pd.DataFrame(columns=[\"monomer_smiles\", \"polymer_smiles\", \"zero_energy\", \"method\", \"calc_time\", \"monomer_count\"])    \n",
        "\n",
        "    with get_context(\"spawn\").Pool(n_processes) as pool:\n",
        "        for out in pool.imap_unordered(process_single_polymer_ani, tasks):\n",
        "            results.append(out)\n",
        "\n",
        "    df = pd.DataFrame(results, columns=[\n",
        "        'monomer_smiles', 'polymer_smiles', 'zero_energy', 'method', 'calc_time', 'monomer_count'\n",
        "    ])\n",
        "    df = df.dropna(subset=['zero_energy'])\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Beispiel-Run: kleine Menge Monomere (ANI2x)\n",
        "\n",
        "example_monomers = [\n",
        "    \"*C=C*\",       # Ethylen-Fragment\n",
        "    \"*c1ccccc1*\", # Phenyl-Fragment\n",
        "]\n",
        "\n",
        "res_df = calculate_polymer_energies_ani(\n",
        "    example_monomers,\n",
        "    max_chain_length=20,\n",
        "    level=\"ANI2x\",\n",
        "    max_polymers_per_monomer=5,\n",
        "    n_processes=min(2, cpu_count())\n",
        ")\n",
        "\n",
        "res_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Beispiel-Run: kleine Menge Monomere\n",
        "\n",
        "example_monomers = [\n",
        "    \"*C=C*\",       # Ethylen-Fragment\n",
        "    \"*c1ccccc1*\", # Phenyl-Fragment\n",
        "]\n",
        "\n",
        "# Vorsicht: max_chain_length klein halten f\u0013r schnellen Test\n",
        "res_df = calculate_polymer_energies_xtb(\n",
        "    example_monomers,\n",
        "    max_chain_length=20,\n",
        "    level=\"GFN2-xTB\",\n",
        "    max_polymers_per_monomer=5,\n",
        "    n_processes=min(2, cpu_count())\n",
        ")\n",
        "\n",
        "res_df.head()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
