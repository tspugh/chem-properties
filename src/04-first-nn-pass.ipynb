{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05ad503c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "696835c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAcIAAACWCAIAAADCEh9HAAAABmJLR0QA/wD/AP+gvaeTAAAYWElEQVR4nO3de1BU5/kH8GeXXZCFeEHCzQsG8cYtampJFCVaah2rTaKu1lS0tnH9tVbSZjLdZnLZpNPWHUcDGadNF23GjanaTUxbxIwNalUwmDQqgqKCAorcUUTuy+6+vz9esxIUWPZyzln5fv5wsufAngfM+fqc933PWRljjAAAwFlysQsAAPBuiFEAAJcgRgEAXIIYBQBwCWIUAMAliFFx7N+//7e//W1VVZXYhQCAq2RY8CSw4uLiTZs2HT9+nIgUCsW+fftWrFghdlEA4Dx0o8K5cuXKiy++GB8ff/z4cblcrlQqLRaLWq1+8cUX6+vrxa4OAJyEGBXC9evXN27cGBcXt2/fPoVCodForl+/3tbW9v777wcGBu7bt2/q1KmZmZm4MgDwSgw8qbKyMi0tzc/Pj4iUSmVqauq1a9d6fkF5efkPfvAD/nexaNGiiooKsUoFAOcgRj2lvr5eq9X6+/sTkVwuV6vVpaWl9r1NTU09v9hkMgUHBxORSqXS6/UWi0XwegHASYhR92tsbNTpdI899hgRyWQytVp9+fJl+97y8nKNRhMUFHTr1q2e31VXV5eamsrb0meeeebChQuCFw4AzkCMutPdu3f1ev2IESN4GqakpJw9e9a+t7y8fP369QqFgoj8/PwOHjz44DscOnRo3LhxfARAq9V2dnYKWD4AOAMx6h6tra16vX7UqFH2AP3qq6/se+vq6rRa7bBhw/oaIe2pubk5LS1NLpcTUWxsbH5+viA/AQA4CTHqqs7OToPBEBYWxgN0zpw5//3vf+17Gxoaeo2QlpSUOPK2eXl506ZN48MCGo3m7t27nvoBAMA1iFHndXV1GQyGiIgI+4DmkSNH7Htv3bql0+mGDx/Oo3DJkiUFBQWDev+Ojg6dTufr60tEERER//rXv9z9EwCAGyBGnWE2m41GY1RUFA/QhIQEk8lk39vS0qLX60eOHGm/wD9z5ozTxyosLPzud7/L30qtVtfX17vjJwAAt0GMDo7VajWZTJMmTeK5FhsbazKZbDYb39va2pqRkRESEvLQEVKndXd3Z2RkBAYGElFQUJDBYHD9PQHAXRCjjrJYLNu2bUtISOAROXXqVKPRaF/gyS/we46QHjt2zL0FlJWVpaSk8PdfvHjx9evX3fv+AOAcxKhDqqurw8PDeYRFRUX1DFCz2WwwGMaMGcP3Pv3001lZWZ6rxGQyjR492r5Q32q1eu5YAOAIxKhDdu3axWeKNm3aZF/LyS/wJ06cyAM0Pj6+5wW+59TW1toX6s+ePbu4uNjTRwSAfiBGHZKZmUlE0dHR/CUP0MmTJ/Msi4mJESZAezpw4ABvkH18fJKTk7u6uoQ8OgDY4QlPDuHr6p988kn+8ubNm2vWrCkpKZk8efLevXuLiorUarVMJhOypGXLll26dGnt2rVWq/XEiRNbtmwR8ugAYKcQuwDv0NLSQkR8rpyIxo8f/9Zbb40ZM2bNmjX85k5RjBgxwmg0Hj16tKqqqqysTKwyAIY4xKhDWltbiYg/bYR7/fXXxSvnW77zne9UVVXNnTtX7EIAhihc1DuEx6i9GxVXQUGBXC6fN28ef9nd3U1E9rVWACAwxKhDJBWjLS0tfGCbv3ywUwYAISFGHcLHRiUSVb0GaiUV8QBDEGLUIZKKql7tp6QiHmAIQow6pFcDKC50owCSghh1iKTGH9GNAkgKYtQhkupGe7afjLH29naZTBYQECB2XQBDFGLUIRLsRnmMtrW12Ww2lUrFP3QEAISHc88hkhp/7HkVjyt6ANEhRh0iqbTqmemSyneAoQkx6hBJpdWD3ahECgMYmhCjA7NarR0dHXK5XKVSiV0L0cO6UYm0yQBDE2J0YK2trYyxwMBAgR+F15eeHaik2mSAoQkxOjCpRVXPDlRSg7YAQxNidGBSG39ENwogKYjRgUlt/BHdKICkIEYHJrWODwueACQFMTowSXV8Fouls7NToVD4+/sTYhRAAhCjA5NUVPUaqJXauC3AEIQYHZikutFemS61cVuAIQgxOjAJdqP23JRUbQBDE2J0YA92o83NzWIV0ys3JdUpAwxNiNGB8eSyP9Dz1q1bU6ZM2bhxI48wgaEbBZAaxOjACgsLqUcHeuLEidu3b2dmZsbFxX322WcCF4NuFEBqEKMDKy8vJ6L9+/d3dXUR0bJly86ePZuYmHjjxo0f/vCHK1eubGhoEKyY4OBgtVr99NNP85foRgHEx2Agb7zxBv9dTZ8+/euvv+YbrVarwWDg+TVq1CiDwSBKbaNHjyai+vp6UY7en6tXWU4OO3uWWSxilwLgWYhRh7z77rsTJkwgIoVCkZaW1tLSwreXlZUtXLiQh+yiRYsqKiqErMpsNvv6+hJRe3u7kMcdQGMjW7CAhYezRYvYlCksKoqdOSN2TQAehBh1VHt7u1ar9fHxIaKoqKicnBz7LpPJFBwcTEQqlUqv11s8339ZrVaTyTRp0qRhw4b5+fnNmzfvypUrnj6oo5YvZ4sXM57sNht78002dizr6BC7LABPQYwOzrlz55566ikikslkqampjY2NfHttbW1qaipvS5955pmLFy96qACr1bp///6pU6fyY40fP37kyJE8wd99910BEnwALS1MLmdFRfe3mM0sNJQdOiReTQCehRgdtO7ubr1eP2zYMCIKCwszmUz2XdnZ2ePGjSMipVKp1Wo7Ozvde+icnJyZM2fyAI2MjDQYDN3d3bdv39ZoNPbR2zPiXkEXFDCFgnV3f2tjcjLLyBCpIACPQ4w66erVq/Pnz+fhtWTJksrKSr79zp07aWlp/OOO4+Li8vPz3XK4nJycWbNm8cONGzcuIyOjV0Z/9tlnkZGRfPTWEwnuqHPnmI9P72ml+fNZero49QB4HmLUeTabzWAwDB8+nIhGjBiRkZFhtVr5rtzcXH7dLZfLNRrN3bt3nT5KXl6ePa8ff/xxvV7f0cc4Y1tbm330Njo6+tixY04f1BnNzezqVXb3LpPJ2OXL97dbLCwigh08KGgx/dq9e/drr71269YtsQuBRwRi1FXV1dXLli3jMTd37tzL3yRIR0eHTqfjM+kTJkw4fPjwYN85Pz9/yZIl/J1Hjx6t1+vb2toG/K5Tp07FxMTw0VuNRtPc3DzoH2mwWltZRgYLCWFz5jDG2OLF7Mc/vt+Q7tjBQkNZWxsrLWV//jP75l8aUZw/f37OnDn8V+rn5/f555+LWAw8MhCj7mEymUJCQojI399fp9OZzWa+/fz58/aLcbVa3dDQ4Mi7FRYWqtVq/l2PPfaYVqu9c+eO48WYzWa9Xu/n50dEERERn376qTM/kiM6Olh6OgsJYUSMiCUnszt32M2bLCGBxcezn/+cLVjAHn+cnTjBbDY2fz4jYklJ7NIlT9XTt/Pnzz///PP8QwnlcrlCoeD/0rz00ktNTU3C19OXrq4u8ecJYZAQo27Dp3r4ifrkk0/+73//49u7u7szMjL4LfkhISFGo7GfN7l48aJareZvEhgYqNVqb9++7Vw9RUVFiYmJ9gR38xJ9s5kZDGzMmHsBmpjIsrLu77VY2PHj7IMPWHY2+2aNLcvKYmPHMiKmVDKtlnV1ubOevl26dCk1NZWPdahUqrS0tOrq6vb29i1btjx0nlAs3d3dGzZsGD58eFhY2IEDB8QuBwYBMepmhw8ftk/1pKWl8Q9nZoyVlpbyIc6QkJCHtpZlZWUajabn2V5bW+tiMR651cpqZSYTmzjxXoDGxTGTidlsDn3vnTssLY3J5YyIxcezL790Qz19q6io0Gg0vPH09fXVaDTV1dU9v8D+l9JrnlBgVqt1z549kyZNst9b6OPj8+abb4o2TwiDhBh1v55TPRMnTjx69CjfbrPZdu7c+WCj8eDZXlVV5cZ6ysvL3XOrFQ/QyZPvBei0acxodGas8+RJNmUKI2JyOdNo7rer7nPjxo20tDQ+rKFUKlNTU8vKyh76lf3MEwrAZrNlZWVNnz7dvoht7dq1KSkpos0T9q26utrpC6NHHmLUU86ePTtjxgz7Qv2HzgtXVlb2OtuvXbvmoXpcudXKZrN98sknJrX6XoBOmsQ++silyaL2dqbTMaWSEbEnnmDum+qpr6/XarX8al0ul6vV6tLS0gG/q695Qo/Kycnht3Lw2yj4KmC+S4R5wr41NjZqtVqVSvXqq6+KWIaUIUY9qOdUT3h4+CeffGLfxc92/rF0jp/tLup1q9WFCxcc+a7s7Gz+74Gvj09rUhL72996r6532pkzbPp0RsRkMrZ+fatrzY79bLf/Sgebhn3NE7pdbm7uvHnz+F/E2LFjH1wFzIScJ+xbU1PTG2+8wR/DKJPJfvKTnwhfg1dAjHpcSUlJcnKyfQCuqKhIp9PZ/9d04mx3keO3WuXm5torDwkJ6WfJqvO6u1lGBgsIODRrVmhoaP/zb325e/euXq8fMWIELzUlJeXs2bPOldPXPKG7nDp1asGCBbzO4OBgvV7f/2NlPDtP2LfW1la9Xj9q1Cj7r9Ttv4pHCWJUCFar9b333uNTPfxPmUz2wgsvFBYWilJPc3Oz/Var2NjYB2+1GuzZ7qorV1Y//zw/3HPPPef40LCHzva+5gldcfr06Z6rgHU6nYOX6gI/krGtrS0jIyM0NJSXOmfOnOPHj3v0iI8AxKhwKioqJk6cOGrUqJ7PLRVRXl7eg7daffnll/azPSgoyPGz3UU2m81oNAYFBTk41ePps72veUInFBUV9VrENqhVwJzb5gn71tXVZTAYIiIi7MM+R44ccftRHkmIUUGtXbuWiHbv3i12Ifd0dHSkpaXxM9zX1zc+Pp6fQoGBgW+//bbwMxs1NTXLly/nNSQlJV162EJ9Ic92R+YJ+1FcXJyamsq7/oCAAFdWAXMeeiSj2Ww2Go1RUVH8V5qQkCCFhbReBDEqqBdeeIGIJLW4uri4mF+98j/5f+zatUvEkrKyssaMGUNEw4YN0+l0Xd8s1BflbO81T+jg3115ebl9FTBfxFZTU+OWepybJ+wLf3BtdHQ0f8PY2FiTyWRzcBUwfAMxKqjvf//7RPSf//xH7ELu++qrr4ho5syZv/jFL1asWMHnlJx4AoB7NTU12ad6EhISvvjiiy1bttgXqAt/tveaJ7x582ZfX3n9+nWPrgLmXH8ko81mM5lMU6ZM4T/U1KlTjUYj7kN1DmJUUPyj6E6dOiV2IfcdO3aMiJ599ln+cvbs2USUl5cnblVcTk4O7z35dTERTZs27R//+IeQK+Tt+EJ9vsRi5MiRBoOhV47X1dXZl6x6ehUwc2CesB85OTl8sIJ6PLjWc6U+8hCjgoqLiyMisSboH+rf//43ES1dupS/5MOjBQUF4lZl19bWtmHDBrlcrlQqf/e734l+tldVVT333HM8gJKTk0tKShhjDQ0NvVYB8+0CGOwjGQd8cC04ATEqKL6Mpq8bE0Xx97//nYhWr17NXz7xxBNE5NE2ygklJSWeWwzvhA8//JBP9SgUioiICP44RJlMtnLlyuLiYoGLcfCRjLm5uc8++ywP0P4fXAuDhRgVFP88ZAcflyeMv/71r0Sk0Wj4S54OdXV14lYlfbdv3+YThtyCBQvE/fiWfh7JmJ+fn5KSwncJsQp46EGMCopP+EqqC9i2bRsRvfLKK/wlH9rDaeagP/zhDwsXLpTICrZej2R87bXXTCbTihUreIDyB9eKe3v+owoxKhyz2cwvA8Uu5Ft0Oh0R6XQ6xlh3dzcR+fj4iF0UOK/n0//sAfrWW285seYfHCQnEEpLSwsR8ale6WhtbSUi3sLw/+b3HYKXio6OPnr06IYNGxQKhVwuT05OLi0tfeedd+zPHAC3Q4wKR5ohxavi4S7NoIfBkslkmZmZN27cuHnz5vHjx+23zIKHKMQuYAiRZkjxqni4SzPowTnh4eFilzBUoBsVjjRDCt0ogIvQjQqnZ98nHfsVCpo82TZiBBFN6uwsSUoyf3ODIAA4AjEqnJ59n3T4V1ZSSQmpVEQ0qqlpVF4eBQWJXRSAN8FFvXCk2Y1SaysREa+q538DgGMQo8KRZjdKLS1EiFEA5yFGhSPNKaZ70cnDnUeq1IIeQNoQo8KRdIyiGwVwFmJUOFJcTtTZSRYL+fmRUkmEGAVwBmJUOFLsRntdxSNGAQYPMSocKc7U98pNjI0CDB5iVDhSnKnvFaPoRgEGDzEqHCl2o7ioB3AZYlQ4XtCN4qIeYPAQo8KR7hQTLuoBXIB76oUjxQVPS5dSYyNZrfdeZmZSTQ1FRopaE4CXQYwKR4rdqK8vnTlDu3ZRWRmNHk3PP08aDfn4iF0WgDfBRb0QzGbz+++/39XV5ePjk5+fL3Y5PXz0Ea1aRQsW0K5d9H//RxkZtHGj2DUBeBkZY0zsGh5lFotlz549v//97ysqKvgWmUy2bt267du3B4n+PDqbjcaOpS1baN26e1uuXqVp06iggGJjRa0MwJugG/UUm8328ccfx8bG/uxnP6uoqIiJifnggw/S09NVKtXu3btjYmI+/PBDkUusqqKaGurxYesUHU0JCfT11+LVBOB9EKPuxxg7ePDgU089tXLlypKSkgkTJhgMhsLCwvXr1//6178uLCz83ve+V1dXt27duiVLllRWVopWaHU1+fnR8OHf2hgaSlVVIhUE4JUQo2525MiRWbNm/ehHPyooKBg/frzBYCgtLdVoND7fzNtERUXl5OQYjcagoKBDhw7Fx8e/9957NptNhFojIqiri5qbv7Wxro7GjhWhGADv5e4Pvh+6cnNz582bx3+rISEher2+o6Ojn6+vqalZvnw5//qkpKRLly4JVuo9VisLD2dG4/0t164xpZJdvCh0JQDeDDHqBqdPn05NTQ0ICCCi0NDQ9PT0/gO0p6ysrDFjxhDRsGHDdDpdV1eXR0vtbfduFhTEdu9mpaXs889ZXBxbv17QAgC8H2bqXXLu3LmtW7eeOHGCiJKSkmbOnLl582aep467c+eOVqvduXMnYywhIWHXrl2zZs3yTL0Pk51Nf/kLXbtGoaG0bBlt3ox1owCDghh10uXLl9PT07OzsxljAQEBP/3pTzdv3jy813TNYJw8eXLDhg0lJSUKheKXv/zln/70p8HGMQCIAjE6aKWlpTt27Pj0009tNptKpVq9evXLL78cHBzs+jt3dHS8884727Zts1qtUVFRBoMhJSXF9bcFAI9CjPbnj3/84+uvv25/WVlZuWPHjn379lmtVqVSuWrVqldffTUkJMS9By0oKHjppZfOnDkjk8nWrFmTnp4+evRo9x4CANwIMfpwhw8fbm5uPn36dGxs7IwZM8LDwzMyMvbv32+xWHiAvvLKK2FhYR46usVi2b59+9tvv93Z2RkWFrZjx44VK1Z46FgA4CKsG3242bNnm83mL774IjIyctq0aWvWrPnoo4+IaPXq1adOndq6davnMpSIFAqFVqstKiqaP39+bW2tWq1eunTpzZs3PXdEAHAaYvTh/P39s7OzjUbjgQMHVCrVyy+/vHz58pMnT27fvn2sUKvTo6Ojjx49ajAYhg8fnp2dHR8fn5mZiasHAKnBRX2fzGazr68v/1PcSmpqajZt2vTPf/6TiObOnbtz584pU6aIWxIA2CFGvcbHH3/8q1/9qr6+3t/ff+XKlZmZmaLnOwAQYtS7NDY2/uY3v+GjtJGRkRcuXJDWQ6ABhiSMjXqT4ODgPXv26PV6uVxeV1fX1NQkdkUAgG7UOymVSovFYjablUql2LUADHXoRr1PR0eHxWLx8/NDhgJIAWLU+0jx8+4BhjDEqPfhH9SMySUAiUCMeh90owCSghj1PuhGASQFMep90I0CSApi1PvwGEU3CiARiFHvwy/q0Y0CSARi1PugGwWQFMSo90E3CiApiFHvExAQsHDhwsjISLELAQAixKg3amhoKCoqUigUYhcCAESIUW+EsVEASUGMep/29nYiUqlUYhcCAESIUW+EbhRAUhCj3qetrY2IAgICxC4EAIgQo94I3SiApCBGvQ8fG0U3CiARiFHvw7tRTDEBSARi1PvwsVFc1ANIBGLUy1it1s7OTrlc7u/vL3YtAECEGPU6bW1tjDGVSiWTycSuBQCIiHBDofdZtWoV7gQFkA58Tr33ycvLS0xMxKcrA0gELuq9ye3bt0tLS/fu3dvW1nb16lWxywEAIsSod2lpadm7d29xcfHWrVtra2vFLgcAiBCj3iUyMjI0NHT9+vWNjY2JiYlilwMARJhi8jozZsxITEyMiYnBLBOARGCKCQDAJbioBwBwCWIUAMAliFEAAJcgRgEAXIIYBQBwCWIUAMAl/w9s6c3+Aww2IwAAAW56VFh0cmRraXRQS0wgcmRraXQgMjAyNS4wMy41AAB4nHu/b+09BiAQAGImBggQhuIGRgYWDZAAow4jkNRiY0gAqWKC0Mzosg4gHjMLm0MGiGZmxMuAqmWA0BwQmglGo9pEiGZk5mZgZGBkYmBiZmBiYWBhzWBiZUtgY89gYudI4ODMYOLkUuDi1mDi4lHg4WXg5WPg42fgF2AQEGQQFGLgZElwArmfjYWTg52NVfwdyEAGWEgsV2M7MMsr5wCIk1TLeGBaOg+YLWYcdCDm5ob9IDb7yckHToawgMUvdvoe2HrqnS2IfUi+/8DPI7n2ILbC6Z4DTiHSDiC2u7nLgVgbdzBbjfn5/hJHYTB7Q+iH/YcrfcHqJ1yR3X+jXGYPiL2k0mD/938HwHYFLze379XTBasRsv9rH/qFax+I3fQv2CH8MQ9YXDpqksN0gTaw+HTOvQ6OBjftQOzClKcOzMuPgcW/c3I4Xp05AywuBgAVpFn+fuZSbAAAAeB6VFh0TU9MIHJka2l0IDIwMjUuMDMuNQAAeJx9VFtuGzEM/PcpdIEIfEkkP2M7CIIiNpC6vUP/c3+UlONKAYRql4SknR2tOKM9lGwf5x9/Psu/RufDoRT4z+3u5TcDwOG9ZKccX17fLuV0ez4+Zk7XX5fbz4KeN+T1Hft8u74/ZrB8lCeq4B3ByxPX7gSNC1QYbb5L5TSQxKSWPXQ2og2SE8mVTLgHEisbdt5xSq4utaORw2Dv2rVtkO2LE7GxxnPE3tw2wJ5AqaLUKYFOpqYboN6B7K7eC1UhBdwx2n1p6M1UCsfSDM03QE8gVgNRk2AMlUxhA0T4Qqp2zN0aUOcdJWIiISrUG7XsAXhrW1Iq1wE159aTvoGq4Q7KAYWqqOxZmii6+E5JlFgeq8d+ooZBjkG9LTumQFwZSSA3Et/IIjtgCiS1KUs4OSgp1Jed5JgKteqgQrkLIcFQagNMhbRitxb1Cko2dd/5DVMii8KQDD+Sh1i7ur9czt+Oyf3gHK+X8zw4edE8HTEoPI8ARsj0eQ7bNLNE9GlZidBpTImwaT+J8GkyicDVSzgSLp6hTEiLNXDM8OIAHEkWqXGktkiKI/VFOxxJF41wJFvEwJF8KbrkF7a1uGspc/z4f0X/8BdI+/K9jJf5GgAAASF6VFh0U01JTEVTIHJka2l0IDIwMjUuMDMuNQAAeJxVkMtOQzEMRH+FZVulll+J7Vas7hap/EJ57KiKECyQ+vFMbldklYzPjO3slmWz277K6zyybB5P29OynofbZq/ENUTa3miUcm/HKalpNlykLBWSkaZbtL2QpQyD5DQktSbFI0bcKZHujUlk9MqV8lDwTKUZGatkhZqSa/A9nEdHyWAzHlCEkj0mY8yrCUrEEOQkI68gMQYYXXvDjbl659ULOcv6mLN2DqQcmULCKuDGpF5oKlSRXtMrgOesRibqswWizL0dnXrYmq/Y1fE3nYrDFYwDLfQLkpFdJmQZVfiaRF/1ubMW9qi2befv6+X56/p5YHr7uVx+n84v7x+0O9i/5+0P+H1cT3omvAgAAAAASUVORK5CYII=",
      "text/plain": [
       "<rdkit.Chem.rdchem.Mol at 0x7fff1524fe60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "full_train_data = pd.read_csv(\"../data/train.csv\")\n",
    "valid_ffv = full_train_data[full_train_data['FFV'].notna()]\n",
    "\n",
    "two_star_ffv = valid_ffv[valid_ffv['SMILES'].str.count(r\"\\*\") == 2]\n",
    "\n",
    "from data_gen_helpers import display_smiles\n",
    "sample_smiles = two_star_ffv.loc[0, 'SMILES']\n",
    "\n",
    "display_smiles(sample_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9aab67c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch_geometric.data.data.Data'>\n",
      "tensor([[ 1,  0,  1,  0,  0,  0],\n",
      "        [ 1,  0,  1,  0,  0,  0],\n",
      "        [ 1,  0,  0,  1,  0,  0],\n",
      "        [ 1,  0,  1,  0,  0,  0],\n",
      "        [ 1,  0,  0,  1,  0,  0],\n",
      "        [ 1,  0,  0,  1,  0,  0],\n",
      "        [ 1,  0,  1,  0,  0,  0],\n",
      "        [ 1,  0,  1,  0,  0,  0],\n",
      "        [ 1,  0,  0,  1,  0,  0],\n",
      "        [-1,  0,  1,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  0,  0,  1,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  0,  0,  1,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  0,  0,  1,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  0,  1,  0,  0,  0],\n",
      "        [-1,  0,  0,  1,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  0,  0,  1,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  0,  1,  0,  0,  0],\n",
      "        [-1,  1,  0,  0,  0,  0],\n",
      "        [-1,  0,  0,  0,  0,  1]])\n",
      "tensor([[ 0,  0,  9,  9,  1,  0, 10, 10,  2,  0, 11, 11,  3,  0, 12, 12,  4,  0,\n",
      "         13, 13,  5,  0, 14, 14,  6,  0, 15, 15,  7,  0, 16, 16,  8,  1,  1, 17,\n",
      "         17,  2,  1, 18, 18,  3,  1, 19, 19,  4,  1, 20, 20,  5,  1, 21, 21,  6,\n",
      "          1, 22, 22,  7,  1, 23, 23,  8,  2,  2, 24, 24,  3,  2, 25, 25,  4,  2,\n",
      "         26, 26,  5,  2, 27, 27,  6,  2, 28, 28,  7,  2, 29, 29,  8,  3,  3, 30,\n",
      "         30,  4,  3, 31, 31,  5,  3, 32, 32,  6,  3, 33, 33,  7,  3, 34, 34,  8,\n",
      "          4,  4, 35, 35,  5,  4, 36, 36,  6,  4, 37, 37,  7,  4, 38, 38,  8,  5,\n",
      "          5, 39, 39,  6,  5, 40, 40,  7,  5, 41, 41,  8,  6,  6, 42, 42,  7,  6,\n",
      "         43, 43,  8,  7,  7, 44, 44,  8,  8],\n",
      "        [ 0,  9,  0,  1,  9, 10,  0,  2, 10, 11,  0,  3, 11, 12,  0,  4, 12, 13,\n",
      "          0,  5, 13, 14,  0,  6, 14, 15,  0,  7, 15, 16,  0,  8, 16,  1, 17,  1,\n",
      "          2, 17, 18,  1,  3, 18, 19,  1,  4, 19, 20,  1,  5, 20, 21,  1,  6, 21,\n",
      "         22,  1,  7, 22, 23,  1,  8, 23,  2, 24,  2,  3, 24, 25,  2,  4, 25, 26,\n",
      "          2,  5, 26, 27,  2,  6, 27, 28,  2,  7, 28, 29,  2,  8, 29,  3, 30,  3,\n",
      "          4, 30, 31,  3,  5, 31, 32,  3,  6, 32, 33,  3,  7, 33, 34,  3,  8, 34,\n",
      "          4, 35,  4,  5, 35, 36,  4,  6, 36, 37,  4,  7, 37, 38,  4,  8, 38,  5,\n",
      "         39,  5,  6, 39, 40,  5,  7, 40, 41,  5,  8, 41,  6, 42,  6,  7, 42, 43,\n",
      "          6,  8, 43,  7, 44,  7,  8, 44,  8]])\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import gzip\n",
    "\n",
    "if 'matching_dataset' not in locals():\n",
    "    matching_dataset = []\n",
    "if not matching_dataset:\n",
    "    file_path = \"../data/vae_github_data/dataset_train_fc.pkl.gz\"\n",
    "    with gzip.open(file_path, \"rb\") as f:\n",
    "        matching_dataset = pickle.load(f)\n",
    "\n",
    "print(type(matching_dataset[0]))\n",
    "print(matching_dataset[0]['x'])\n",
    "print(matching_dataset[0]['edge_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0256873c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4., 1., 4., 5., 0.])\n",
      "tensor([ 5.,  0.,  6., 10.,  0.])\n",
      "tensor([ 7.,  2.,  1., 10., 14.])\n",
      "tensor([2., 2., 2., 0., 0.])\n",
      "tensor([1., 1., 0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "from dataset_helpers import *\n",
    "\n",
    "print(minimal_orbital_token(24))\n",
    "print(minimal_orbital_token(46))\n",
    "print(minimal_orbital_token(113))\n",
    "print(minimal_orbital_token(6))\n",
    "print(minimal_orbital_token(1, include_star_dim=True))  # Hydrogen\n",
    "print(minimal_orbital_token(-4, include_star_dim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2737b1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_star_ffv_graph_data = smiles_iter_to_graph_dataset(two_star_ffv.SMILES, two_star_ffv.FFV)\n",
    "\n",
    "save_dataset(two_star_ffv_graph_data, '../data/working_datasets/two_star_include_stars')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b45d2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(x=[23, 6], edge_index=[2, 44], edge_attr=[44, 4], y=0.32618869)\n",
      "tensor([[0., 0., 0., 0., 0., 1.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 1.],\n",
      "        [2., 2., 4., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 4., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 2., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.],\n",
      "        [2., 2., 5., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [0., 0., 1., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.],\n",
      "        [1., 0., 0., 0.]])\n",
      "tensor([[ 0,  1,  1,  2,  2,  3,  2,  4,  4,  5,  5,  6,  5,  7,  7,  8,  7,  9,\n",
      "          7, 10, 10, 11, 10, 12, 10, 13, 13, 14, 13, 15, 13, 16, 16, 17, 16, 18,\n",
      "         16, 19, 19, 20, 19, 21, 19, 22],\n",
      "        [ 1,  0,  2,  1,  3,  2,  4,  2,  5,  4,  6,  5,  7,  5,  8,  7,  9,  7,\n",
      "         10,  7, 11, 10, 12, 10, 13, 10, 14, 13, 15, 13, 16, 13, 17, 16, 18, 16,\n",
      "         19, 16, 20, 19, 21, 19, 22, 19]])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torch_geometric.loader import DataListLoader\n",
    "\n",
    "TRAIN_TEST_RATIO = 0.9\n",
    "train_len = int(TRAIN_TEST_RATIO* len(two_star_ffv_graph_data))\n",
    "train_dataset, test_dataset = random_split(two_star_ffv_graph_data, [train_len, len(two_star_ffv_graph_data)-train_len])\n",
    "\n",
    "train_dataloader = DataListLoader(train_dataset)\n",
    "test_dataloader = DataListLoader(test_dataset)\n",
    "\n",
    "print(train_dataset[0])\n",
    "print(train_dataset[0]['x'])\n",
    "print(train_dataset[0]['edge_attr'])\n",
    "print(train_dataset[0]['edge_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63fc26ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union, Tuple, Optional\n",
    "from torch import Tensor\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    GINConv, \n",
    "    MessagePassing, \n",
    "    GATConv, \n",
    "    JumpingKnowledge,\n",
    ")\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.nn.resolver import normalization_resolver, activation_resolver\n",
    "\n",
    "from torch.nn import Linear, ModuleList\n",
    "\n",
    "class CustomGINEncoder(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_dims, out_channels, dropout=0.0, act='relu', act_kwargs={}, norm=None, norm_kwargs={}, jk=\"cat\", **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.supports_edge_weights=False\n",
    "        self.supports_edge_attrs=True\n",
    "\n",
    "        self.num_layers = len(hidden_dims)\n",
    "        self.hidden_layers = list(hidden_dims)\n",
    "\n",
    "        if len(self.hidden_layers) == 0:\n",
    "            raise ValueError(\"Must have at least one layer\")\n",
    "        \n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "        self.act = activation_resolver(act, **act_kwargs)\n",
    "        self.norm = norm\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        di = [in_channels] + hidden_dims\n",
    "        do = hidden_dims + [out_channels]\n",
    "\n",
    "        self.convs = ModuleList()\n",
    "        self.norms = ModuleList()\n",
    "\n",
    "        for (in_dim, out_dim) in zip(di, do):\n",
    "            self.convs.append(\n",
    "                self.init_conv(in_dim, out_dim, **kwargs)\n",
    "            )\n",
    "            layer_norm = normalization_resolver(\n",
    "                norm, out_dim, **norm_kwargs\n",
    "            ) if norm is not None else nn.Identity()\n",
    "            self.norms.append(layer_norm)\n",
    "\n",
    "        self.jk_mode = jk\n",
    "        self.jk = None\n",
    "        if jk is not None and jk != 'last':\n",
    "            self.jk = JumpingKnowledge(jk, hidden_dims[-1], len(hidden_dims)-1)\n",
    "        \n",
    "        if jk is not None:\n",
    "            if jk == 'cat':\n",
    "                in_channels = sum(do)\n",
    "            else:\n",
    "                in_channels = out_channels\n",
    "            self.lin = Linear(in_channels, self.out_channels)\n",
    "\n",
    "    def init_conv(self, \n",
    "        in_channels: Union[int, Tuple[int, int]],\n",
    "        out_channels: int, **kwargs) -> MessagePassing:\n",
    "        # GAN attentions conv, supports node and edge weight\n",
    "        return GATConv(in_channels, out_channels, heads=6, concat=False, dropout=self.dropout.p)\n",
    "\n",
    "    def forward( \n",
    "        self,\n",
    "        data,\n",
    "        batch: Optional[Tensor] = None\n",
    "    ):\n",
    "        x=data.x\n",
    "        \n",
    "        edge_index=data.edge_index\n",
    "        if data.edge_attr is None:\n",
    "            edge_attr=data.edge_weight\n",
    "            edge_weight=data.edge_weight\n",
    "        else:\n",
    "            edge_attr=data.edge_attr\n",
    "            edge_weight=data.edge_attr\n",
    "        if data.batch is not None:\n",
    "            batch=data.batch\n",
    "            \n",
    "        xs: List[Tensor] = []\n",
    "        for i, (conv, norm) in enumerate(zip(self.convs, self.norms)):\n",
    "\n",
    "            if self.supports_edge_weights and self.supports_edge_attrs:\n",
    "                x = conv(x, edge_index, edge_weight=edge_weight,\n",
    "                    edge_attr=edge_attr)\n",
    "            elif self.supports_edge_weights:\n",
    "                x = conv(x, edge_index, edge_weight)\n",
    "            elif self.supports_edge_attrs:\n",
    "                x = conv(x, edge_index, edge_attr=edge_attr)\n",
    "            else:\n",
    "                x = conv(x, edge_index)\n",
    "\n",
    "            if i < self.num_layers or self.jk_mode is not None:\n",
    "\n",
    "                if self.norm in [\"instance\",\"pairnorm\",\"layer\",\"graph\"]:\n",
    "                    x = norm(x, batch)\n",
    "                else:\n",
    "                    x = norm(x)\n",
    "\n",
    "                if self.act is not None:\n",
    "                    x = self.act(x)\n",
    "                x = self.dropout(x)\n",
    "\n",
    "                if self.jk is not None:\n",
    "                    xs.append(x)\n",
    "\n",
    "        # if self.jk_mode is not None:\n",
    "        #     xj = self.jk(xs)\n",
    "        #     return self.lin(xj)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9db8e6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from torch_geometric.nn import GATConv\n",
    "from torch_geometric.nn.models import InnerProductDecoder\n",
    "from torch_geometric.utils import (\n",
    "    add_self_loops, remove_self_loops, dropout_edge, \n",
    "    coalesce, negative_sampling, to_undirected\n",
    ")\n",
    "from torch_geometric.data import Data, Batch\n",
    "import torch_geometric.utils as utils\n",
    "\n",
    "\n",
    "class DualGraphRecurrentModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent module that processes two sets of graph data as one combined graph,\n",
    "    but only outputs results for the second graph using the first set of edges.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, hidden_dim: int = 64, num_attention_layers: int = 3, \n",
    "                 heads: int = 4, dropout: float = 0.1):\n",
    "        super(DualGraphRecurrentModule, self).__init__()\n",
    "        \n",
    "        self.num_attention_layers = num_attention_layers\n",
    "        self.edge_dropout_prob = dropout\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        \n",
    "        # First layer: in_channels -> hidden_dim\n",
    "        self.attention_layers.append(\n",
    "            GATConv(in_channels, hidden_dim, heads=heads, concat=False, dropout=dropout)\n",
    "        )\n",
    "        \n",
    "        # Intermediate layers: hidden_dim -> hidden_dim\n",
    "        for _ in range(num_attention_layers - 2):\n",
    "            self.attention_layers.append(\n",
    "                GATConv(hidden_dim, hidden_dim, heads=heads, concat=False, dropout=dropout)\n",
    "            )\n",
    "        \n",
    "        # Last layer: hidden_dim -> in_channels (to maintain shape)\n",
    "        if num_attention_layers > 1:\n",
    "            self.attention_layers.append(\n",
    "                GATConv(hidden_dim, in_channels, heads=heads, concat=False, dropout=dropout)\n",
    "            )\n",
    "        else:\n",
    "            # If only one layer, output in_channels directly\n",
    "            self.attention_layers[0] = GATConv(in_channels, in_channels, heads=heads, concat=False, dropout=dropout)\n",
    "        \n",
    "        self.final_attention = GATConv(in_channels, in_channels, heads=heads, concat=False, dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual = nn.Linear(in_channels, in_channels) if num_attention_layers > 0 else nn.Identity()\n",
    "        \n",
    "        # Edge manipulation layer\n",
    "        self.edge_manipulator = EdgeManipulationLayer(dropout)\n",
    "        \n",
    "    def forward(self, x1: Tensor, x2: Tensor, edge_index1: Tensor, edge_index2: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Combine both graphs into one large graph, apply convolutions, then return only second graph results.\n",
    "        \n",
    "        Args:\n",
    "            x1: Node features of first graph [num_nodes1, in_channels]\n",
    "            x2: Node features of second graph [num_nodes2, in_channels]\n",
    "            edge_index1: Edge connectivity of first graph [2, num_edges1]\n",
    "            edge_index2: Edge connectivity of second graph [2, num_edges2]\n",
    "            \n",
    "        Returns:\n",
    "            Updated and normalized node features for second graph [num_nodes2, in_channels]\n",
    "        \"\"\"\n",
    "        num_nodes1, num_nodes2 = x1.size(0), x2.size(0)\n",
    "        \n",
    "        # Combine node features\n",
    "        x_combined = torch.cat([x1, x2], dim=0)  # [num_nodes1 + num_nodes2, in_channels]\n",
    "        \n",
    "        # Offset edge indices for second graph to account for combined graph\n",
    "        edge_index2_offset = edge_index2 + num_nodes1\n",
    "        \n",
    "        # Apply edge manipulation to first set of edges, but ensure they're valid for second graph\n",
    "        # We need to adjust edge_index1 to point to nodes in the second graph\n",
    "        edge_index1_for_second = self.edge_manipulator(\n",
    "            edge_index1, num_nodes2, offset=num_nodes1\n",
    "        )\n",
    "        \n",
    "        # Combine all edges into one large graph\n",
    "        edge_index_combined = torch.cat([edge_index1, edge_index2_offset, edge_index1_for_second], dim=1)\n",
    "        \n",
    "        # Remove duplicates and ensure valid connectivity\n",
    "        edge_index_combined = coalesce(edge_index_combined, num_nodes=num_nodes1 + num_nodes2)\n",
    "        \n",
    "        residual = self.residual(x2)\n",
    "        \n",
    "        # Process combined graph (no gradients for intermediate layers)\n",
    "        with torch.no_grad():\n",
    "            h_combined = x_combined\n",
    "            for attention_layer in self.attention_layers:\n",
    "                h_combined = attention_layer(h_combined, edge_index_combined)\n",
    "                h_combined = F.relu(h_combined)\n",
    "                h_combined = self.dropout(h_combined)\n",
    "        \n",
    "        # Final attention with gradients (only extract second graph portion)\n",
    "        h_combined = self.final_attention(h_combined, edge_index_combined)\n",
    "        h2_output = h_combined[num_nodes1:num_nodes1 + num_nodes2]  # Extract second graph nodes\n",
    "        \n",
    "        # Normalize the output\n",
    "        h2_normalized = F.normalize(h2_output, p=2, dim=1)\n",
    "        \n",
    "        # Add residual connection\n",
    "        output = h2_normalized + residual\n",
    "        \n",
    "        return output\n",
    "\n",
    "\n",
    "class EdgePredictionModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Module that uses PyG's InnerProductDecoder for proper edge prediction.\n",
    "    Includes edge manipulation capabilities using PyG utilities.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, hidden_dim: int = 64, \n",
    "                 edge_threshold: float = 0.5, edge_dropout_prob: float = 0.1):\n",
    "        super(EdgePredictionModule, self).__init__()\n",
    "        \n",
    "        self.edge_threshold = edge_threshold\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Node encoder to create embeddings for edge prediction\n",
    "        self.gat1 = GATConv(in_channels, hidden_dim)\n",
    "        self.gat2 = GATConv(hidden_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        \n",
    "        # Use PyG's InnerProductDecoder for edge prediction\n",
    "        self.decoder = InnerProductDecoder()\n",
    "        \n",
    "        # Edge manipulation layer\n",
    "        self.edge_manipulator = EdgeManipulationLayer(edge_dropout_prob)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Predict new edges using PyG's proper edge prediction pipeline.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Existing edge connectivity [2, num_edges]\n",
    "            \n",
    "        Returns:\n",
    "            New edge_index [2, num_new_edges]\n",
    "        \"\"\"\n",
    "        num_nodes = x.size(0)\n",
    "        \n",
    "        # Apply edge manipulation to input edges\n",
    "        edge_index_modified = self.edge_manipulator(edge_index, num_nodes)\n",
    "        \n",
    "        # Create node embeddings using the encoder\n",
    "        z = self.gat1(x, edge_index_modified)  # [num_nodes, hidden_dim]\n",
    "        z = self.relu(z)\n",
    "        z = self.dropout(z)\n",
    "        z = self.gat2(z, edge_index_modified)\n",
    "        z = self.relu(z)\n",
    "        \n",
    "        # Generate all possible node pairs for edge prediction\n",
    "        row, col = torch.meshgrid(\n",
    "            torch.arange(num_nodes, device=x.device),\n",
    "            torch.arange(num_nodes, device=x.device),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        \n",
    "        # Create edge index for all possible edges (excluding self-loops)\n",
    "        mask = row != col\n",
    "        all_edges = torch.stack([row[mask], col[mask]], dim=0)\n",
    "        \n",
    "        # Use InnerProductDecoder to predict edge probabilities\n",
    "        edge_probs = self.decoder(z, all_edges, sigmoid=True)  # [num_possible_edges]\n",
    "        \n",
    "        # Select edges above threshold\n",
    "        selected_mask = edge_probs > self.edge_threshold\n",
    "        new_edge_index = all_edges[:, selected_mask]\n",
    "        \n",
    "        # Ensure edges are undirected\n",
    "        new_edge_index = to_undirected(new_edge_index, num_nodes=num_nodes)\n",
    "        \n",
    "        return new_edge_index\n",
    "\n",
    "\n",
    "class EdgeManipulationLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Layer that uses PyG utilities for proper edge manipulation (add/remove/dropout).\n",
    "    \"\"\"\n",
    "    def __init__(self, dropout_prob: float = 0.1):\n",
    "        super(EdgeManipulationLayer, self).__init__()\n",
    "        \n",
    "        self.dropout_prob = dropout_prob\n",
    "        \n",
    "    def forward(self, edge_index: Tensor, num_nodes: int, offset: int = 0) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply edge manipulation using PyG utilities.\n",
    "        \n",
    "        Args:\n",
    "            edge_index: Current edge connectivity [2, num_edges]\n",
    "            num_nodes: Number of nodes in the target graph\n",
    "            offset: Offset to add to edge indices (for graph combination)\n",
    "            \n",
    "        Returns:\n",
    "            Modified edge_index [2, num_modified_edges]\n",
    "        \"\"\"\n",
    "        if not self.training:\n",
    "            # Apply offset if specified\n",
    "            if offset > 0:\n",
    "                edge_index = edge_index + offset\n",
    "                # Ensure edges are within valid range\n",
    "                valid_mask = (edge_index[0] < offset + num_nodes) & (edge_index[1] < offset + num_nodes)\n",
    "                edge_index = edge_index[:, valid_mask]\n",
    "            return edge_index\n",
    "        \n",
    "        # Apply edge dropout using PyG utility\n",
    "        edge_index, _ = dropout_edge(\n",
    "            edge_index, \n",
    "            p=self.dropout_prob, \n",
    "            force_undirected=True,\n",
    "            training=self.training\n",
    "        )\n",
    "        \n",
    "        # Apply offset if specified\n",
    "        if offset > 0:\n",
    "            edge_index = edge_index + offset\n",
    "            # Ensure edges are within valid range after offset\n",
    "            valid_mask = (edge_index[0] < offset + num_nodes) & (edge_index[1] < offset + num_nodes)\n",
    "            edge_index = edge_index[:, valid_mask]\n",
    "        \n",
    "        # Optional: Add some random edges (edge addition)\n",
    "        if torch.rand(1).item() < self.dropout_prob:\n",
    "            num_new_edges = max(1, int(edge_index.size(1) * 0.1))  # Add 10% more edges\n",
    "            \n",
    "            if offset > 0:\n",
    "                # For offset case, add edges within the target range\n",
    "                new_source = torch.randint(offset, offset + num_nodes, (num_new_edges,), device=edge_index.device)\n",
    "                new_target = torch.randint(offset, offset + num_nodes, (num_new_edges,), device=edge_index.device)\n",
    "            else:\n",
    "                new_source = torch.randint(0, num_nodes, (num_new_edges,), device=edge_index.device)\n",
    "                new_target = torch.randint(0, num_nodes, (num_new_edges,), device=edge_index.device)\n",
    "            \n",
    "            # Remove self-loops from new edges\n",
    "            valid_new = new_source != new_target\n",
    "            if valid_new.any():\n",
    "                new_edges = torch.stack([new_source[valid_new], new_target[valid_new]], dim=0)\n",
    "                edge_index = torch.cat([edge_index, new_edges], dim=1)\n",
    "        \n",
    "        # Remove duplicates and ensure proper format\n",
    "        edge_index = coalesce(edge_index, num_nodes=offset + num_nodes if offset > 0 else num_nodes)\n",
    "        \n",
    "        return edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "572ddb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import InnerProductDecoder, GATConv, global_mean_pool, global_max_pool\n",
    "from torch_geometric.nn.resolver import activation_resolver\n",
    "from torch import Tensor\n",
    "from typing import Optional, Union\n",
    "\n",
    "\n",
    "class PolicyModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Policy module that determines whether to continue (0) or stop (1) the recurrent process.\n",
    "    Acts like a VGAE by encoding the graph and making a binary decision.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, hidden_dim: int = 64, dropout: float = 0.1):\n",
    "        super(PolicyModule, self).__init__()\n",
    "        \n",
    "        # Graph attention for encoding\n",
    "        self.gat1 = GATConv(in_channels, hidden_dim, heads=4, concat=False, dropout=dropout)\n",
    "        self.gat2 = GATConv(hidden_dim, hidden_dim // 2, heads=2, concat=False, dropout=dropout)\n",
    "        \n",
    "        # Policy decision layers\n",
    "        self.continue_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Policy decision layers\n",
    "        self.halt_fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim // 2, hidden_dim // 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 4, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor, batch: Optional[Tensor] = None) -> tuple:\n",
    "        \"\"\"\n",
    "        Forward pass of policy module.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Edge connectivity [2, num_edges]\n",
    "            batch: Batch indices [num_nodes] for batched graphs\n",
    "            \n",
    "        Returns:\n",
    "            policy_decision: Binary decision (0=continue, 1=stop) [batch_size, 1]\n",
    "            reconstructed: Reconstructed node features [num_nodes, in_channels]\n",
    "        \"\"\"\n",
    "        # Encode graph\n",
    "        h1 = F.relu(self.gat1(x, edge_index))\n",
    "        h1 = self.dropout(h1)\n",
    "        h2 = F.relu(self.gat2(h1, edge_index))\n",
    "        h2 = self.dropout(h2)\n",
    "        \n",
    "        # Pool to graph-level representation for policy decision\n",
    "        if batch is not None:\n",
    "            graph_embedding = global_mean_pool(h2, batch)\n",
    "        else:\n",
    "            graph_embedding = torch.mean(h2, dim=0, keepdim=True)\n",
    "        \n",
    "        # Policy decision (0 = continue, 1 = stop)\n",
    "        continue_q_val = self.continue_fc(graph_embedding)\n",
    "        halt_q_val = self.halt_fc(graph_embedding)\n",
    "        \n",
    "        return halt_q_val, continue_q_val\n",
    "\n",
    "\n",
    "class RecurrentModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent module that applies attention convolution multiple times.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, hidden_dim: int = 64, num_attention_layers: int = 3, \n",
    "                 heads: int = 4, dropout: float = 0.1):\n",
    "        super(RecurrentModule, self).__init__()\n",
    "        \n",
    "        self.num_attention_layers = num_attention_layers\n",
    "        self.attention_layers = nn.ModuleList()\n",
    "        \n",
    "        # First layer: in_channels -> hidden_dim\n",
    "        self.attention_layers.append(\n",
    "            GATConv(in_channels, hidden_dim, heads=heads, concat=False, dropout=dropout)\n",
    "        )\n",
    "        \n",
    "        # Intermediate layers: hidden_dim -> hidden_dim\n",
    "        for _ in range(num_attention_layers - 2):\n",
    "            self.attention_layers.append(\n",
    "                GATConv(hidden_dim, hidden_dim, heads=heads, concat=False, dropout=dropout)\n",
    "            )\n",
    "        \n",
    "        # Last layer: hidden_dim -> in_channels (to maintain shape)\n",
    "        if num_attention_layers > 1:\n",
    "            self.attention_layers.append(\n",
    "                GATConv(hidden_dim, in_channels, heads=heads, concat=False, dropout=dropout)\n",
    "            )\n",
    "        else:\n",
    "            # If only one layer, output in_channels directly\n",
    "            self.attention_layers[0] = GATConv(in_channels, in_channels, heads=heads, concat=False, dropout=dropout)\n",
    "        \n",
    "        self.final_attention = GATConv.GATConv(in_channels, in_channels, heads=heads, concat=False, dropout=dropout)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.residual = nn.Linear(in_channels, in_channels) if num_attention_layers > 0 else nn.Identity()\n",
    "        \n",
    "    def forward(self, x: Tensor, edge_index: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Apply attention convolution layers recurrently.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            edge_index: Edge connectivity [2, num_edges]\n",
    "            \n",
    "        Returns:\n",
    "            Updated node features [num_nodes, in_channels]\n",
    "        \"\"\"\n",
    "        residual = self.residual(x)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            h = x\n",
    "            for i, attention_layer in enumerate(self.attention_layers):\n",
    "                h = attention_layer(h, edge_index)\n",
    "                h = F.relu(h)\n",
    "                h = self.dropout(h)\n",
    "        \n",
    "        h = self.final_attention(h, edge_index)\n",
    "        \n",
    "        # Add residual connection\n",
    "        return h + residual\n",
    "\n",
    "\n",
    "class OutputModule(nn.Module):\n",
    "    \"\"\"\n",
    "    Output module that pools, convolves, and produces a single scalar output.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, pool_size: int = 32, hidden_layers: int = 2, \n",
    "                 hidden_dim: int = 64, act: str = 'relu', dropout: float = 0.1):\n",
    "        super(OutputModule, self).__init__()\n",
    "        \n",
    "        self.pool_size = pool_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        \n",
    "        # Adaptive pooling to fixed size\n",
    "        self.adaptive_pool = nn.AdaptiveAvgPool1d(pool_size)\n",
    "        \n",
    "        # Convolution layers\n",
    "        self.conv1 = nn.Conv1d(in_channels, hidden_dim, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim // 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Calculate flattened size after pooling and convolution\n",
    "        conv_output_size = (hidden_dim // 2) * pool_size\n",
    "        \n",
    "        # Build fully connected layers\n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        \n",
    "        # First FC layer\n",
    "        self.fc_layers.append(nn.Linear(conv_output_size, hidden_dim))\n",
    "        \n",
    "        # Hidden FC layers\n",
    "        for _ in range(hidden_layers - 1):\n",
    "            self.fc_layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "        \n",
    "        # Output layer\n",
    "        self.output_layer = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        self.act = activation_resolver(act)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_dim)\n",
    "        \n",
    "    def forward(self, x: Tensor, batch: Optional[Tensor] = None) -> Tensor:\n",
    "        \"\"\"\n",
    "        Process graph tensor through pooling, convolution, and FC layers.\n",
    "        \n",
    "        Args:\n",
    "            x: Node features [num_nodes, in_channels]\n",
    "            batch: Batch indices for handling multiple graphs\n",
    "            \n",
    "        Returns:\n",
    "            Single scalar output [batch_size, 1]\n",
    "        \"\"\"\n",
    "        if batch is not None:\n",
    "            # Handle batched graphs\n",
    "            batch_size = batch.max().item() + 1\n",
    "            outputs = []\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                mask = (batch == i)\n",
    "                graph_x = x[mask]  # [num_nodes_in_graph, in_channels]\n",
    "                \n",
    "                # Process single graph\n",
    "                single_output = self._process_single_graph(graph_x)\n",
    "                outputs.append(single_output)\n",
    "            \n",
    "            return torch.stack(outputs)\n",
    "        else:\n",
    "            # Single graph\n",
    "            return self._process_single_graph(x).unsqueeze(0)\n",
    "    \n",
    "    def _process_single_graph(self, x: Tensor) -> Tensor:\n",
    "        \"\"\"Process a single graph through the output pipeline.\"\"\"\n",
    "        # x: [num_nodes, in_channels]\n",
    "        \n",
    "        # Transpose for Conv1d: [in_channels, num_nodes]\n",
    "        x = x.transpose(0, 1)\n",
    "        \n",
    "        # Adaptive pooling to fixed size: [in_channels, pool_size]\n",
    "        x = self.adaptive_pool(x)\n",
    "        \n",
    "        # Convolution layers\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Flatten: [in_channels * pool_size]\n",
    "        x = x.flatten()\n",
    "        \n",
    "        # Fully connected layers with normalization\n",
    "        for fc_layer in self.fc_layers:\n",
    "            x = fc_layer(x)\n",
    "            x = self.layer_norm(x)\n",
    "            x = self.act(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class RecurrentDecoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent decoder that processes graphs iteratively using a policy-guided approach.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels: int, hidden_layers: int = 2, act: str = 'relu', \n",
    "                 max_iter: int = 5, hidden_dim: int = 64, num_attention_layers: int = 3,\n",
    "                 pool_size: int = 32, dropout: float = 0.1):\n",
    "        super(RecurrentDecoder, self).__init__()\n",
    "        \n",
    "        self.base_decoder = InnerProductDecoder()\n",
    "        self.max_iter = max_iter\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # Initialize modules\n",
    "        self.policy_module = PolicyModule(in_channels, hidden_dim, dropout)\n",
    "        self.low_module = DualGraphRecurrentModule(in_channels, hidden_dim, num_attention_layers, \n",
    "                                              heads=4, dropout=dropout)\n",
    "        self.high_module = EdgePredictionModule(in_channels, hidden_dim, 0.6, dropout)\n",
    "        self.output_module = OutputModule(in_channels, pool_size, hidden_layers, \n",
    "                                        hidden_dim, act, dropout)\n",
    "        \n",
    "    def forward(self, z: List[Tensor], edge_indices: List[Tensor], batch: Optional[Tensor] = None, segment_number = 0) -> Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass through the recurrent decoder.\n",
    "        \n",
    "        Args:\n",
    "            z: Node embeddings of x, z_l: [2, num_nodes, in_channels]\n",
    "            edge_index: Edge connectivity of x, z_l: [2, 2, num_edges]\n",
    "            batch: Batch indices for multiple graphs\n",
    "            \n",
    "        Returns:\n",
    "            Final scalar output [batch_size, 1]\n",
    "        \"\"\"\n",
    "        z_l = z[1]\n",
    "        z_h_edge = edge_indices[1]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i in range(max(self.max_iter-1, 0)):\n",
    "                z_l = self.low_module(z[0], z_l, edge_indices[0], z_h_edge)\n",
    "                z_h_edge = self.high_module(z_l, z_h_edge)\n",
    "        \n",
    "        z_l = self.low_module(z[0], z_l, edge_indices[0], z_h_edge)\n",
    "        z_h_edge = self.high_module(z_l, z_h_edge)\n",
    "        \n",
    "        # Generate final output (fix to work as hrm with only zh as param)\n",
    "        output = self.output_module(z_l, batch)\n",
    "        \n",
    "        # Policy decision: should we continue or stop?\n",
    "        with torch.no_grad():\n",
    "            halt_q_val, continue_q_val = self.policy_module(z_l, z_h_edge, batch)\n",
    "        \n",
    "        return z_l, z_h_edge, output, halt_q_val, continue_q_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f4ff4353",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from typing import Optional, List, Tuple, Dict\n",
    "\n",
    "\n",
    "class SingleIterationACTLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Single-iteration ACT Loss following the Q-learning formulation from the paper.\n",
    "    \n",
    "    Implements the proper ACT loss with Q-learning targets:\n",
    "    - G_halt = 1{ŷ^m = y} (binary reward for correct prediction)  \n",
    "    - G_continue = Q_halt^{m+1} if m < N_max, else Q_halt^{m+1}\n",
    "    - Loss = LOSS(ŷ^m, y) + BINARYCROSSENTROPY(Q̂^m, Ĝ^m)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "        policy_weight: float = 1.0,\n",
    "        output_weight: float = 1.0,\n",
    "        max_segments: int = 5,\n",
    "        prediction_threshold: float = 0.1\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            policy_weight: Weight for Q-learning policy loss\n",
    "            output_weight: Weight for main prediction loss\n",
    "            max_segments: Maximum number of segments (N_max in paper)\n",
    "            prediction_threshold: Threshold for determining prediction correctness\n",
    "        \"\"\"\n",
    "        super(SingleIterationACTLoss, self).__init__()\n",
    "        \n",
    "        self.policy_weight = policy_weight\n",
    "        self.output_weight = output_weight\n",
    "        self.max_segments = max_segments\n",
    "        self.prediction_threshold = prediction_threshold\n",
    "        \n",
    "        # Loss functions\n",
    "        self.output_loss_fn = nn.MSELoss()  # Sequence-to-sequence loss\n",
    "        self.q_loss_fn = nn.BCEWithLogitsLoss()  # Q-learning loss\n",
    "        \n",
    "    def forward(self, \n",
    "        q_halt: Tensor,                 # Q-value for halt action [batch_size, 1]\n",
    "        q_continue: Tensor,             # Q-value for continue action [batch_size, 1]\n",
    "        final_output: Tensor,          # Final prediction [batch_size, output_dim]\n",
    "        targets: Tensor,               # Ground truth [batch_size, output_dim]\n",
    "        current_segment: int,          # Current segment number (m in paper)\n",
    "        next_q_values: Optional[Tensor] = None  # Q-values from next iteration (for G_continue)\n",
    "        ) -> Tuple[Tensor, Dict]:\n",
    "        \"\"\"\n",
    "        Compute single-iteration ACT losses following the paper's Q-learning formulation.\n",
    "        \n",
    "        Args:\n",
    "            q_halt: Predicted Q-value for halt action\n",
    "            q_continue: Predicted Q-value for continue action  \n",
    "            recurrent_output: Output from recurrent module (will be detached for encoder loss)\n",
    "            final_output: Final output from output module\n",
    "            targets: Ground truth targets\n",
    "            current_segment: Current segment number (m)\n",
    "            next_q_values: Optional Q-values from next iteration [batch_size, 2] -> [Q_halt^{m+1}, Q_continue^{m+1}]\n",
    "            \n",
    "        Returns:\n",
    "            policy_output_loss: Loss for policy + recurrent + output modules\n",
    "            encoder_loss: Loss for encoder module (with detached recurrent output)\n",
    "            loss_dict: Dictionary with loss components for monitoring\n",
    "        \"\"\"\n",
    "        batch_size = targets.shape[0]\n",
    "        \n",
    "        # 1. Sequence-to-sequence loss: LOSS(ŷ^m, y)\n",
    "        output_loss = self.output_loss_fn(final_output, targets)\n",
    "        \n",
    "        # 2. Compute Q-learning targets following ACT paper\n",
    "        with torch.no_grad():\n",
    "            # G_halt^m = 1{ŷ^m = y} - binary reward for correct prediction\n",
    "            if targets.dtype == torch.float:\n",
    "                # For regression: use threshold-based correctness\n",
    "                prediction_correct = (torch.abs(final_output - targets).mean(dim=1, keepdim=True) \n",
    "                    < self.prediction_threshold).float()\n",
    "            else:\n",
    "                # For classification: exact match\n",
    "                prediction_correct = (final_output.argmax(dim=-1, keepdim=True) == targets).float()\n",
    "            \n",
    "            g_halt = prediction_correct  # [batch_size, 1]\n",
    "            \n",
    "            # G_continue^m computation\n",
    "            if current_segment >= self.max_segments:\n",
    "                # If m >= N_max: G_continue^m = Q_halt^{m+1} = G_halt^m (terminal)\n",
    "                g_continue = g_halt\n",
    "            else:\n",
    "                # Otherwise: G_continue^m = max(Q_halt^{m+1}, Q_continue^{m+1})\n",
    "                if next_q_values is not None:\n",
    "                    # Use actual next Q-values if provided\n",
    "                    next_q_halt, next_q_continue = next_q_values[:, 0:1], next_q_values[:, 1:2]\n",
    "                    g_continue = torch.max(next_q_halt, next_q_continue)\n",
    "                else:\n",
    "                    # Estimate using current values (approximation for single iteration)\n",
    "                    g_continue = torch.max(g_halt, torch.zeros_like(g_halt))\n",
    "            \n",
    "            # Combined Q-targets: Ĝ^m = (G_halt^m, G_continue^m)\n",
    "            q_targets_halt = g_halt\n",
    "            q_targets_continue = g_continue\n",
    "        \n",
    "        # 3. Q-learning loss: BINARYCROSSENTROPY(Q̂^m, Ĝ^m)\n",
    "        q_loss_halt = self.q_loss_fn(q_halt, q_targets_halt)\n",
    "        q_loss_continue = self.q_loss_fn(q_continue, q_targets_continue)\n",
    "        q_loss = q_loss_halt + q_loss_continue\n",
    "        \n",
    "        # 4. Combined ACT loss: L_ACT^m = LOSS(ŷ^m, y) + BINARYCROSSENTROPY(Q̂^m, Ĝ^m)\n",
    "        policy_output_loss = self.output_weight * output_loss + self.policy_weight * q_loss\n",
    "     \n",
    "        # 6. SAFER: Create loss_dict without .item() calls during gradient computation\n",
    "        loss_dict = {\n",
    "            'policy_output_loss': policy_output_loss.detach().item(),\n",
    "            'output_loss': output_loss.detach().item(),\n",
    "            'q_loss': q_loss.detach().item(),\n",
    "            'q_loss_halt': q_loss_halt.detach().item(),\n",
    "            'q_loss_continue': q_loss_continue.detach().item(),\n",
    "            'avg_g_halt': g_halt.mean().item(),\n",
    "            'avg_g_continue': g_continue.mean().item(),\n",
    "            'avg_q_halt': q_halt.detach().mean().item(),\n",
    "            'avg_q_continue': q_continue.detach().mean().item()\n",
    "        }\n",
    "        \n",
    "        return policy_output_loss, loss_dict\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15389f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import models from the new models.py file\n",
    "from models import (\n",
    "    CustomGINEncoder,\n",
    "    DualGraphRecurrentModule,\n",
    "    EdgePredictionModule,\n",
    "    EdgeManipulationLayer,\n",
    "    PolicyModule,\n",
    "    RecurrentModule,\n",
    "    OutputModule,\n",
    "    RecurrentDecoder,\n",
    "    SingleIterationACTLoss\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57eff4d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder shape:  torch.Size([23, 128])\n",
      "torch.Size([1, 1])\n",
      "tensor([0.3883], grad_fn=<SelectBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import Set2Set, AttentionalAggregation, GIN, VGAE, InnerProductDecoder\n",
    "# from torch_geometric.nn.models.gpse import GPSE\n",
    "from torch_geometric.transforms import add_positional_encoding\n",
    "\n",
    "from torch_geometric.utils import negative_sampling, dropout_edge, dropout_node, from_smiles, to_smiles, to_rdmol, from_rdmol\n",
    "from torch.optim import Adam\n",
    "from torch.nn import SmoothL1Loss\n",
    "\n",
    "config = {\n",
    "    \"epochs\": 100,\n",
    "    \"batch_size\": 64,\n",
    "\n",
    "    \"dropout\": 0.01,\n",
    "    \"lr\": 1e-5,\n",
    "    \"latent_dim\": 50,\n",
    "\n",
    "    \"alpha\": 0.1,\n",
    "    \"beta\": 0.001,\n",
    "\n",
    "    \"segments\": 5,\n",
    "\n",
    "    \"encoder\": {\n",
    "        \"hidden_dims\": [16, 32, 64],\n",
    "        \"output_dim\": 128,\n",
    "        \"layers\": 4,\n",
    "        \"norm\": \"layer\"\n",
    "    },\n",
    "\n",
    "    \"decoder\": {\n",
    "        \"norm\": \"layer\",\n",
    "        \"attention_heads\": 2,\n",
    "        \"hidden_dim\": 64,\n",
    "        \"decoder_layers\": 3,\n",
    "        \n",
    "        # RecurrentDecoder specific parameters\n",
    "        \"in_channels\": 128,  # Same as encoder output_dim\n",
    "        \"hidden_layers\": 3,  # Number of hidden FC layers in output module\n",
    "        \"act\": \"relu\",       # Activation function\n",
    "        \"max_iter\": 5,       # Maximum recurrent iterations\n",
    "        \"num_attention_layers\": 4,  # Attention layers in recurrent module\n",
    "        \"pool_size\": 32,     # Pooling size for output module\n",
    "        \"policy_hidden_dim\": 64,  # Hidden dimension for policy module\n",
    "        \"heads\": 4,          # Attention heads for recurrent module\n",
    "    }\n",
    "}\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "econfig=dict(config[\"encoder\"])\n",
    "\n",
    "encoder = CustomGINEncoder(\n",
    "    in_channels=6, #length of 'x' data\n",
    "    hidden_dims=econfig[\"hidden_dims\"],\n",
    "    out_channels=econfig[\"output_dim\"],\n",
    "    dropout=config[\"dropout\"],\n",
    "    act=\"leakyrelu\",\n",
    "    norm=econfig[\"norm\"]\n",
    ")\n",
    "print(\"Encoder shape: \", encoder(train_dataset[0]).shape)\n",
    "\n",
    "dconfig=dict(config[\"decoder\"])\n",
    "decoder = RecurrentDecoder(\n",
    "    in_channels=dconfig[\"in_channels\"],\n",
    "    hidden_layers=dconfig[\"hidden_layers\"],\n",
    "    act=dconfig[\"act\"],\n",
    "    max_iter=dconfig[\"max_iter\"],\n",
    "    hidden_dim=dconfig[\"hidden_dim\"],\n",
    "    num_attention_layers=dconfig[\"num_attention_layers\"],\n",
    "    pool_size=dconfig[\"pool_size\"],\n",
    "    dropout=config[\"dropout\"]\n",
    ")\n",
    "\n",
    "encoder_loss = SmoothL1Loss()\n",
    "decoder_loss = SingleIterationACTLoss(policy_weight=0.5, max_segments=config[\"segments\"], prediction_threshold=0.01)\n",
    "\n",
    "encoder_opt = Adam(encoder.parameters())\n",
    "output_opt = Adam(decoder.parameters())\n",
    "\n",
    "x = encoder(train_dataset[0])\n",
    "x_edg = train_dataset[0]['edge_index']\n",
    "\n",
    "z_l, z_h_edge, pred, _, _ = decoder([x, x.detach()], [x_edg, torch.zeros_like(x_edg)])\n",
    "print(pred.shape)\n",
    "print(pred[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8271e5fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Data(x=[23, 6], edge_index=[2, 44], edge_attr=[44, 4], y=0.32618869)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thomaspugh/projects/chem-properties/.venv/lib/python3.11/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.data import Batch\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def single_iter_train(data_batch):\n",
    "    \n",
    "    x: torch.Tensor = encoder(data_batch)\n",
    "    x_edge = data_batch['edge_index']\n",
    "\n",
    "    z_l = x.clone().detach()\n",
    "    z_h_edge = torch.zeros_like(x_edge)\n",
    "\n",
    "    for idx in range(0, config[\"segments\"]):\n",
    "        # Forward pass\n",
    "        z_l, z_h_edge, y, halt_q, cont_q = decoder(\n",
    "            [x, z_l], [x_edge, z_h_edge], segment_number=idx\n",
    "        )\n",
    "\n",
    "        with torch.no_grad():\n",
    "            g_halt, g_cont = decoder.policy_module(z_l, z_h_edge)\n",
    "        loss, dictionary_loss = decoder_loss(\n",
    "            halt_q, \n",
    "            cont_q,\n",
    "            y,\n",
    "            data_batch[\"y\"], \n",
    "            idx\n",
    "        )\n",
    "        \n",
    "        loss.backward(retain_graph=True)\n",
    "        output_opt.step()\n",
    "        output_opt.zero_grad()\n",
    "    \n",
    "    # encoder_loss(x,)\n",
    "    encoder_opt.zero_grad()\n",
    "\n",
    "\n",
    "for batch in train_dataloader:\n",
    "    print(batch)\n",
    "    batch = Batch.from_data_list(batch)\n",
    "    single_iter_train(batch)\n",
    "    \n",
    "    break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chem-properties",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
